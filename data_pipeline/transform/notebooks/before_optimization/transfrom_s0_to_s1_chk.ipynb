{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2af1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import yaml, os\n",
    "\n",
    "from utils.spark_path import (\n",
    "    get_latest_year_month_path,\n",
    "    get_current_year_month_week_path,\n",
    "    get_latest_year_month_week_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "356151f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8\n",
      "Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8\n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/21 11:32:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Spark\n",
    "# ============================================================\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"silver_s0_to_s1_chk\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b6055d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " restaurant clean = /opt/spark/data/silver/clean/restaurant/year=2026/month=02/week=03\n",
      "restaurant = /opt/spark/data/silver/s1/restaurant_coord/year=2026/month=02/week=03\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Config\n",
    "# ============================================================\n",
    "\n",
    "CONFIG_PATH = \"./config.yaml\"\n",
    "\n",
    "with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "ROOT = cfg[\"data_lake\"][\"root\"]\n",
    "LAYERS = cfg[\"data_lake\"][\"layers\"]\n",
    "\n",
    "# -------------------------\n",
    "# INPUT\n",
    "# -------------------------\n",
    "\n",
    "restaurant_clean_base = os.path.join(\n",
    "    ROOT,\n",
    "    LAYERS[\"silver\"][\"clean\"][\"domains\"][\"restaurant_clean\"][\"paths\"][\"parquet\"]\n",
    ")\n",
    "\n",
    "restaurant_clean_path = get_latest_year_month_week_path(\n",
    "    spark, restaurant_clean_base\n",
    ")\n",
    "\n",
    "s0_address_base = os.path.join(\n",
    "    ROOT,\n",
    "    LAYERS[\"silver\"][\"stages\"][\"s0\"][\"domains\"][\"address\"][\"paths\"][\"parquet\"]\n",
    ")\n",
    "\n",
    "s0_address_path = get_latest_year_month_path(\n",
    "    spark, s0_address_base\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# OUTPUT\n",
    "# -------------------------\n",
    "\n",
    "restaurant_coord_src_base = os.path.join(\n",
    "    ROOT,\n",
    "    LAYERS[\"silver\"][\"stages\"][\"s1\"][\"domains\"][\"restaurant_coord\"][\"paths\"][\"parquet\"]\n",
    ")\n",
    "restaurant_coord_src_path = get_current_year_month_week_path(restaurant_coord_src_base)\n",
    "\n",
    "partition_cols = LAYERS[\"silver\"][\"stages\"][\"s1\"][\"domains\"][\"restaurant_coord\"].get(\n",
    "    \"partition\"\n",
    ")\n",
    "\n",
    "print(\" restaurant clean =\", restaurant_clean_path)\n",
    "print(\"restaurant =\", restaurant_coord_src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e16d8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 업체명: string (nullable = true)\n",
      " |-- 대표자: string (nullable = true)\n",
      " |-- 소재지: string (nullable = true)\n",
      " |-- 업종: string (nullable = true)\n",
      " |-- 대표자수: integer (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- PNU코드: string (nullable = true)\n",
      " |-- 도로명주소: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LOAD\n",
    "# ============================================================\n",
    "\n",
    "rest_df = (\n",
    "    spark.read.parquet(restaurant_clean_path)\n",
    ")\n",
    "\n",
    "addr_df = (\n",
    "    spark.read.parquet(s0_address_path)\n",
    "    .select(\"PNU코드\", \"도로명주소\", \"longitude\", \"latitude\")\n",
    ")\n",
    "\n",
    "\n",
    "rest_df.printSchema()\n",
    "addr_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3b3ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = (\n",
    "    rest_df.alias(\"r\")\n",
    "    .join(\n",
    "        addr_df.alias(\"a\"),\n",
    "        F.col(\"r.소재지\") == F.col(\"a.도로명주소\"),\n",
    "        \"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "joined_rest_df = (\n",
    "    joined_df\n",
    "    .filter(F.col(\"PNU코드\").isNotNull())\n",
    "    .select(\n",
    "        F.col(\"r.업체명\").alias(\"업체명\"),\n",
    "        F.col(\"r.업종\").alias(\"업종\"),\n",
    "        F.col(\"r.대표자\").alias(\"대표자\"),\n",
    "        F.col(\"r.대표자수\").alias(\"대표자_수\"),\n",
    "        F.col(\"r.소재지\").alias(\"도로명주소\"),\n",
    "        F.col(\"r.region\").alias(\"region\"),\n",
    "        F.col(\"a.PNU코드\").alias(\"PNU코드\"),\n",
    "        F.col(\"a.longitude\").alias(\"longitude\"),\n",
    "        F.col(\"a.latitude\").alias(\"latitude\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4156c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 업체명: string (nullable = true)\n",
      " |-- 업종: string (nullable = true)\n",
      " |-- 대표자: string (nullable = true)\n",
      " |-- 대표자_수: integer (nullable = true)\n",
      " |-- 도로명주소: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- PNU코드: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_rest_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfc6e76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ saved: /opt/spark/data/silver/s1/restaurant_coord/year=2026/month=02/week=03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    joined_rest_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(*partition_cols)\n",
    "    .parquet(restaurant_coord_src_path)\n",
    ")\n",
    "\n",
    "print(\"✅ saved:\", restaurant_coord_src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6035fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
