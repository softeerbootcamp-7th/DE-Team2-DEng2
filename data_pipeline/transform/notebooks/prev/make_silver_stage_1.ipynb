{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95a41493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "import datetime as dt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68bf769-9485-4f7f-bbad-b444ecd7a212",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e8e06e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선택된 REGION 축약형:  경기\n",
      "선택된 REGION 영어:  gyunggi\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 사용자 설정 (원하는 지역과 시군구를 입력하세요)\n",
    "# ==========================================\n",
    "REGION = \"경기도\"          # \"ex.서울특별시\"\n",
    "SIGUNGU = \"용인시 처인구\"    # \"ex.강남구\"\n",
    "\n",
    "# 베이스 경로\n",
    "OUTPUT_BASE = \"/opt/spark/data/output/silver_stage_1\"\n",
    "ADDRESS_BASE = \"/opt/spark/data/address/parquet/road\"\n",
    "COORD_BASE = \"opt/spark/data/address/parquet/coord\"\n",
    "BUILDING_BASE   = \"/opt/spark/data/buildingLeader/parquet\"\n",
    "TOJI_BASE       = \"/opt/spark/data/tojiSoyuJeongbo/parquet\"\n",
    "REST_OWNER_BASE = \"/opt/spark/data/restaurant_owner/parquet\"\n",
    "\n",
    "\n",
    "# 데이터 필터용(축약형) 매핑\n",
    "REGION_MAP = {\n",
    "    \"서울특별시\": \"서울\",\n",
    "    \"부산광역시\": \"부산\",\n",
    "    \"대구광역시\": \"대구\",\n",
    "    \"인천광역시\": \"인천\",\n",
    "    \"광주광역시\": \"광주\",\n",
    "    \"대전광역시\": \"대전\",\n",
    "    \"울산광역시\": \"울산\",\n",
    "    \"세종특별자치시\": \"세종\",\n",
    "    \"경기도\": \"경기\",\n",
    "    \"강원도\": \"강원\",\n",
    "    \"충청북도\": \"충북\",\n",
    "    \"충청남도\": \"충남\",\n",
    "    \"전라북도\": \"전북\",\n",
    "    \"전라남도\": \"전남\",\n",
    "    \"경상북도\": \"경북\",\n",
    "    \"경상남도\": \"경남\",\n",
    "    \"제주특별자치도\": \"제주\",\n",
    "}\n",
    "\n",
    "REGION_SHORT = REGION_MAP[REGION]\n",
    "print(\"선택된 REGION 축약형: \", REGION_SHORT)\n",
    "\n",
    "# 데이터 필터용(축약형) 매핑\n",
    "REGION_ENG_MAP = {\n",
    "    \"서울특별시\": \"seoul\",\n",
    "    \"부산광역시\": \"busan\",\n",
    "    \"대구광역시\": \"daegu\",\n",
    "    \"인천광역시\": \"incheon\",\n",
    "    \"광주광역시\": \"gwangju\",\n",
    "    \"대전광역시\": \"daejeon\",\n",
    "    \"울산광역시\": \"ulsan\",\n",
    "    \"세종특별자치시\": \"sejong\",\n",
    "    \"경기도\": \"gyunggi\",\n",
    "    \"강원도\": \"gangwon\",\n",
    "    \"충청북도\": \"chungbuk\",\n",
    "    \"충청남도\": \"chungnam\",\n",
    "    \"전라북도\": \"jeonbuk\",\n",
    "    \"전라남도\": \"jeonnam\",\n",
    "    \"경상북도\": \"gyeongbuk\",\n",
    "    \"경상남도\": \"gyeongnam\",\n",
    "    \"제주특별자치도\": \"jeju\",\n",
    "}\n",
    "\n",
    "REGION_ENG = REGION_ENG_MAP[REGION]\n",
    "print(\"선택된 REGION 영어: \", REGION_ENG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00470063-c18a-4570-b14a-94fc96382790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8\n",
      "Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8\n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/18 14:56:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Spark Session 설정\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(f'silver_stage_1_{REGION}_{SIGUNGU.replace(\" \", \"_\")}') \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"400\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1830e81-e118-4b80-b3be-c6acd565f153",
   "metadata": {},
   "source": [
    "# 원천 데이터 로드 + 지역/시군구 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6375e99-bf6d-4055-af8d-8d0f8c3824cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|sigungu_code|\n",
      "+------------+\n",
      "|41220       |\n",
      "+------------+\n",
      "\n",
      "root\n",
      " |-- PNU코드: string (nullable = true)\n",
      " |-- 도로명주소: string (nullable = false)\n",
      "\n",
      "60761\n",
      "+-------------------+---------------------------+\n",
      "|PNU코드            |도로명주소                 |\n",
      "+-------------------+---------------------------+\n",
      "|4122010100101590005|경기도 평택시 경기대로 1337|\n",
      "|4122010100101590105|경기도 평택시 경기대로 1339|\n",
      "|4122010100101590000|경기도 평택시 경기대로 1341|\n",
      "|4122010100108120006|경기도 평택시 경기대로 1345|\n",
      "|4122010100108120005|경기도 평택시 경기대로 1347|\n",
      "+-------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "SIGUNGU_CODE = 41220\n"
     ]
    }
   ],
   "source": [
    "address_path = f\"{ADDRESS_BASE}/rnaddrkor_{REGION_ENG}.parquet\"\n",
    "address_df = spark.read.option(\"mergeSchema\", \"false\").parquet(address_path)\n",
    "\n",
    "address_df.printSchema\n",
    "\n",
    "address_filtered_df = (\n",
    "    address_df\n",
    "    .filter(F.col(\"시도명\") == REGION)\n",
    "    .filter(F.col(\"시군구명\").startswith(SIGUNGU))\n",
    "    .withColumn(\n",
    "        \"pnu_land_gb\",\n",
    "        F.when(F.col(\"산여부\") == 1, F.lit(\"2\")).otherwise(F.lit(\"1\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"PNU코드\",\n",
    "        F.concat(\n",
    "            F.col(\"법정동코드\"),\n",
    "            F.col(\"pnu_land_gb\"),\n",
    "            F.lpad(F.col(\"지번본번(번지)\").cast(\"string\"), 4, \"0\"),\n",
    "            F.lpad(F.coalesce(F.col(\"지번부번(호)\"), F.lit(0)).cast(\"string\"), 4, \"0\")\n",
    "        )\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"PNU코드\").cast(\"string\"),\n",
    "\n",
    "        F.concat_ws(\n",
    "            \" \",\n",
    "            F.col(\"시도명\"),\n",
    "            F.col(\"시군구명\"),\n",
    "            F.col(\"도로명\"),\n",
    "            F.concat(\n",
    "                F.col(\"건물본번\").cast(\"string\"),\n",
    "                F.when(\n",
    "                    (F.col(\"건물부번\").isNotNull()) & (F.col(\"건물부번\") != 0),\n",
    "                    F.concat(F.lit(\"-\"), F.col(\"건물부번\").cast(\"string\"))\n",
    "                ).otherwise(F.lit(\"\"))\n",
    "            )\n",
    "        ).alias(\"도로명주소\")\n",
    "    )\n",
    ")\n",
    "\n",
    "address_filtered_df.select(\n",
    "    F.substring(\"PNU코드\", 1, 5).alias(\"sigungu_code\")\n",
    ").distinct().show(truncate=False)\n",
    "\n",
    "\n",
    "SIGUNGU_CODE = address_filtered_df.select(F.substring(\"PNU코드\", 1, 5).alias(\"sigungu\")).first()[\"sigungu\"]\n",
    "\n",
    "address_filtered_df.printSchema()\n",
    "print(address_filtered_df.count())\n",
    "address_filtered_df.show(5, truncate=False)\n",
    "print(\"SIGUNGU_CODE =\", SIGUNGU_CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d30e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜 계산 (직전 분기 말일 기준)\n",
    "today = dt.date.today()\n",
    "cur_q = (today.month - 1) // 3 + 1\n",
    "prev_q = cur_q - 1\n",
    "prev_q_year = today.year\n",
    "if prev_q == 0:\n",
    "    prev_q = 4\n",
    "    prev_q_year -= 1\n",
    "prev_q_last_month = prev_q * 3\n",
    "\n",
    "# 데이터 로드 경로\n",
    "building_path = f\"{BUILDING_BASE}/year={prev_q_year}/month={prev_q_last_month:02d}\"\n",
    "toji_path     = f\"{TOJI_BASE}/year={prev_q_year}/month={prev_q_last_month:02d}\"\n",
    "rest_owner_path  = f\"{REST_OWNER_BASE}/year={prev_q_year}/month={prev_q_last_month:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b8c2ad8-fcdf-4500-b188-a0a84106e27b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "건축물대장 스키마:\n",
      "root\n",
      " |-- 관리_건축물대장_PK: string (nullable = true)\n",
      " |-- 대장_구분_코드: string (nullable = true)\n",
      " |-- 대장_구분_코드_명: string (nullable = true)\n",
      " |-- 대장_종류_코드: string (nullable = true)\n",
      " |-- 대장_종류_코드_명: string (nullable = true)\n",
      " |-- 대지_위치: string (nullable = true)\n",
      " |-- 도로명_대지_위치: string (nullable = true)\n",
      " |-- 건물_명: string (nullable = true)\n",
      " |-- 시군구_코드: string (nullable = true)\n",
      " |-- 법정동_코드: string (nullable = true)\n",
      " |-- 대지_구분_코드: string (nullable = true)\n",
      " |-- 번: string (nullable = true)\n",
      " |-- 지: string (nullable = true)\n",
      " |-- 특수지_명: string (nullable = true)\n",
      " |-- 블록: string (nullable = true)\n",
      " |-- 로트: string (nullable = true)\n",
      " |-- 외필지_수: string (nullable = true)\n",
      " |-- 새주소_도로_코드: string (nullable = true)\n",
      " |-- 새주소_법정동_코드: string (nullable = true)\n",
      " |-- 새주소_지상지하_코드: string (nullable = true)\n",
      " |-- 새주소_본_번: string (nullable = true)\n",
      " |-- 새주소_부_번: string (nullable = true)\n",
      " |-- 동_명: string (nullable = true)\n",
      " |-- 주_부속_구분_코드: string (nullable = true)\n",
      " |-- 주_부속_구분_코드_명: string (nullable = true)\n",
      " |-- 대지_면적(㎡): string (nullable = true)\n",
      " |-- 건축_면적(㎡): string (nullable = true)\n",
      " |-- 건폐_율(%): string (nullable = true)\n",
      " |-- 연면적(㎡): string (nullable = true)\n",
      " |-- 용적_률_산정_연면적(㎡): string (nullable = true)\n",
      " |-- 용적_률(%): string (nullable = true)\n",
      " |-- 구조_코드: string (nullable = true)\n",
      " |-- 구조_코드_명: string (nullable = true)\n",
      " |-- 기타_구조: string (nullable = true)\n",
      " |-- 주_용도_코드: string (nullable = true)\n",
      " |-- 주_용도_코드_명: string (nullable = true)\n",
      " |-- 기타_용도: string (nullable = true)\n",
      " |-- 지붕_코드: string (nullable = true)\n",
      " |-- 지붕_코드_명: string (nullable = true)\n",
      " |-- 기타_지붕: string (nullable = true)\n",
      " |-- 세대_수(세대): string (nullable = true)\n",
      " |-- 가구_수(가구): string (nullable = true)\n",
      " |-- 높이(m): string (nullable = true)\n",
      " |-- 지상_층_수: string (nullable = true)\n",
      " |-- 지하_층_수: string (nullable = true)\n",
      " |-- 승용_승강기_수: string (nullable = true)\n",
      " |-- 비상용_승강기_수: string (nullable = true)\n",
      " |-- 부속_건축물_수: string (nullable = true)\n",
      " |-- 부속_건축물_면적(㎡): string (nullable = true)\n",
      " |-- 총_동_연면적(㎡): string (nullable = true)\n",
      " |-- 옥내_기계식_대수(대): string (nullable = true)\n",
      " |-- 옥내_기계식_면적(㎡): string (nullable = true)\n",
      " |-- 옥외_기계식_대수(대): string (nullable = true)\n",
      " |-- 옥외_기계식_면적(㎡): string (nullable = true)\n",
      " |-- 옥내_자주식_대수(대): string (nullable = true)\n",
      " |-- 옥내_자주식_면적(㎡): string (nullable = true)\n",
      " |-- 옥외_자주식_대수(대): string (nullable = true)\n",
      " |-- 옥외_자주식_면적(㎡): string (nullable = true)\n",
      " |-- 허가_일: string (nullable = true)\n",
      " |-- 착공_일: string (nullable = true)\n",
      " |-- 사용승인_일: string (nullable = true)\n",
      " |-- 허가번호_년: string (nullable = true)\n",
      " |-- 허가번호_기관_코드: string (nullable = true)\n",
      " |-- 허가번호_기관_코드_명: string (nullable = true)\n",
      " |-- 허가번호_구분_코드: string (nullable = true)\n",
      " |-- 허가번호_구분_코드_명: string (nullable = true)\n",
      " |-- 호_수(호): string (nullable = true)\n",
      " |-- 에너지효율_등급: string (nullable = true)\n",
      " |-- 에너지절감_율: string (nullable = true)\n",
      " |-- 에너지_EPI점수: string (nullable = true)\n",
      " |-- 친환경_건축물_등급: string (nullable = true)\n",
      " |-- 친환경_건축물_인증점수: string (nullable = true)\n",
      " |-- 지능형_건축물_등급: string (nullable = true)\n",
      " |-- 지능형_건축물_인증점수: string (nullable = true)\n",
      " |-- 생성_일자: string (nullable = true)\n",
      " |-- 내진_설계_적용_여부: string (nullable = true)\n",
      " |-- 내진_능력: string (nullable = true)\n",
      " |-- sido: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "토지소유정보 스키마:\n",
      "root\n",
      " |-- 고유번호: long (nullable = true)\n",
      " |-- 법정동코드: long (nullable = true)\n",
      " |-- 법정동명: string (nullable = true)\n",
      " |-- 대장구분코드: long (nullable = true)\n",
      " |-- 대장구분명: string (nullable = true)\n",
      " |-- 지번: string (nullable = true)\n",
      " |-- 집합건물일련번호: long (nullable = true)\n",
      " |-- 건물동명: string (nullable = true)\n",
      " |-- 건물층명: string (nullable = true)\n",
      " |-- 건물호명: string (nullable = true)\n",
      " |-- 건물실명: string (nullable = true)\n",
      " |-- 공유인일련번호: long (nullable = true)\n",
      " |-- 기준연월: string (nullable = true)\n",
      " |-- 지목코드: long (nullable = true)\n",
      " |-- 지목: string (nullable = true)\n",
      " |-- 토지면적: double (nullable = true)\n",
      " |-- 공시지가: long (nullable = true)\n",
      " |-- 소유구분코드: string (nullable = true)\n",
      " |-- 국가기관구분코드: string (nullable = true)\n",
      " |-- 국가기관구분: string (nullable = true)\n",
      " |-- 소유권변동원인코드: string (nullable = true)\n",
      " |-- 소유권변동원인: string (nullable = true)\n",
      " |-- 소유권변동일자: string (nullable = true)\n",
      " |-- 공유인수: long (nullable = true)\n",
      " |-- 데이터기준일자: string (nullable = true)\n",
      " |-- 원천시도시군구코드: double (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "Distinct 지목 리스트:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=========================================>               (8 + 3) / 11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|지목      |\n",
      "+----------+\n",
      "|공원      |\n",
      "|공장용지  |\n",
      "|과수원    |\n",
      "|광천지    |\n",
      "|구거      |\n",
      "|답        |\n",
      "|대        |\n",
      "|도로      |\n",
      "|목장용지  |\n",
      "|묘지      |\n",
      "|사적지    |\n",
      "|수도용지  |\n",
      "|양어장    |\n",
      "|염전      |\n",
      "|유원지    |\n",
      "|유지      |\n",
      "|임야      |\n",
      "|잡종지    |\n",
      "|전        |\n",
      "|제방      |\n",
      "|종교용지  |\n",
      "|주유소용지|\n",
      "|주차장    |\n",
      "|창고용지  |\n",
      "|철도용지  |\n",
      "|체육용지  |\n",
      "|하천      |\n",
      "|학교용지  |\n",
      "+----------+\n",
      "\n",
      "식품안전나라 스키마:\n",
      "root\n",
      " |-- 번호: long (nullable = true)\n",
      " |-- 인허가번호: long (nullable = true)\n",
      " |-- 업체명: string (nullable = true)\n",
      " |-- 업종: string (nullable = true)\n",
      " |-- 대표자: string (nullable = true)\n",
      " |-- 소재지: string (nullable = true)\n",
      " |-- 인허가기관: string (nullable = true)\n",
      " |-- 영업상태: string (nullable = true)\n",
      " |-- 비고: double (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 1) Building\n",
    "building_df = spark.read.option(\"mergeSchema\", \"false\").parquet(building_path)\n",
    "building_gg_df = building_df.filter(F.col(\"region\") == REGION_SHORT)\n",
    "#if SIGUNGU_CODE:\n",
    "    #building_gg_df = building_gg_df.filter(F.col(\"시군구_코드\").startswith(SIGUNGU_CODE))\n",
    "\n",
    "print(\"건축물대장 스키마:\")\n",
    "building_df.printSchema()\n",
    "\n",
    "\n",
    "# 2) Toji\n",
    "toji_df = spark.read.option(\"mergeSchema\", \"false\").parquet(toji_path)\n",
    "toji_gg_df = toji_df.filter(F.col(\"region\") == REGION_SHORT)\n",
    "if SIGUNGU_CODE:\n",
    "    toji_gg_df = toji_gg_df.filter(F.col(\"법정동코드\").startswith(SIGUNGU_CODE))\n",
    "\n",
    "print(\"토지소유정보 스키마:\")\n",
    "toji_df.printSchema()\n",
    "\n",
    "print(\"Distinct 지목 리스트:\")\n",
    "toji_df.select(\"지목\").distinct().orderBy(\"지목\").show(1000, truncate=False)\n",
    "\n",
    "print(\"식품안전나라 스키마:\")\n",
    "# 3) Owner\n",
    "owner_df = spark.read.option(\"mergeSchema\", \"false\").parquet(rest_owner_path)\n",
    "owner_gg_df = owner_df.filter((F.col(\"region\") == REGION_SHORT) & F.col(\"소재지\").startswith(f'{REGION} {SIGUNGU}'))\n",
    "\n",
    "owner_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af2758-e0d6-4c07-bd85-3fd1e4358302",
   "metadata": {},
   "source": [
    "# Building Pruning (PNU 고유번호 생성 + 필요한 컬럼만 select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e168a0e-3ed6-4efc-a6a4-b76b1963fe52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 관리_건축물대장_PK: string (nullable = true)\n",
      " |-- 고유번호: string (nullable = true)\n",
      " |-- 옥외자주식면적: string (nullable = true)\n",
      " |-- 대장_구분_코드: string (nullable = true)\n",
      "\n",
      "전체 건축물대장 row 수: 1324795\n",
      "+------------------+-------------------+--------------+--------------+\n",
      "|관리_건축물대장_PK|고유번호           |옥외자주식면적|대장_구분_코드|\n",
      "+------------------+-------------------+--------------+--------------+\n",
      "|1099110746        |4122010100108840002|0             |2             |\n",
      "|109918059         |4122025930103930014|0             |1             |\n",
      "|1099111239        |4122010300104390000|0             |1             |\n",
      "|10991100508032    |4122037029100160002|27.5          |1             |\n",
      "|1099110636        |4122010100201370000|0             |1             |\n",
      "+------------------+-------------------+--------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "building_gg_df = building_gg_df.withColumn(\n",
    "    \"고유번호\",\n",
    "    F.concat(\n",
    "        F.col(\"시군구_코드\"),        # 5\n",
    "        F.col(\"법정동_코드\"),        # 5\n",
    "        F.when(F.col(\"대지_구분_코드\") == \"0\", F.lit(\"1\"))  # 대지 -> 1\n",
    "         .when(F.col(\"대지_구분_코드\") == \"1\", F.lit(\"2\"))  # 산   -> 2\n",
    "         .otherwise(F.col(\"대지_구분_코드\")),\n",
    "        F.lpad(F.col(\"번\"), 4, \"0\"), # 본번\n",
    "        F.lpad(F.col(\"지\"), 4, \"0\")  # 부번\n",
    "    )\n",
    ")\n",
    "\n",
    "building_gg_df = (\n",
    "    building_gg_df\n",
    "    .select(\n",
    "        F.col(\"관리_건축물대장_PK\"),\n",
    "        F.col(\"고유번호\"),\n",
    "        F.col(\"옥외_자주식_면적(㎡)\").alias(\"옥외자주식면적\"),\n",
    "        F.col(\"대장_구분_코드\")\n",
    "    )\n",
    ")\n",
    "\n",
    "building_gg_df.printSchema()\n",
    "print(f\"전체 건축물대장 row 수: {building_gg_df.count()}\")\n",
    "building_gg_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd49500-d317-44ed-ad2e-07892e481076",
   "metadata": {},
   "source": [
    "# Toji Pruning (개인 소유, 가장 최근 소유권 변동 정보만 필터링)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edd8a5d7-e683-4051-a887-1b31c1474d2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 토지 수: 944829\n",
      "root\n",
      " |-- 고유번호: string (nullable = true)\n",
      " |-- 법정동명: string (nullable = true)\n",
      " |-- 지번: string (nullable = true)\n",
      " |-- 소유권변동일자: string (nullable = true)\n",
      " |-- 토지면적: double (nullable = true)\n",
      " |-- 지목: string (nullable = true)\n",
      " |-- 본번: string (nullable = true)\n",
      " |-- 부번: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필터링 후 토지 수: 198560 (21.02%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+--------------+--------+----+----+----+\n",
      "|고유번호           |법정동명            |지번 |소유권변동일자|토지면적|지목|본번|부번|\n",
      "+-------------------+--------------------+-----+--------------+--------+----+----+----+\n",
      "|4122010100100020001|경기도 평택시 서정동|2-1  |2014-02-13    |194.0   |대  |2   |1   |\n",
      "|4122010100100020006|경기도 평택시 서정동|2-6  |2023-05-10    |35.36   |대  |2   |6   |\n",
      "|4122010100100020008|경기도 평택시 서정동|2-8  |2022-05-23    |46.3    |대  |2   |8   |\n",
      "|4122010100100020026|경기도 평택시 서정동|2-26 |2013-09-02    |23.74   |대  |2   |26  |\n",
      "|4122010100100020079|경기도 평택시 서정동|2-79 |2021-12-17    |331.0   |임야|2   |79  |\n",
      "|4122010100100020084|경기도 평택시 서정동|2-84 |2018-10-31    |331.0   |임야|2   |84  |\n",
      "|4122010100100020086|경기도 평택시 서정동|2-86 |2024-11-27    |329.0   |대  |2   |86  |\n",
      "|4122010100100200001|경기도 평택시 서정동|20-1 |2025-03-21    |24.58   |대  |20  |1   |\n",
      "|4122010100100200040|경기도 평택시 서정동|20-40|2023-07-19    |13.93   |대  |20  |40  |\n",
      "|4122010100100200041|경기도 평택시 서정동|20-41|2023-07-19    |15.27   |대  |20  |41  |\n",
      "+-------------------+--------------------+-----+--------------+--------+----+----+----+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "all_count = toji_gg_df.count()\n",
    "print(f\"전체 토지 수: {all_count}\")\n",
    "\n",
    "toji_gg_df = (\n",
    "    toji_gg_df\n",
    "    # 1) 조건 필터\n",
    "    #.filter(F.col(\"공유인수\") == 0)\n",
    "    .filter(F.col(\"소유구분코드\") == \"01\")\n",
    "    # 2) 필요한 컬럼만 선택\n",
    "    .select(\n",
    "        F.col(\"고유번호\").cast(\"string\"),\n",
    "        F.col(\"법정동명\"),\n",
    "        F.col(\"지번\"),\n",
    "        F.col(\"소유권변동일자\"),\n",
    "        F.col(\"토지면적\"),\n",
    "        F.col(\"지목\"),\n",
    "    )\n",
    "\n",
    "    # 3) 날짜 파싱\n",
    "    .withColumn(\n",
    "        \"소유권변동일자_dt\",\n",
    "        F.to_date(F.col(\"소유권변동일자\"), \"yyyy-MM-dd\")\n",
    "    )\n",
    "\n",
    "    # 4) 고유번호별 최신 1행\n",
    "    .withColumn(\n",
    "        \"rn\",\n",
    "        F.row_number().over(\n",
    "            Window.partitionBy(\"고유번호\")\n",
    "                  .orderBy(F.col(\"소유권변동일자_dt\").desc_nulls_last())\n",
    "        )\n",
    "    )\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\", \"소유권변동일자_dt\")\n",
    "\n",
    "    # 5) 지번 분리\n",
    "    .withColumn(\n",
    "        \"본번\",\n",
    "        F.split(F.col(\"지번\"), \"-\").getItem(0)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"부번\",\n",
    "        F.when(\n",
    "            F.size(F.split(F.col(\"지번\"), \"-\")) > 1,\n",
    "            F.split(F.col(\"지번\"), \"-\").getItem(1)\n",
    "        ).otherwise(F.lit(None))\n",
    "    )\n",
    ")\n",
    "\n",
    "# 확인\n",
    "toji_gg_df.printSchema()\n",
    "print(f\"필터링 후 토지 수: {toji_gg_df.count()} ({toji_gg_df.count() / all_count * 100:.2f}%)\")\n",
    "toji_gg_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af459c8-50fe-43d2-a453-0d4fa9c795c3",
   "metadata": {},
   "source": [
    "# Restaurant Pruning (필요한 컬럼만 + 중복 제거)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "698ea479-ef66-4c2e-9504-9abd957be01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "owner (업체명, 소재지) 중복 제거 후: 11026 (89.24%)\n",
      "+-----------------------------------+-------------------------+------------------------------------------------------------------------------+\n",
      "|업체명                             |대표자                   |소재지_정제                                                                   |\n",
      "+-----------------------------------+-------------------------+------------------------------------------------------------------------------+\n",
      "|(DNO)디엔오                        |오*석                    |경기도 평택시 세교6로 12-47                                                   |\n",
      "|(The)착한                          |김*선                    |경기도 평택시 서동대로 2043                                                   |\n",
      "|(돈야우야)모산골                   |한*숙                    |경기도 평택시 동삭1로22번길 19-6                                              |\n",
      "|(신)열날개                         |성*영                    |경기도 평택시 함박산8길 7                                                     |\n",
      "|(유)아웃백스테이크하우스 평택역사점|S*NG DAVID HOSUP (송호섭)|경기도 평택시 평택로 51                                                       |\n",
      "|(유한)애플이엔씨 평택화양2차       |최*식                    |경기도 평택시 안중읍 현화리 770 평택화양지구도시개발사업 A3BL 외 1필지 1동 1층|\n",
      "|(주) 아워홈 엘지전자 평택연수원    |김*원                    |경기도 평택시 엘지로 222                                                      |\n",
      "|(주)경기보훈협의회사업단           |김*제 외 1명             |경기도 평택시 안중읍 현화리 산 397-2 화양지구 23블럭-2                        |\n",
      "|(주)경동나비엔                     |김*욱                    |경기도 평택시 경기대로 663                                                    |\n",
      "|(주)경동나비엔서탄공장             |김*욱                    |경기도 평택시 수월암길 95                                                     |\n",
      "+-----------------------------------+-------------------------+------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Restaurant pruning\n",
    "# =========================\n",
    "# =========================\n",
    "# Restaurant pruning\n",
    "# =========================\n",
    "owner_gg_df_clean = (\n",
    "    owner_gg_df\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1. 괄호 제거\n",
    "    # -----------------------------\n",
    "    .withColumn(\n",
    "        \"소재지_tmp\",\n",
    "        F.regexp_replace(F.col(\"소재지\"), r\"\\s*\\([^)]*\\)\", \"\")\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2. 쉼표 뒤 제거\n",
    "    # -----------------------------\n",
    "    .withColumn(\n",
    "        \"소재지_tmp\",\n",
    "        F.regexp_replace(F.col(\"소재지_tmp\"), r\",.*$\", \"\")\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3. 도로명 앞 행정동 제거\n",
    "    # (고림동 고림로 123 → 고림로 123)\n",
    "    # -----------------------------\n",
    "    .withColumn(\n",
    "        \"소재지_tmp\",\n",
    "        F.regexp_replace(\n",
    "            F.col(\"소재지_tmp\"),\n",
    "            r'\\s+\\S+(동|읍|면|리)\\s+(?=\\S+(로|길))',\n",
    "            ' '\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. 공백 정리\n",
    "    # -----------------------------\n",
    "    .withColumn(\n",
    "        \"소재지_정제\",\n",
    "        F.trim(F.regexp_replace(F.col(\"소재지_tmp\"), r'\\s+', ' '))\n",
    "    )\n",
    "\n",
    "    .select(\n",
    "        F.col(\"업체명\"),\n",
    "        F.col(\"대표자\"),\n",
    "        F.col(\"소재지_정제\"),\n",
    "    )\n",
    "    .filter(F.col(\"업체명\").isNotNull())\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Owner 중복 제거\n",
    "# =========================\n",
    "all_count = owner_gg_df_clean.count()\n",
    "\n",
    "owner_gg_df_clean = owner_gg_df_clean.dropDuplicates([\"업체명\", \"소재지_정제\"])\n",
    "\n",
    "print(f\"owner (업체명, 소재지) 중복 제거 후: {owner_gg_df_clean.count()} ({owner_gg_df_clean.count() / all_count * 100:.2f}%)\")\n",
    "\n",
    "owner_gg_df_clean.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333a5b9-fd4d-487b-9d27-a775278785c6",
   "metadata": {},
   "source": [
    "# Restaurant + Address Left Join (도로명주소 -> 법정동코드 컬럼 추가)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76637e16-6a01-4514-a765-6ffb0edd2add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 업체명: string (nullable = true)\n",
      " |-- 대표자: string (nullable = true)\n",
      " |-- 소재지_정제: string (nullable = true)\n",
      " |-- PNU코드: string (nullable = true)\n",
      "\n",
      "owner 도로명주소 매칭 후: 10912 (98.97%)\n",
      "+-----------------------------------+-------------------------+--------------------------------+-------------------+\n",
      "|업체명                             |대표자                   |소재지_정제                     |PNU코드            |\n",
      "+-----------------------------------+-------------------------+--------------------------------+-------------------+\n",
      "|(DNO)디엔오                        |오*석                    |경기도 평택시 세교6로 12-47     |4122012000105840010|\n",
      "|(The)착한                          |김*선                    |경기도 평택시 서동대로 2043     |4122034026100010002|\n",
      "|(돈야우야)모산골                   |한*숙                    |경기도 평택시 동삭1로22번길 19-6|4122011900100000000|\n",
      "|(신)열날개                         |성*영                    |경기도 평택시 함박산8길 7       |4122012800126580004|\n",
      "|(유)아웃백스테이크하우스 평택역사점|S*NG DAVID HOSUP (송호섭)|경기도 평택시 평택로 51         |4122011300101850579|\n",
      "|(주) 아워홈 엘지전자 평택연수원    |김*원                    |경기도 평택시 엘지로 222        |4122031026100010000|\n",
      "|(주)경동나비엔                     |김*욱                    |경기도 평택시 경기대로 663      |4122012000104370003|\n",
      "|(주)경동나비엔서탄공장             |김*욱                    |경기도 평택시 수월암길 95       |4122032023120220000|\n",
      "|(주)고기배달                       |신*애                    |경기도 평택시 송탄로 247        |4122010100108660001|\n",
      "|(주)고메드갤러리아 AK플라자평택점  |차*팔                    |경기도 평택시 평택로 51         |4122011300101850579|\n",
      "+-----------------------------------+-------------------------+--------------------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "매칭 실패 개수: 114\n",
      "+----------------------------------+------------+------------------------------------------------------------------------------+-------+----------+\n",
      "|업체명                            |대표자      |소재지_정제                                                                   |PNU코드|도로명주소|\n",
      "+----------------------------------+------------+------------------------------------------------------------------------------+-------+----------+\n",
      "|(유한)애플이엔씨 평택화양2차      |최*식       |경기도 평택시 안중읍 현화리 770 평택화양지구도시개발사업 A3BL 외 1필지 1동 1층|NULL   |NULL      |\n",
      "|(주)경기보훈협의회사업단          |김*제 외 1명|경기도 평택시 안중읍 현화리 산 397-2 화양지구 23블럭-2                        |NULL   |NULL      |\n",
      "|(주)광원목재                      |정*진       |경기도 평택시 평택항만길 425                                                  |NULL   |NULL      |\n",
      "|(주)광진 평택점(광혁건설 현장식당)|장*철       |경기도 평택시 도일동 239-1 외3필지 1동                                        |NULL   |NULL      |\n",
      "|(주)에이치케이푸드시스템          |이*기       |경기도 평택시 가재동 345-8 외 5필지                                           |NULL   |NULL      |\n",
      "|(주)케이더블유기우산업            |이*순       |경기도 평택시 문곡3길 115                                                     |NULL   |NULL      |\n",
      "|(주)특수건설현장식당              |김*헌       |경기도 평택시 독곡동 227 외 1필지 3동                                         |NULL   |NULL      |\n",
      "|(주)포세이돈커피                  |이*헌       |경기도 평택시 포승읍 원정리 1279 1층                                          |NULL   |NULL      |\n",
      "|D카페테리아 한식당                |윤*분       |경기도 평택시 도일동 995-5 1층                                                |NULL   |NULL      |\n",
      "|Mr.팔봉이쿡                       |정*희       |경기도 평택시 안중읍 안중리 268-5                                             |NULL   |NULL      |\n",
      "|경기도립노인전문평택병원          |이*수       |경기도 평택시 삼남로 312                                                      |NULL   |NULL      |\n",
      "|고향왕만두                        |송*영       |경기도 평택시 안중로 134                                                      |NULL   |NULL      |\n",
      "|교동분식                          |심*유       |경기도 평택시 포승읍 원정리 1283 사병회관 1층                                 |NULL   |NULL      |\n",
      "|그리핀(griffin)                   |전*채 외 1명|경기도 평택시 팽성읍 안정리 146-6                                             |NULL   |NULL      |\n",
      "|김밥나라송탄지산점                |정*민       |경기도 평택시 지산동 743-4                                                    |NULL   |NULL      |\n",
      "|김영희 강남동태찜.탕.포승점       |손*수       |경기도 평택시 포승읍 원정리 352-5 외2필지                                     |NULL   |NULL      |\n",
      "|꼬끼또(Coquito)                   |권*혜       |경기도 평택시 안정쇼핑로 14-1                                                 |NULL   |NULL      |\n",
      "|너랑나랑                          |엄*희       |경기도 평택시 서정역로26번길 2                                                |NULL   |NULL      |\n",
      "|달레나순대국                      |민*원       |경기도 평택시 청원로 1453                                                     |NULL   |NULL      |\n",
      "|대야산업(주) 현장식당(백년집밥)   |김*은       |경기도 평택시 고덕면 동고리 379-7 1동 1층                                     |NULL   |NULL      |\n",
      "+----------------------------------+------------+------------------------------------------------------------------------------+-------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "o = owner_gg_df_clean.alias(\"o\")\n",
    "a = address_filtered_df.alias(\"a\")\n",
    "\n",
    "restaurant_address_df = (\n",
    "    o.join(\n",
    "        a,\n",
    "        F.col(\"o.소재지_정제\") == F.col(\"a.도로명주소\"),\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .filter(F.col(\"도로명주소\").isNotNull())\n",
    "    .drop(\"도로명주소\")\n",
    ")\n",
    "\n",
    "restaurant_address_df.printSchema()\n",
    "print(f\"owner 도로명주소 매칭 후: {restaurant_address_df.count()} ({restaurant_address_df.count() / owner_gg_df_clean.count() * 100:.2f}%)\")\n",
    "restaurant_address_df.show(10, truncate=False)\n",
    "\n",
    "o = owner_gg_df_clean.alias(\"o\")\n",
    "a = address_filtered_df.alias(\"a\")\n",
    "\n",
    "unmatched_df = (\n",
    "    o.join(\n",
    "        a,\n",
    "        F.col(\"o.소재지_정제\") == F.col(\"a.도로명주소\"),\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .filter(F.col(\"a.도로명주소\").isNull())   # ← 매칭 실패만\n",
    ")\n",
    "\n",
    "print(\"매칭 실패 개수:\", unmatched_df.count())\n",
    "unmatched_df.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7053f-a433-43ba-b094-c25f6e3d083d",
   "metadata": {},
   "source": [
    "# 토지 필터링\n",
    "1. 토지에 건물이 없거나\n",
    "2. 건물이 1개 있고, 일반 건물이며, 음식점이 있는 토지만 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15d966df-a903-4338-ab9d-f07162ceca0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 고유번호: string (nullable = true)\n",
      " |-- 법정동명: string (nullable = true)\n",
      " |-- 지번: string (nullable = true)\n",
      " |-- 소유권변동일자: string (nullable = true)\n",
      " |-- 토지면적: double (nullable = true)\n",
      " |-- 지목: string (nullable = true)\n",
      " |-- 본번: string (nullable = true)\n",
      " |-- 부번: string (nullable = true)\n",
      " |-- 관리_건축물대장_PK: string (nullable = true)\n",
      " |-- 옥외자주식면적: string (nullable = true)\n",
      " |-- 대장_구분_코드: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 101:============>    (3 + 1) / 4][Stage 102:======>          (2 + 3) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+----+--------------+--------+----+----+----+------------------+--------------+--------------+\n",
      "|고유번호           |법정동명            |지번|소유권변동일자|토지면적|지목|본번|부번|관리_건축물대장_PK|옥외자주식면적|대장_구분_코드|\n",
      "+-------------------+--------------------+----+--------------+--------+----+----+----+------------------+--------------+--------------+\n",
      "|4122010100100020001|경기도 평택시 서정동|2-1 |2014-02-13    |194.0   |대  |2   |1   |10991100179000    |0             |1             |\n",
      "|4122010100100020006|경기도 평택시 서정동|2-6 |2023-05-10    |35.36   |대  |2   |6   |1099125856        |81.5          |2             |\n",
      "|4122010100100020008|경기도 평택시 서정동|2-8 |2022-05-23    |46.3    |대  |2   |8   |1099152176        |80.5          |2             |\n",
      "|4122010100100020026|경기도 평택시 서정동|2-26|2013-09-02    |23.74   |대  |2   |26  |10991100244732    |24            |2             |\n",
      "|4122010100100020026|경기도 평택시 서정동|2-26|2013-09-02    |23.74   |대  |2   |26  |10991100244718    |24            |2             |\n",
      "+-------------------+--------------------+----+--------------+--------+----+----+----+------------------+--------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "t = toji_gg_df.alias(\"t\")\n",
    "b = building_gg_df.alias(\"b\")\n",
    "\n",
    "toji_building_gg_df = (\n",
    "    t.join(b, F.col(\"t.고유번호\") == F.col(\"b.고유번호\"), \"left\")\n",
    "     .drop(F.col(\"b.고유번호\"))\n",
    ")\n",
    "\n",
    "toji_building_gg_df.printSchema()\n",
    "print(toji_building_gg_df.count())\n",
    "toji_building_gg_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1828ea4-492f-486e-a71e-a040340ad948",
   "metadata": {},
   "source": [
    "### 건물이 1개 이하인 토지 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "742bf6d5-a652-4c15-bd99-dba50b402a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 121:============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+----+--------------+--------+----+----+----+------------------+--------------+--------------+\n",
      "|고유번호           |법정동명            |지번|소유권변동일자|토지면적|지목|본번|부번|관리_건축물대장_PK|옥외자주식면적|대장_구분_코드|\n",
      "+-------------------+--------------------+----+--------------+--------+----+----+----+------------------+--------------+--------------+\n",
      "|4122010100100020001|경기도 평택시 서정동|2-1 |2014-02-13    |194.0   |대  |2   |1   |10991100179000    |0             |1             |\n",
      "|4122010100100020006|경기도 평택시 서정동|2-6 |2023-05-10    |35.36   |대  |2   |6   |1099125856        |81.5          |2             |\n",
      "|4122010100100020008|경기도 평택시 서정동|2-8 |2022-05-23    |46.3    |대  |2   |8   |1099152176        |80.5          |2             |\n",
      "|4122010100100020079|경기도 평택시 서정동|2-79|2021-12-17    |331.0   |임야|2   |79  |NULL              |NULL          |NULL          |\n",
      "|4122010100100020084|경기도 평택시 서정동|2-84|2018-10-31    |331.0   |임야|2   |84  |NULL              |NULL          |NULL          |\n",
      "+-------------------+--------------------+----+--------------+--------+----+----+----+------------------+--------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "pk_cnt_df = (\n",
    "    toji_building_gg_df\n",
    "    .groupBy(\"고유번호\")\n",
    "    .agg(F.count(\"관리_건축물대장_PK\").alias(\"pk_cnt\"))\n",
    ")\n",
    "\n",
    "toji_binary_building_gg_df = (\n",
    "    toji_building_gg_df\n",
    "    .join(\n",
    "        pk_cnt_df.filter(F.col(\"pk_cnt\") <= 1),\n",
    "        on=\"고유번호\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .drop(\"pk_cnt\")\n",
    ")\n",
    "\n",
    "print(toji_binary_building_gg_df.count())\n",
    "toji_binary_building_gg_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3e1c04a-eea5-4a73-8786-22e9d7ce9821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 131:>                                                        (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|건물개수| count|\n",
      "+--------+------+\n",
      "|       0|151328|\n",
      "|       1| 41546|\n",
      "+--------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "toji_binary_building_gg_df.withColumn(\n",
    "    \"건물개수\",\n",
    "    F.when(F.col(\"관리_건축물대장_PK\").isNull(), F.lit(0))\n",
    "     .otherwise(F.lit(1))\n",
    ").groupBy(\"건물개수\").count().orderBy(\"건물개수\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea7f2f8-a5f3-4327-a5e2-41bef42b452d",
   "metadata": {},
   "source": [
    "### 건물이 1개 있고, 일반 건물이며, 음식점이 포함된 토지 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1389a0fc-8145-4511-9c52-7ed08bb8b362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/18 14:56:35 ERROR TaskSchedulerImpl: Lost executor 1 on 172.22.0.3: Command exited with code 137\n",
      "26/02/18 14:56:35 WARN TaskSetManager: Lost task 2.0 in stage 138.0 (TID 220) (172.22.0.3 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "26/02/18 14:56:35 WARN TaskSetManager: Lost task 3.0 in stage 138.0 (TID 221) (172.22.0.3 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "26/02/18 14:56:35 WARN TaskSetManager: Lost task 0.0 in stage 140.0 (TID 225) (172.22.0.4 executor 0): FetchFailed(null, shuffleId=42, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 42 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1781)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1726)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1725)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1313)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1725)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1359)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1321)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:135)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:67)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:61)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      ")\n",
      "26/02/18 14:56:35 WARN TaskSetManager: Lost task 1.0 in stage 140.0 (TID 226) (172.22.0.4 executor 0): FetchFailed(null, shuffleId=42, mapIndex=-1, mapId=-1, reduceId=121, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 42 partition 121\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1781)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1726)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1725)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1313)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1725)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1359)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1321)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:135)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:67)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:61)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      ")\n",
      "[Stage 144:>                                                        (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "건물 1개인 토지 수: 41546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 건물이 1개 있는 토지만 필터링\n",
    "toji_with_1_building_gg_df = (\n",
    "    toji_binary_building_gg_df\n",
    "    .filter(F.col(\"관리_건축물대장_PK\").isNotNull())\n",
    ")\n",
    "\n",
    "print(\"건물 1개인 토지 수:\", toji_with_1_building_gg_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14cdab92-e04d-41ec-9efb-82571b5c5248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "건물이 1개이고, 음식점이 있는 토지: 7148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+--------------+--------+----+----+----+------------------+--------------+--------------+--------------+------------+--------------------------------+\n",
      "|           고유번호|            법정동명| 지번|소유권변동일자|토지면적|지목|본번|부번|관리_건축물대장_PK|옥외자주식면적|대장_구분_코드|        업체명|      대표자|                     소재지_정제|\n",
      "+-------------------+--------------------+-----+--------------+--------+----+----+----+------------------+--------------+--------------+--------------+------------+--------------------------------+\n",
      "|4122010100100020001|경기도 평택시 서정동|  2-1|    2014-02-13|   194.0|  대|   2|   1|    10991100179000|             0|             1|   송탄 왕오리|       안*덕| 경기도 평택시 경기대로1376번...|\n",
      "|4122010100100510015|경기도 평택시 서정동|51-15|    2022-03-30|    6.77|  대|  51|  15|    10991100389571|             0|             2|      포레스트|       송*희|  경기도 평택시 송탄2로19번길 44|\n",
      "|4122010100100760021|경기도 평택시 서정동|76-21|    2017-08-11|    88.0|  대|  76|  21|        1099141241|             0|             1|        맛이야|       정*분|  경기도 평택시 송탄2로47번길 61|\n",
      "|4122010100102630001|경기도 평택시 서정동|263-1|    2025-09-24|     7.0|  대| 263|   1|    10991100387090|        401.86|             2|        똥상피|전*민 외 1명|경기도 평택시 서두물로35번길 113|\n",
      "|4122010100102630001|경기도 평택시 서정동|263-1|    2025-09-24|     7.0|  대| 263|   1|    10991100387090|        401.86|             2|GS25파크리움점|       김*야|경기도 평택시 서두물로35번길 113|\n",
      "+-------------------+--------------------+-----+--------------+--------+----+----+----+------------------+--------------+--------------+--------------+------------+--------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 건물 있는데 음식점 아닌거 거르려고 만드는 join table\n",
    "\n",
    "t = toji_with_1_building_gg_df.alias(\"t\")\n",
    "r = restaurant_address_df.alias(\"r\")\n",
    "\n",
    "toji_building_restaurant_gg_df = (\n",
    "    t.join(\n",
    "        r,\n",
    "        F.col(\"t.고유번호\") == F.col(\"r.PNU코드\"),\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .drop(F.col(\"r.PNU코드\"))\n",
    "    .filter(F.col(\"업체명\").isNotNull())\n",
    ")\n",
    "\n",
    "print(\"건물이 1개이고, 음식점이 있는 토지:\", toji_building_restaurant_gg_df.count())\n",
    "toji_building_restaurant_gg_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f937eef-35f5-4532-a8d0-5145e16e629d",
   "metadata": {},
   "source": [
    "### 건물 0개 + 건물 1개 (일반건물, 음식점있음) 토지 concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4e0e506-84b0-41a1-8cb8-5a847b24f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아무 건물도 없는 필지\n",
    "toji_with_0_building_gg_df = (\n",
    "    toji_binary_building_gg_df\n",
    "    .filter(F.col(\"관리_건축물대장_PK\").isNull())\n",
    ")\n",
    "\n",
    "# 위에서 만든 join table과 concat을 위해 column 추가\n",
    "toji_with_0_building_gg_df = (\n",
    "    toji_with_0_building_gg_df\n",
    "    .withColumn(\"업체명\", F.lit(None).cast(\"string\"))\n",
    "    .withColumn(\"대표자\", F.lit(None).cast(\"string\"))\n",
    "    .withColumn(\"소재지_정제\", F.lit(None).cast(\"string\"))\n",
    ")\n",
    "\n",
    "final_toji_df = (\n",
    "    toji_building_restaurant_gg_df\n",
    "    .unionByName(toji_with_0_building_gg_df)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c708edd6-e739-4a92-9643-8438a9e6dd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final: 158476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "건물1+음식점: 7148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "건물0: 151328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|건물개수| count|\n",
      "+--------+------+\n",
      "|       0|151328|\n",
      "|       1|  7148|\n",
      "+--------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 272:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+--------------+--------+----+----+----+------------------+--------------+--------------+---------------------------------+------+-------------------------------+\n",
      "|           고유번호|            법정동명| 지번|소유권변동일자|토지면적|지목|본번|부번|관리_건축물대장_PK|옥외자주식면적|대장_구분_코드|                           업체명|대표자|                    소재지_정제|\n",
      "+-------------------+--------------------+-----+--------------+--------+----+----+----+------------------+--------------+--------------+---------------------------------+------+-------------------------------+\n",
      "|4122010100100010000|경기도 평택시 서정동|    1|    2020-05-22|   125.0|  대|   1|NULL|    10991100207237|             0|             1|                       서정동책방| 김*숙|경기도 평택시 경기대로1376번...|\n",
      "|4122010100100020001|경기도 평택시 서정동|  2-1|    2014-02-13|   194.0|  대|   2|   1|    10991100179000|             0|             1|                      송탄 왕오리| 안*덕|경기도 평택시 경기대로1376번...|\n",
      "|4122010100100480000|경기도 평택시 서정동|   48|    2025-06-30|   12.85|  대|  48|NULL|    10991100343405|             0|             2|      씨유 평택서정테헤란로V1호점| 황*남| 경기도 평택시 송탄2로31번길 23|\n",
      "|4122010100100510015|경기도 평택시 서정동|51-15|    2022-03-30|    6.77|  대|  51|  15|    10991100389571|             0|             2|                         포레스트| 송*희| 경기도 평택시 송탄2로19번길 44|\n",
      "|4122010100100620003|경기도 평택시 서정동| 62-3|    2011-11-17|    83.0|  대|  62|   3|        1099139340|             0|             1|달롱도르 요거트 아이스크림 송탄점|   이*|    경기도 평택시 송탄2로 20-12|\n",
      "+-------------------+--------------------+-----+--------------+--------+----+----+----+------------------+--------------+--------------+---------------------------------+------+-------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 전체 row 수 = (건물1+음식점) + (건물0)\n",
    "print(\"final:\", final_toji_df.count())\n",
    "print(\"건물1+음식점:\", toji_building_restaurant_gg_df.count())\n",
    "print(\"건물0:\", toji_with_0_building_gg_df.count())\n",
    "\n",
    "# 건물 개수 분포\n",
    "final_toji_df.withColumn(\n",
    "    \"건물개수\",\n",
    "    F.when(F.col(\"관리_건축물대장_PK\").isNull(), 0).otherwise(1)\n",
    ").groupBy(\"건물개수\").count().orderBy(\"건물개수\").show()\n",
    "\n",
    "final_toji_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c400e516-959b-4006-9f4f-119fc7404cb0",
   "metadata": {},
   "source": [
    "# 같은 본번을 가진 토지 그룹핑 -> 음식점이 있는 그룹만 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65c9b815-5cfd-4c4d-af24-440002021707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/18 14:57:22 ERROR TaskSchedulerImpl: Lost executor 0 on 172.22.0.4: Command exited with code 137\n",
      "26/02/18 14:57:22 WARN TaskSetManager: Lost task 2.0 in stage 294.0 (TID 475) (172.22.0.4 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "26/02/18 14:57:22 WARN TaskSetManager: Lost task 3.0 in stage 294.0 (TID 476) (172.22.0.4 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "26/02/18 14:57:22 WARN TaskSetManager: Lost task 3.1 in stage 294.0 (TID 477) (172.22.0.3 executor 2): FetchFailed(BlockManagerId(0, 172.22.0.4, 38609, None), shuffleId=90, mapIndex=1, mapId=454, reduceId=199, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:426)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1261)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:988)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:604)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:618)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage51.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage51.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.fetchNextRow(WindowGroupLimitExec.scala:213)\n",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.<init>(WindowGroupLimitExec.scala:219)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory.$anonfun$createEvaluator$4(WindowGroupLimitEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory$WindowGroupLimitPartitionEvaluator.eval(WindowGroupLimitEvaluatorFactory.scala:60)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1(WindowGroupLimitExec.scala:89)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1$adapted(WindowGroupLimitExec.scala:87)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:108)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: [INTERNAL_ERROR_NETWORK] The relative remote executor(Id: 0), which maintains the block data to fetch is dead. SQLSTATE: XX000\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:146)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:180)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:159)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:157)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:383)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1228)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1220)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.$anonfun$initialize$2(ShuffleBlockFetcherIterator.scala:729)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.withFetchWaitTimeTracked(ShuffleBlockFetcherIterator.scala:198)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:729)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:194)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:232)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\t... 39 more\n",
      "\n",
      ")\n",
      "26/02/18 14:57:22 WARN TaskSetManager: Lost task 2.1 in stage 294.0 (TID 478) (172.22.0.3 executor 2): FetchFailed(BlockManagerId(0, 172.22.0.4, 38609, None), shuffleId=90, mapIndex=1, mapId=454, reduceId=0, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:426)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1261)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:988)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:604)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:618)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage51.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage51.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.fetchNextRow(WindowGroupLimitExec.scala:213)\n",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.<init>(WindowGroupLimitExec.scala:219)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory.$anonfun$createEvaluator$4(WindowGroupLimitEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory$WindowGroupLimitPartitionEvaluator.eval(WindowGroupLimitEvaluatorFactory.scala:60)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1(WindowGroupLimitExec.scala:89)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1$adapted(WindowGroupLimitExec.scala:87)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:108)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: [INTERNAL_ERROR_NETWORK] The relative remote executor(Id: 0), which maintains the block data to fetch is dead. SQLSTATE: XX000\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:146)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:180)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:159)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:157)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:383)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1228)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1220)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.$anonfun$initialize$2(ShuffleBlockFetcherIterator.scala:729)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.withFetchWaitTimeTracked(ShuffleBlockFetcherIterator.scala:198)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:729)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:194)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:232)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\t... 39 more\n",
      "\n",
      ")\n",
      "26/02/18 14:57:22 WARN TaskSetManager: Lost task 0.0 in stage 296.0 (TID 479) (172.22.0.3 executor 2): FetchFailed(null, shuffleId=93, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 93 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1781)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1726)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1725)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1313)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1725)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1359)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1321)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:135)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:67)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:61)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      ")\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필터 후 row 수: 19352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/18 14:57:40 ERROR TaskSchedulerImpl: Lost executor 2 on 172.22.0.3: Command exited with code 137\n",
      "26/02/18 14:57:40 WARN TaskSetManager: Lost task 0.0 in stage 338.0 (TID 550) (172.22.0.3 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "26/02/18 14:57:40 WARN TaskSetManager: Lost task 1.0 in stage 338.0 (TID 551) (172.22.0.3 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "26/02/18 14:57:41 WARN TaskSetManager: Lost task 2.1 in stage 334.0 (TID 553) (172.22.0.4 executor 3): FetchFailed(BlockManagerId(2, 172.22.0.3, 38721, None), shuffleId=101, mapIndex=0, mapId=521, reduceId=0, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:426)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1261)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:988)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:604)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:618)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage51.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage51.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.fetchNextRow(WindowGroupLimitExec.scala:213)\n",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.<init>(WindowGroupLimitExec.scala:219)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory.$anonfun$createEvaluator$4(WindowGroupLimitEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory$WindowGroupLimitPartitionEvaluator.eval(WindowGroupLimitEvaluatorFactory.scala:60)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1(WindowGroupLimitExec.scala:89)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1$adapted(WindowGroupLimitExec.scala:87)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:108)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: [INTERNAL_ERROR_NETWORK] The relative remote executor(Id: 2), which maintains the block data to fetch is dead. SQLSTATE: XX000\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:146)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:180)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:159)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:157)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:383)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1228)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1220)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.$anonfun$initialize$2(ShuffleBlockFetcherIterator.scala:729)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.withFetchWaitTimeTracked(ShuffleBlockFetcherIterator.scala:198)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:729)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:194)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:232)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\t... 42 more\n",
      "\n",
      ")\n",
      "26/02/18 14:57:41 WARN TaskSetManager: Lost task 0.1 in stage 334.0 (TID 554) (172.22.0.4 executor 3): FetchFailed(BlockManagerId(2, 172.22.0.3, 38721, None), shuffleId=101, mapIndex=0, mapId=521, reduceId=0, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:426)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1261)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:988)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:604)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:618)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage42.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage42.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.fetchNextRow(WindowGroupLimitExec.scala:213)\n",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.<init>(WindowGroupLimitExec.scala:219)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory.$anonfun$createEvaluator$4(WindowGroupLimitEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory$WindowGroupLimitPartitionEvaluator.eval(WindowGroupLimitEvaluatorFactory.scala:60)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1(WindowGroupLimitExec.scala:89)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1$adapted(WindowGroupLimitExec.scala:87)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:108)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: [INTERNAL_ERROR_NETWORK] The relative remote executor(Id: 2), which maintains the block data to fetch is dead. SQLSTATE: XX000\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:146)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:180)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:159)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:157)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:383)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1228)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1220)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.$anonfun$initialize$2(ShuffleBlockFetcherIterator.scala:729)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.withFetchWaitTimeTracked(ShuffleBlockFetcherIterator.scala:198)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:729)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:194)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:232)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\t... 42 more\n",
      "\n",
      ")\n",
      "26/02/18 14:57:41 WARN TaskSetManager: Lost task 1.1 in stage 338.0 (TID 555) (172.22.0.4 executor 3): FetchFailed(BlockManagerId(2, 172.22.0.3, 38721, None), shuffleId=100, mapIndex=0, mapId=517, reduceId=200, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:426)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1261)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:988)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:604)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:618)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage24.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage24.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.fetchNextRow(WindowGroupLimitExec.scala:213)\n",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.<init>(WindowGroupLimitExec.scala:219)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory.$anonfun$createEvaluator$4(WindowGroupLimitEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory$WindowGroupLimitPartitionEvaluator.eval(WindowGroupLimitEvaluatorFactory.scala:60)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1(WindowGroupLimitExec.scala:89)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1$adapted(WindowGroupLimitExec.scala:87)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:108)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: [INTERNAL_ERROR_NETWORK] The relative remote executor(Id: 2), which maintains the block data to fetch is dead. SQLSTATE: XX000\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:146)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:180)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:159)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:157)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:383)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1228)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1220)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.$anonfun$initialize$2(ShuffleBlockFetcherIterator.scala:729)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.withFetchWaitTimeTracked(ShuffleBlockFetcherIterator.scala:198)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:729)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:194)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:232)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\t... 39 more\n",
      "\n",
      ")\n",
      "26/02/18 14:57:41 WARN TaskSetManager: Lost task 0.1 in stage 338.0 (TID 556) (172.22.0.4 executor 3): FetchFailed(BlockManagerId(2, 172.22.0.3, 38721, None), shuffleId=100, mapIndex=0, mapId=517, reduceId=0, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:426)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1261)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:988)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:604)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:618)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage24.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage24.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.fetchNextRow(WindowGroupLimitExec.scala:213)\n",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.<init>(WindowGroupLimitExec.scala:219)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory.$anonfun$createEvaluator$4(WindowGroupLimitEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory$WindowGroupLimitPartitionEvaluator.eval(WindowGroupLimitEvaluatorFactory.scala:60)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1(WindowGroupLimitExec.scala:89)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1$adapted(WindowGroupLimitExec.scala:87)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:108)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: [INTERNAL_ERROR_NETWORK] The relative remote executor(Id: 2), which maintains the block data to fetch is dead. SQLSTATE: XX000\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:146)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:180)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:159)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:157)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:383)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1228)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1220)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.$anonfun$initialize$2(ShuffleBlockFetcherIterator.scala:729)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.withFetchWaitTimeTracked(ShuffleBlockFetcherIterator.scala:198)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:729)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:194)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:232)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\t... 39 more\n",
      "\n",
      ")\n",
      "26/02/18 14:57:45 WARN TaskSetManager: Lost task 2.0 in stage 338.0 (TID 552) (172.22.0.4 executor 3): FetchFailed(BlockManagerId(2, 172.22.0.3, 38721, None), shuffleId=100, mapIndex=0, mapId=517, reduceId=0, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:426)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1261)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:988)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:604)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:618)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage33.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage33.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.fetchNextRow(WindowGroupLimitExec.scala:213)\n",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.<init>(WindowGroupLimitExec.scala:219)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory.$anonfun$createEvaluator$4(WindowGroupLimitEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory$WindowGroupLimitPartitionEvaluator.eval(WindowGroupLimitEvaluatorFactory.scala:60)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1(WindowGroupLimitExec.scala:89)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1$adapted(WindowGroupLimitExec.scala:87)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:108)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: [INTERNAL_ERROR_NETWORK] The relative remote executor(Id: 2), which maintains the block data to fetch is dead. SQLSTATE: XX000\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:146)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:180)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:227)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "\n",
      ")\n",
      "[Stage 330:>  (0 + 0) / 4][Stage 333:>  (0 + 0) / 2][Stage 338:=> (2 + 2) / 4]4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------------+------+--------------+--------+----+----+------------------+--------------+--------------+-----------------------+-------------+------------------------------+\n",
      "|법정동명            |본번|고유번호           |지번  |소유권변동일자|토지면적|지목|부번|관리_건축물대장_PK|옥외자주식면적|대장_구분_코드|업체명                 |대표자       |소재지_정제                   |\n",
      "+--------------------+----+-------------------+------+--------------+--------+----+----+------------------+--------------+--------------+-----------------------+-------------+------------------------------+\n",
      "|경기도 평택시 합정동|964 |4122011700109640007|964-7 |2025-08-26    |257.1   |대  |7   |10991100300950    |0             |1             |투다리 배미점          |최*아        |경기도 평택시 중앙로 264      |\n",
      "|경기도 평택시 합정동|964 |4122011700109640007|964-7 |2025-08-26    |257.1   |대  |7   |10991100300950    |0             |1             |청기와24시감자탕 평택점|Z*ANG CHUNMEI|경기도 평택시 중앙로 264      |\n",
      "|경기도 평택시 합정동|964 |4122011700109640012|964-12|2014-11-18    |321.4   |대  |12  |10991100300769    |0             |1             |투썸플레이스평택시청점 |이*주        |경기도 평택시 중앙로 272      |\n",
      "|경기도 평택시 합정동|964 |4122011700109640015|964-15|2012-08-21    |173.5   |대  |15  |10991100224637    |0             |1             |롯데리아 평택시청점    |유*미        |경기도 평택시 평택5로20번길 35|\n",
      "|경기도 평택시 합정동|964 |4122011700109640016|964-16|2013-11-20    |555.1   |대  |16  |10991100248057    |28            |1             |돌풍평택시청점         |민*홍        |경기도 평택시 평택5로20번길 33|\n",
      "+--------------------+----+-------------------+------+--------------+--------+----+----+------------------+--------------+--------------+-----------------------+-------------+------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "group_has_restaurant_df = (\n",
    "    final_toji_df\n",
    "    .groupBy(\"법정동명\", \"본번\")\n",
    "    .agg(\n",
    "        F.max(\n",
    "            F.when(F.col(\"업체명\").isNotNull(), 1).otherwise(0)\n",
    "        ).alias(\"has_restaurant\")\n",
    "    )\n",
    "    .filter(F.col(\"has_restaurant\") == 1)\n",
    "    .select(\"법정동명\", \"본번\")\n",
    ")\n",
    "\n",
    "filtered_final_toji_df = (\n",
    "    final_toji_df\n",
    "    .join(group_has_restaurant_df, on=[\"법정동명\", \"본번\"], how=\"inner\")\n",
    ")\n",
    "\n",
    "print(\"필터 후 row 수:\", filtered_final_toji_df.count())\n",
    "filtered_final_toji_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b956a7ba-6b1c-435c-9999-1ed5959665ef",
   "metadata": {},
   "source": [
    "# 같은 지주 소유로 추정되는 토지 그룹핑 -> 토지대장 발급 리스트\n",
    "* 같은 본번으로 묶이는 토지 내에서 소유권변동일자가 같으면 같은 사람이 매입한 것으로 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b1673e9-1bb1-40c6-8c57-ac2f6e2b0619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/18 14:58:58 ERROR TaskSchedulerImpl: Lost executor 3 on 172.22.0.4: Command exited with code 137\n",
      "26/02/18 14:58:58 WARN TaskSetManager: Lost task 0.0 in stage 392.0 (TID 634) (172.22.0.4 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "26/02/18 14:58:58 WARN TaskSetManager: Lost task 2.0 in stage 392.0 (TID 636) (172.22.0.4 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------------+-----------+\n",
      "|법정동명            |본번|소유권변동일자|부번_리스트|\n",
      "+--------------------+----+--------------+-----------+\n",
      "|경기도 평택시 가재동|106 |2022-12-27    |1          |\n",
      "|경기도 평택시 가재동|106 |2023-01-02    |2          |\n",
      "|경기도 평택시 가재동|143 |2002-09-10    |3          |\n",
      "|경기도 평택시 가재동|143 |2017-08-22    |5          |\n",
      "|경기도 평택시 가재동|308 |2018-11-16    |1          |\n",
      "|경기도 평택시 가재동|308 |2019-05-28    |11         |\n",
      "|경기도 평택시 가재동|308 |2022-01-21    |12         |\n",
      "|경기도 평택시 가재동|314 |2010-01-11    |2          |\n",
      "|경기도 평택시 가재동|82  |2018-10-24    |6          |\n",
      "|경기도 평택시 가재동|82  |2023-11-14    |4          |\n",
      "+--------------------+----+--------------+-----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "toji_group_df = (\n",
    "    filtered_final_toji_df\n",
    "    .groupBy(\"법정동명\", \"본번\", \"소유권변동일자\")\n",
    "    .agg(\n",
    "        F.min(\"부번\").alias(\"부번_리스트\")\n",
    "    )\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "print(toji_group_df.count())\n",
    "toji_group_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b1f1dfc-5d6a-4f1a-b569-fa970b232dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered 저장 경로: /opt/spark/data/output/silver_stage_1/경기도_평택시_filtered_final_toji\n",
      "group 저장 경로: /opt/spark/data/output/silver_stage_1/경기도_평택시_toji_group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/18 15:00:06 ERROR TaskSchedulerImpl: Lost executor 4 on 172.22.0.3: Command exited with code 137\n",
      "26/02/18 15:00:06 WARN TaskSetManager: Lost task 4.0 in stage 429.0 (TID 690) (172.22.0.3 executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "26/02/18 15:00:24 ERROR TaskSchedulerImpl: Lost executor 5 on 172.22.0.4: Command exited with code 137\n",
      "26/02/18 15:00:24 WARN TaskSetManager: Lost task 1.0 in stage 469.0 (TID 752) (172.22.0.4 executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "26/02/18 15:00:24 WARN TaskSetManager: Lost task 2.0 in stage 469.0 (TID 753) (172.22.0.4 executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장 완료\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 저장 경로 생성\n",
    "# =========================\n",
    "base_output_path = os.path.join(\n",
    "    OUTPUT_BASE,\n",
    "    f\"{REGION}_{SIGUNGU.replace(' ', '_')}\"\n",
    ")\n",
    "\n",
    "filtered_path = base_output_path + \"_filtered_final_toji\"\n",
    "group_path = base_output_path + \"_toji_group\"\n",
    "\n",
    "print(\"filtered 저장 경로:\", filtered_path)\n",
    "print(\"group 저장 경로:\", group_path)\n",
    "\n",
    "# =========================\n",
    "# parquet 저장\n",
    "# =========================\n",
    "filtered_final_toji_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(filtered_path)\n",
    "\n",
    "toji_group_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(group_path)\n",
    "\n",
    "print(\"✅ 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d4ad1e3-8cef-40e8-96e0-00ddf0ceebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8728721-e983-4242-823c-36c1807991f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
