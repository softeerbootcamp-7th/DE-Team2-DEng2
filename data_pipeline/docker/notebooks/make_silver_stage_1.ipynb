{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95a41493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "import datetime as dt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68bf769-9485-4f7f-bbad-b444ecd7a212",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e8e06e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선택된 REGION 축약형:  경기\n",
      "선택된 REGION 영어:  gyunggi\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 사용자 설정 (원하는 지역과 시군구를 입력하세요)\n",
    "# ==========================================\n",
    "REGION = \"경기도\"          # \"ex.서울특별시\"\n",
    "SIGUNGU = \"용인시 처인구\"    # \"ex.강남구\"\n",
    "\n",
    "# 베이스 경로\n",
    "OUTPUT_BASE = \"/opt/spark/data/output/silver_stage_1\"\n",
    "ADDRESS_BASE = \"/opt/spark/data/address/parquet\"\n",
    "BUILDING_BASE   = \"/opt/spark/data/buildingLeader/parquet\"\n",
    "TOJI_BASE       = \"/opt/spark/data/tojiSoyuJeongbo/parquet\"\n",
    "REST_OWNER_BASE = \"/opt/spark/data/restaurant_owner/parquet\"\n",
    "\n",
    "\n",
    "# 데이터 필터용(축약형) 매핑\n",
    "REGION_MAP = {\n",
    "    \"서울특별시\": \"서울\",\n",
    "    \"부산광역시\": \"부산\",\n",
    "    \"대구광역시\": \"대구\",\n",
    "    \"인천광역시\": \"인천\",\n",
    "    \"광주광역시\": \"광주\",\n",
    "    \"대전광역시\": \"대전\",\n",
    "    \"울산광역시\": \"울산\",\n",
    "    \"세종특별자치시\": \"세종\",\n",
    "    \"경기도\": \"경기\",\n",
    "    \"강원도\": \"강원\",\n",
    "    \"충청북도\": \"충북\",\n",
    "    \"충청남도\": \"충남\",\n",
    "    \"전라북도\": \"전북\",\n",
    "    \"전라남도\": \"전남\",\n",
    "    \"경상북도\": \"경북\",\n",
    "    \"경상남도\": \"경남\",\n",
    "    \"제주특별자치도\": \"제주\",\n",
    "}\n",
    "\n",
    "REGION_SHORT = REGION_MAP[REGION]\n",
    "print(\"선택된 REGION 축약형: \", REGION_SHORT)\n",
    "\n",
    "# 데이터 필터용(축약형) 매핑\n",
    "REGION_ENG_MAP = {\n",
    "    \"서울특별시\": \"seoul\",\n",
    "    \"부산광역시\": \"busan\",\n",
    "    \"대구광역시\": \"daegu\",\n",
    "    \"인천광역시\": \"incheon\",\n",
    "    \"광주광역시\": \"gwangju\",\n",
    "    \"대전광역시\": \"daejeon\",\n",
    "    \"울산광역시\": \"ulsan\",\n",
    "    \"세종특별자치시\": \"sejong\",\n",
    "    \"경기도\": \"gyunggi\",\n",
    "    \"강원도\": \"gangwon\",\n",
    "    \"충청북도\": \"chungbuk\",\n",
    "    \"충청남도\": \"chungnam\",\n",
    "    \"전라북도\": \"jeonbuk\",\n",
    "    \"전라남도\": \"jeonnam\",\n",
    "    \"경상북도\": \"gyeongbuk\",\n",
    "    \"경상남도\": \"gyeongnam\",\n",
    "    \"제주특별자치도\": \"jeju\",\n",
    "}\n",
    "\n",
    "REGION_ENG = REGION_ENG_MAP[REGION]\n",
    "print(\"선택된 REGION 영어: \", REGION_ENG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00470063-c18a-4570-b14a-94fc96382790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8\n",
      "Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8\n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/16 10:14:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Spark Session 설정\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(f'silver_stage_1_{REGION}_{SIGUNGU.replace(\" \", \"_\")}') \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"400\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1830e81-e118-4b80-b3be-c6acd565f153",
   "metadata": {},
   "source": [
    "# 원천 데이터 로드 + 지역/시군구 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6375e99-bf6d-4055-af8d-8d0f8c3824cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 도로명주소관리번호: string (nullable = true)\n",
      " |-- 법정동코드: long (nullable = true)\n",
      " |-- 시도명: string (nullable = true)\n",
      " |-- 시군구명: string (nullable = true)\n",
      " |-- 법정읍면동명: string (nullable = true)\n",
      " |-- 법정리명: string (nullable = true)\n",
      " |-- 산여부: long (nullable = true)\n",
      " |-- 지번본번(번지): long (nullable = true)\n",
      " |-- 지번부번(호): long (nullable = true)\n",
      " |-- 도로명코드: long (nullable = true)\n",
      " |-- 도로명: string (nullable = true)\n",
      " |-- 지하여부: long (nullable = true)\n",
      " |-- 건물본번: long (nullable = true)\n",
      " |-- 건물부번: long (nullable = true)\n",
      " |-- 행정동코드: double (nullable = true)\n",
      " |-- 행정동명: string (nullable = true)\n",
      " |-- 기초구역번호(우편번호): long (nullable = true)\n",
      " |-- 이전도로명주소: double (nullable = true)\n",
      " |-- 효력발생일: double (nullable = true)\n",
      " |-- 공동주택구분: long (nullable = true)\n",
      " |-- 이동사유코드: double (nullable = true)\n",
      " |-- 건축물대장건물명: string (nullable = true)\n",
      " |-- 시군구용건물명: string (nullable = true)\n",
      " |-- 비고: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PNU코드: string (nullable = true)\n",
      " |-- 도로명주소: string (nullable = false)\n",
      "\n",
      "41534\n",
      "+-------------------+------------------------------------+\n",
      "|PNU코드            |도로명주소                          |\n",
      "+-------------------+------------------------------------+\n",
      "|4146110100100060009|경기도 용인시 처인구 백옥대로 1032  |\n",
      "|4146110100100060002|경기도 용인시 처인구 백옥대로 1034  |\n",
      "|4146110100100040002|경기도 용인시 처인구 백옥대로 1034-1|\n",
      "|4146110100100060010|경기도 용인시 처인구 백옥대로 1038  |\n",
      "|4146110100100060005|경기도 용인시 처인구 백옥대로 1040  |\n",
      "+-------------------+------------------------------------+\n",
      "only showing top 5 rows\n",
      "SIGUNGU_CODE = 41461\n"
     ]
    }
   ],
   "source": [
    "address_path = f\"{ADDRESS_BASE}/rnaddrkor_{REGION_ENG}.parquet\"\n",
    "address_df = spark.read.option(\"mergeSchema\", \"false\").parquet(address_path)\n",
    "\n",
    "address_df.printSchema()\n",
    "\n",
    "address_filtered_df = (\n",
    "    address_df\n",
    "    .filter(F.col(\"시도명\") == REGION)\n",
    "    .filter(F.col(\"시군구명\") == SIGUNGU)\n",
    "    .withColumn(\n",
    "        \"pnu_land_gb\",\n",
    "        F.when(F.col(\"산여부\") == 1, F.lit(\"2\")).otherwise(F.lit(\"1\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"PNU코드\",\n",
    "        F.concat(\n",
    "            F.col(\"법정동코드\"),\n",
    "            F.col(\"pnu_land_gb\"),\n",
    "            F.lpad(F.col(\"지번본번(번지)\").cast(\"string\"), 4, \"0\"),\n",
    "            F.lpad(F.coalesce(F.col(\"지번부번(호)\"), F.lit(0)).cast(\"string\"), 4, \"0\")\n",
    "        )\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"PNU코드\").cast(\"string\"),\n",
    "\n",
    "        F.concat_ws(\n",
    "            \" \",\n",
    "            F.col(\"시도명\"),\n",
    "            F.col(\"시군구명\"),\n",
    "            F.col(\"도로명\"),\n",
    "            F.concat(\n",
    "                F.col(\"건물본번\").cast(\"string\"),\n",
    "                F.when(\n",
    "                    (F.col(\"건물부번\").isNotNull()) & (F.col(\"건물부번\") != 0),\n",
    "                    F.concat(F.lit(\"-\"), F.col(\"건물부번\").cast(\"string\"))\n",
    "                ).otherwise(F.lit(\"\"))\n",
    "            )\n",
    "        ).alias(\"도로명주소\")\n",
    "    )\n",
    ")\n",
    "\n",
    "SIGUNGU_CODE = address_filtered_df.select(F.substring(\"PNU코드\", 1, 5).alias(\"sigungu\")).first()[\"sigungu\"]\n",
    "\n",
    "address_filtered_df.printSchema()\n",
    "print(address_filtered_df.count())\n",
    "address_filtered_df.show(5, truncate=False)\n",
    "print(\"SIGUNGU_CODE =\", SIGUNGU_CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d30e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜 계산 (직전 분기 말일 기준)\n",
    "today = dt.date.today()\n",
    "cur_q = (today.month - 1) // 3 + 1\n",
    "prev_q = cur_q - 1\n",
    "prev_q_year = today.year\n",
    "if prev_q == 0:\n",
    "    prev_q = 4\n",
    "    prev_q_year -= 1\n",
    "prev_q_last_month = prev_q * 3\n",
    "\n",
    "# 데이터 로드 경로\n",
    "building_path = f\"{BUILDING_BASE}/year={prev_q_year}/month={prev_q_last_month:02d}\"\n",
    "toji_path     = f\"{TOJI_BASE}/year={prev_q_year}/month={prev_q_last_month:02d}\"\n",
    "rest_owner_path  = f\"{REST_OWNER_BASE}/year={prev_q_year}/month={prev_q_last_month:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b8c2ad8-fcdf-4500-b188-a0a84106e27b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1) Building\n",
    "building_df = spark.read.option(\"mergeSchema\", \"false\").parquet(building_path)\n",
    "building_gg_df = building_df.filter(F.col(\"region\") == REGION_SHORT)\n",
    "if SIGUNGU_CODE:\n",
    "    building_gg_df = building_gg_df.filter(F.col(\"시군구_코드\") == SIGUNGU_CODE)\n",
    "\n",
    "# 2) Toji\n",
    "toji_df = spark.read.option(\"mergeSchema\", \"false\").parquet(toji_path)\n",
    "toji_gg_df = toji_df.filter(F.col(\"region\") == REGION_SHORT)\n",
    "if SIGUNGU_CODE:\n",
    "    toji_gg_df = toji_gg_df.filter(F.col(\"법정동코드\").startswith(SIGUNGU_CODE))\n",
    "\n",
    "# 3) Owner\n",
    "owner_df = spark.read.option(\"mergeSchema\", \"false\").parquet(rest_owner_path)\n",
    "owner_gg_df = owner_df.filter(F.col(\"region\") == REGION_SHORT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af2758-e0d6-4c07-bd85-3fd1e4358302",
   "metadata": {},
   "source": [
    "# Building Pruning (PNU 고유번호 생성 + 필요한 컬럼만 select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e168a0e-3ed6-4efc-a6a4-b76b1963fe52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 관리_건축물대장_PK: string (nullable = true)\n",
      " |-- 고유번호: string (nullable = true)\n",
      " |-- 옥외자주식면적: string (nullable = true)\n",
      " |-- 대장_구분_코드: string (nullable = true)\n",
      "\n",
      "전체 건축물대장 row 수: 51701\n",
      "+------------------+-------------------+--------------+--------------+\n",
      "|관리_건축물대장_PK|고유번호           |옥외자주식면적|대장_구분_코드|\n",
      "+------------------+-------------------+--------------+--------------+\n",
      "|111615014         |4146125322103440000|0             |1             |\n",
      "|111616678         |4146125323105250000|0             |1             |\n",
      "|111619285         |4146126221104650004|0             |2             |\n",
      "|1116112168        |4146126226100420000|0             |1             |\n",
      "|111619732         |4146135027104580000|0             |1             |\n",
      "+------------------+-------------------+--------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "building_gg_df = building_gg_df.withColumn(\n",
    "    \"고유번호\",\n",
    "    F.concat(\n",
    "        F.col(\"시군구_코드\"),        # 5\n",
    "        F.col(\"법정동_코드\"),        # 5\n",
    "        F.when(F.col(\"대지_구분_코드\") == \"0\", F.lit(\"1\"))  # 대지 -> 1\n",
    "         .when(F.col(\"대지_구분_코드\") == \"1\", F.lit(\"2\"))  # 산   -> 2\n",
    "         .otherwise(F.col(\"대지_구분_코드\")),\n",
    "        F.lpad(F.col(\"번\"), 4, \"0\"), # 본번\n",
    "        F.lpad(F.col(\"지\"), 4, \"0\")  # 부번\n",
    "    )\n",
    ")\n",
    "\n",
    "building_gg_df = (\n",
    "    building_gg_df\n",
    "    .select(\n",
    "        F.col(\"관리_건축물대장_PK\"),\n",
    "        F.col(\"고유번호\"),\n",
    "        F.col(\"옥외_자주식_면적(㎡)\").alias(\"옥외자주식면적\"),\n",
    "        F.col(\"대장_구분_코드\")\n",
    "    )\n",
    ")\n",
    "\n",
    "building_gg_df.printSchema()\n",
    "print(f\"전체 건축물대장 row 수: {building_gg_df.count()}\")\n",
    "building_gg_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd49500-d317-44ed-ad2e-07892e481076",
   "metadata": {},
   "source": [
    "# Toji Pruning (단독+개인 소유, 가장 최근 소유권 변동 정보만 필터링)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edd8a5d7-e683-4051-a887-1b31c1474d2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 토지 수: 569885\n",
      "root\n",
      " |-- 고유번호: string (nullable = true)\n",
      " |-- 법정동명: string (nullable = true)\n",
      " |-- 지번: string (nullable = true)\n",
      " |-- 소유권변동일자: string (nullable = true)\n",
      " |-- 토지면적: double (nullable = true)\n",
      " |-- 지목: string (nullable = true)\n",
      " |-- 본번: string (nullable = true)\n",
      " |-- 부번: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필터링 후 토지 수: 111744 (19.61%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:=============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------------+-----+--------------+--------+----+----+----+\n",
      "|고유번호           |법정동명                     |지번 |소유권변동일자|토지면적|지목|본번|부번|\n",
      "+-------------------+-----------------------------+-----+--------------+--------+----+----+----+\n",
      "|4146110100100080001|경기도 용인시 처인구 김량장동|8-1  |2024-09-05    |31.41   |대  |8   |1   |\n",
      "|4146110100100080004|경기도 용인시 처인구 김량장동|8-4  |2003-03-17    |394.0   |대  |8   |4   |\n",
      "|4146110100100080011|경기도 용인시 처인구 김량장동|8-11 |2025-07-14    |48.38   |대  |8   |11  |\n",
      "|4146110100100080013|경기도 용인시 처인구 김량장동|8-13 |2004-08-16    |55.0    |답  |8   |13  |\n",
      "|4146110100100200017|경기도 용인시 처인구 김량장동|20-17|2016-09-22    |44.0    |대  |20  |17  |\n",
      "|4146110100100220003|경기도 용인시 처인구 김량장동|22-3 |2002-06-24    |29.0    |도로|22  |3   |\n",
      "|4146110100100310008|경기도 용인시 처인구 김량장동|31-8 |2016-02-01    |195.0   |대  |31  |8   |\n",
      "|4146110100100330000|경기도 용인시 처인구 김량장동|33   |1981-08-18    |808.0   |전  |33  |NULL|\n",
      "|4146110100100390002|경기도 용인시 처인구 김량장동|39-2 |2024-05-21    |49.57   |대  |39  |2   |\n",
      "|4146110100100400009|경기도 용인시 처인구 김량장동|40-9 |2004-06-23    |151.0   |대  |40  |9   |\n",
      "+-------------------+-----------------------------+-----+--------------+--------+----+----+----+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "all_count = toji_gg_df.count()\n",
    "print(f\"전체 토지 수: {all_count}\")\n",
    "\n",
    "toji_gg_df = (\n",
    "    toji_gg_df\n",
    "    # 1) 조건 필터\n",
    "    .filter(F.col(\"공유인수\") == 0)\n",
    "    .filter(F.col(\"소유구분코드\") == \"01\")\n",
    "    # 2) 필요한 컬럼만 선택\n",
    "    .select(\n",
    "        F.col(\"고유번호\").cast(\"string\"),\n",
    "        F.col(\"법정동명\"),\n",
    "        F.col(\"지번\"),\n",
    "        F.col(\"소유권변동일자\"),\n",
    "        F.col(\"토지면적\"),\n",
    "        F.col(\"지목\"),\n",
    "    )\n",
    "\n",
    "    # 3) 날짜 파싱\n",
    "    .withColumn(\n",
    "        \"소유권변동일자_dt\",\n",
    "        F.to_date(F.col(\"소유권변동일자\"), \"yyyy-MM-dd\")\n",
    "    )\n",
    "\n",
    "    # 4) 고유번호별 최신 1행\n",
    "    .withColumn(\n",
    "        \"rn\",\n",
    "        F.row_number().over(\n",
    "            Window.partitionBy(\"고유번호\")\n",
    "                  .orderBy(F.col(\"소유권변동일자_dt\").desc_nulls_last())\n",
    "        )\n",
    "    )\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\", \"소유권변동일자_dt\")\n",
    "\n",
    "    # 5) 지번 분리\n",
    "    .withColumn(\n",
    "        \"본번\",\n",
    "        F.split(F.col(\"지번\"), \"-\").getItem(0)\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"부번\",\n",
    "        F.when(\n",
    "            F.size(F.split(F.col(\"지번\"), \"-\")) > 1,\n",
    "            F.split(F.col(\"지번\"), \"-\").getItem(1)\n",
    "        ).otherwise(F.lit(None))\n",
    "    )\n",
    ")\n",
    "\n",
    "# 확인\n",
    "toji_gg_df.printSchema()\n",
    "print(f\"필터링 후 토지 수: {toji_gg_df.count()} ({toji_gg_df.count() / all_count * 100:.2f}%)\")\n",
    "toji_gg_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af459c8-50fe-43d2-a453-0d4fa9c795c3",
   "metadata": {},
   "source": [
    "# Restaurant Pruning (필요한 컬럼만 + 중복 제거)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "698ea479-ef66-4c2e-9504-9abd957be01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "owner (업체명, 소재지) 중복 제거 후: 5453 (98.71%)\n",
      "+----------------------------+------+-------------------------------------------+\n",
      "|업체명                      |대표자|소재지_정제                                |\n",
      "+----------------------------+------+-------------------------------------------+\n",
      "|(B.H.C)비에이치씨 용인송전점|김*숙 |경기도 용인시 처인구 경기동로687번길 6     |\n",
      "|(BHC)비에이치씨둔전점       |김*정 |경기도 용인시 처인구 포곡로108번길 5-13    |\n",
      "|(BHC)비에이치씨역북점       |김*하 |경기도 용인시 처인구 중부대로1281번길 10-29|\n",
      "|(顥)호돼지네                |용*귀 |경기도 용인시 처인구 경안천로 232          |\n",
      "|(사)천주교인보회요한의집    |한*란 |경기도 용인시 처인구 백옥대로1832번길 58   |\n",
      "|(사)천주교인보회인보마을    |곽*리 |경기도 용인시 처인구 백옥대로1832번길 42   |\n",
      "|(의)영문의료재단 다보스병원 |양*범 |경기도 용인시 처인구 백옥대로1082번길 18   |\n",
      "|(주)SCD                     |오*호 |경기도 용인시 처인구 형제로17번길 21       |\n",
      "|(주)갈비명가소들녘          |전*민 |경기도 용인시 처인구 양지로 242            |\n",
      "|(주)기가텍                  |이*대 |경기도 용인시 처인구 대지로 409-15         |\n",
      "+----------------------------+------+-------------------------------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Restaurant pruning\n",
    "# =========================\n",
    "# =========================\n",
    "# Restaurant pruning\n",
    "# =========================\n",
    "owner_gg_df_clean = (\n",
    "    owner_gg_df\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1. 괄호 제거\n",
    "    # -----------------------------\n",
    "    .withColumn(\n",
    "        \"소재지_tmp\",\n",
    "        F.regexp_replace(F.col(\"소재지\"), r\"\\s*\\([^)]*\\)\", \"\")\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2. 쉼표 뒤 제거\n",
    "    # -----------------------------\n",
    "    .withColumn(\n",
    "        \"소재지_tmp\",\n",
    "        F.regexp_replace(F.col(\"소재지_tmp\"), r\",.*$\", \"\")\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3. 도로명 앞 행정동 제거\n",
    "    # (고림동 고림로 123 → 고림로 123)\n",
    "    # -----------------------------\n",
    "    .withColumn(\n",
    "        \"소재지_tmp\",\n",
    "        F.regexp_replace(\n",
    "            F.col(\"소재지_tmp\"),\n",
    "            r'\\s+\\S+(동|읍|면|리)\\s+(?=\\S+(로|길))',\n",
    "            ' '\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. 공백 정리\n",
    "    # -----------------------------\n",
    "    .withColumn(\n",
    "        \"소재지_정제\",\n",
    "        F.trim(F.regexp_replace(F.col(\"소재지_tmp\"), r'\\s+', ' '))\n",
    "    )\n",
    "\n",
    "    .select(\n",
    "        F.col(\"업체명\"),\n",
    "        F.col(\"대표자\"),\n",
    "        F.col(\"소재지_정제\"),\n",
    "    )\n",
    "    .filter(F.col(\"업체명\").isNotNull())\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Owner 중복 제거\n",
    "# =========================\n",
    "all_count = owner_gg_df_clean.count()\n",
    "\n",
    "owner_gg_df_clean = owner_gg_df_clean.dropDuplicates([\"업체명\", \"소재지_정제\"])\n",
    "\n",
    "print(f\"owner (업체명, 소재지) 중복 제거 후: {owner_gg_df_clean.count()} ({owner_gg_df_clean.count() / all_count * 100:.2f}%)\")\n",
    "\n",
    "owner_gg_df_clean.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333a5b9-fd4d-487b-9d27-a775278785c6",
   "metadata": {},
   "source": [
    "# Restaurant + Address Left Join (도로명주소 -> 법정동코드 컬럼 추가)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76637e16-6a01-4514-a765-6ffb0edd2add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 업체명: string (nullable = true)\n",
      " |-- 대표자: string (nullable = true)\n",
      " |-- 소재지_정제: string (nullable = true)\n",
      " |-- PNU코드: string (nullable = true)\n",
      "\n",
      "owner 도로명주소 매칭 후: 5325 (97.65%)\n",
      "+--------------------------+-------+----------------------------------+-------------------+\n",
      "|업체명                    |대표자 |소재지_정제                       |PNU코드            |\n",
      "+--------------------------+-------+----------------------------------+-------------------+\n",
      "|토종흑염소전문            |전*규  |경기도 용인시 처인구 백옥대로 1048|4146110100100090005|\n",
      "|은희네 행복한 밥집        |이*나라|경기도 용인시 처인구 백옥대로 1048|4146110100100090005|\n",
      "|봉산짬뽕                  |김*두  |경기도 용인시 처인구 백옥대로 1048|4146110100100090005|\n",
      "|복가                      |임*진  |경기도 용인시 처인구 백옥대로 1058|4146110100100120015|\n",
      "|GS25용인태성점            |이*용  |경기도 용인시 처인구 백옥대로 1059|4146110100100310011|\n",
      "|자마르(ZAMAR)             |최*영  |경기도 용인시 처인구 백옥대로 1066|4146110100100130000|\n",
      "|롯데쇼핑(주)롯데슈퍼용인점|김*재  |경기도 용인시 처인구 백옥대로 1066|4146110100100130000|\n",
      "|동해횟집                  |황*희  |경기도 용인시 처인구 백옥대로 1072|4146110100100240000|\n",
      "|그리고 옹심이메밀칼국수   |안*선  |경기도 용인시 처인구 백옥대로 1072|4146110100100240000|\n",
      "|저팔계                    |이*아  |경기도 용인시 처인구 백옥대로 1076|4146110100100240000|\n",
      "+--------------------------+-------+----------------------------------+-------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "o = owner_gg_df_clean.alias(\"o\")\n",
    "a = address_filtered_df.alias(\"a\")\n",
    "\n",
    "restaurant_address_df = (\n",
    "    o.join(\n",
    "        a,\n",
    "        F.col(\"o.소재지_정제\") == F.col(\"a.도로명주소\"),\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .filter(F.col(\"도로명주소\").isNotNull())\n",
    "    .drop(\"도로명주소\")\n",
    ")\n",
    "\n",
    "restaurant_address_df.printSchema()\n",
    "print(f\"owner 도로명주소 매칭 후: {restaurant_address_df.count()} ({restaurant_address_df.count() / owner_gg_df_clean.count() * 100:.2f}%)\")\n",
    "restaurant_address_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7053f-a433-43ba-b094-c25f6e3d083d",
   "metadata": {},
   "source": [
    "# 토지 필터링\n",
    "1. 토지에 건물이 없거나\n",
    "2. 건물이 1개 있고, 일반 건물이며, 음식점이 있는 토지만 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15d966df-a903-4338-ab9d-f07162ceca0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 고유번호: string (nullable = true)\n",
      " |-- 법정동명: string (nullable = true)\n",
      " |-- 지번: string (nullable = true)\n",
      " |-- 소유권변동일자: string (nullable = true)\n",
      " |-- 토지면적: double (nullable = true)\n",
      " |-- 지목: string (nullable = true)\n",
      " |-- 본번: string (nullable = true)\n",
      " |-- 부번: string (nullable = true)\n",
      " |-- 관리_건축물대장_PK: string (nullable = true)\n",
      " |-- 옥외자주식면적: string (nullable = true)\n",
      " |-- 대장_구분_코드: string (nullable = true)\n",
      "\n",
      "118721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 84:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------------+----+--------------+--------+----+----+----+------------------+--------------+--------------+\n",
      "|고유번호           |법정동명                     |지번|소유권변동일자|토지면적|지목|본번|부번|관리_건축물대장_PK|옥외자주식면적|대장_구분_코드|\n",
      "+-------------------+-----------------------------+----+--------------+--------+----+----+----+------------------+--------------+--------------+\n",
      "|4146110100100080001|경기도 용인시 처인구 김량장동|8-1 |2024-09-05    |31.41   |대  |8   |1   |11161100478093    |0             |2             |\n",
      "|4146110100100080001|경기도 용인시 처인구 김량장동|8-1 |2024-09-05    |31.41   |대  |8   |1   |11161100478078    |0             |2             |\n",
      "|4146110100100080004|경기도 용인시 처인구 김량장동|8-4 |2003-03-17    |394.0   |대  |8   |4   |1116116027        |0             |1             |\n",
      "|4146110100100080004|경기도 용인시 처인구 김량장동|8-4 |2003-03-17    |394.0   |대  |8   |4   |1116116026        |0             |1             |\n",
      "|4146110100100080011|경기도 용인시 처인구 김량장동|8-11|2025-07-14    |48.38   |대  |8   |11  |1116136285        |92            |2             |\n",
      "+-------------------+-----------------------------+----+--------------+--------+----+----+----+------------------+--------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "t = toji_gg_df.alias(\"t\")\n",
    "b = building_gg_df.alias(\"b\")\n",
    "\n",
    "toji_building_gg_df = (\n",
    "    t.join(b, F.col(\"t.고유번호\") == F.col(\"b.고유번호\"), \"left\")\n",
    "     .drop(F.col(\"b.고유번호\"))\n",
    ")\n",
    "\n",
    "toji_building_gg_df.printSchema()\n",
    "print(toji_building_gg_df.count())\n",
    "toji_building_gg_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1828ea4-492f-486e-a71e-a040340ad948",
   "metadata": {},
   "source": [
    "### 건물이 1개 이하인 토지 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "742bf6d5-a652-4c15-bd99-dba50b402a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------------+-----+--------------+--------+----+----+----+------------------+--------------+--------------+\n",
      "|고유번호           |법정동명                     |지번 |소유권변동일자|토지면적|지목|본번|부번|관리_건축물대장_PK|옥외자주식면적|대장_구분_코드|\n",
      "+-------------------+-----------------------------+-----+--------------+--------+----+----+----+------------------+--------------+--------------+\n",
      "|4146110100100080011|경기도 용인시 처인구 김량장동|8-11 |2025-07-14    |48.38   |대  |8   |11  |1116136285        |92            |2             |\n",
      "|4146110100100080013|경기도 용인시 처인구 김량장동|8-13 |2004-08-16    |55.0    |답  |8   |13  |NULL              |NULL          |NULL          |\n",
      "|4146110100100200017|경기도 용인시 처인구 김량장동|20-17|2016-09-22    |44.0    |대  |20  |17  |NULL              |NULL          |NULL          |\n",
      "|4146110100100220003|경기도 용인시 처인구 김량장동|22-3 |2002-06-24    |29.0    |도로|22  |3   |NULL              |NULL          |NULL          |\n",
      "|4146110100100310008|경기도 용인시 처인구 김량장동|31-8 |2016-02-01    |195.0   |대  |31  |8   |11161100269304    |0             |1             |\n",
      "+-------------------+-----------------------------+-----+--------------+--------+----+----+----+------------------+--------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "pk_cnt_df = (\n",
    "    toji_building_gg_df\n",
    "    .groupBy(\"고유번호\")\n",
    "    .agg(F.count(\"관리_건축물대장_PK\").alias(\"pk_cnt\"))\n",
    ")\n",
    "\n",
    "toji_binary_building_gg_df = (\n",
    "    toji_building_gg_df\n",
    "    .join(\n",
    "        pk_cnt_df.filter(F.col(\"pk_cnt\") <= 1),\n",
    "        on=\"고유번호\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .drop(\"pk_cnt\")\n",
    ")\n",
    "\n",
    "print(toji_binary_building_gg_df.count())\n",
    "toji_binary_building_gg_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3e1c04a-eea5-4a73-8786-22e9d7ce9821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|건물개수|count|\n",
      "+--------+-----+\n",
      "|       0|90264|\n",
      "|       1|17598|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toji_binary_building_gg_df.withColumn(\n",
    "    \"건물개수\",\n",
    "    F.when(F.col(\"관리_건축물대장_PK\").isNull(), F.lit(0))\n",
    "     .otherwise(F.lit(1))\n",
    ").groupBy(\"건물개수\").count().orderBy(\"건물개수\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea7f2f8-a5f3-4327-a5e2-41bef42b452d",
   "metadata": {},
   "source": [
    "### 건물이 1개 있고, 일반 건물이며, 음식점이 포함된 토지 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1389a0fc-8145-4511-9c52-7ed08bb8b362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "건물 1개인 토지 수: 17598\n"
     ]
    }
   ],
   "source": [
    "# 건물이 1개 있는 토지만 필터링\n",
    "toji_with_1_building_gg_df = (\n",
    "    toji_binary_building_gg_df\n",
    "    .filter(F.col(\"관리_건축물대장_PK\").isNotNull())\n",
    ")\n",
    "\n",
    "print(\"건물 1개인 토지 수:\", toji_with_1_building_gg_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14cdab92-e04d-41ec-9efb-82571b5c5248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "건물이 1개이고, 음식점이 있는 토지: 2223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------------+-------+--------------+--------+----+----+----+------------------+--------------+--------------+------------+---------+--------------------------------+\n",
      "|           고유번호|                     법정동명|   지번|소유권변동일자|토지면적|지목|본번|부번|관리_건축물대장_PK|옥외자주식면적|대장_구분_코드|      업체명|   대표자|                     소재지_정제|\n",
      "+-------------------+-----------------------------+-------+--------------+--------+----+----+----+------------------+--------------+--------------+------------+---------+--------------------------------+\n",
      "|4146110100101330060|경기도 용인시 처인구 김량장동| 133-60|    2008-04-24|   106.0|  대| 133|  60|        1116136484|             0|             1|    고기창고|    김*하|경기도 용인시 처인구 금령로85...|\n",
      "|4146110100101330060|경기도 용인시 처인구 김량장동| 133-60|    2008-04-24|   106.0|  대| 133|  60|        1116136484|             0|             1|      엠와이|    장*애|경기도 용인시 처인구 금령로85...|\n",
      "|4146110100101330123|경기도 용인시 처인구 김량장동|133-123|    2015-11-10|   152.0|  대| 133| 123|        1116129623|             0|             1|      달다방|J*N HAIZI| 경기도 용인시 처인구 용문로 198|\n",
      "|4146110100101330158|경기도 용인시 처인구 김량장동|133-158|    2022-09-08|   155.0|  대| 133| 158|        1116119859|             0|             1|  전라도밥상|    박*태|경기도 용인시 처인구 금령로99...|\n",
      "|4146110100101330166|경기도 용인시 처인구 김량장동|133-166|    1980-02-27|   221.0|  대| 133| 166|        1116119972|             0|             1|향수호프커피|    허*옥|경기도 용인시 처인구 금령로99...|\n",
      "+-------------------+-----------------------------+-------+--------------+--------+----+----+----+------------------+--------------+--------------+------------+---------+--------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 건물 있는데 음식점 아닌거 거르려고 만드는 join table\n",
    "\n",
    "t = toji_with_1_building_gg_df.alias(\"t\")\n",
    "r = restaurant_address_df.alias(\"r\")\n",
    "\n",
    "toji_building_restaurant_gg_df = (\n",
    "    t.join(\n",
    "        r,\n",
    "        F.col(\"t.고유번호\") == F.col(\"r.PNU코드\"),\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .drop(F.col(\"r.PNU코드\"))\n",
    "    .filter(F.col(\"업체명\").isNotNull())\n",
    ")\n",
    "\n",
    "print(\"건물이 1개이고, 음식점이 있는 토지:\", toji_building_restaurant_gg_df.count())\n",
    "toji_building_restaurant_gg_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f937eef-35f5-4532-a8d0-5145e16e629d",
   "metadata": {},
   "source": [
    "### 건물 0개 + 건물 1개 (일반건물, 음식점있음) 토지 concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4e0e506-84b0-41a1-8cb8-5a847b24f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아무 건물도 없는 필지\n",
    "toji_with_0_building_gg_df = (\n",
    "    toji_binary_building_gg_df\n",
    "    .filter(F.col(\"관리_건축물대장_PK\").isNull())\n",
    ")\n",
    "\n",
    "# 위에서 만든 join table과 concat을 위해 column 추가\n",
    "toji_with_0_building_gg_df = (\n",
    "    toji_with_0_building_gg_df\n",
    "    .withColumn(\"업체명\", F.lit(None).cast(\"string\"))\n",
    "    .withColumn(\"대표자\", F.lit(None).cast(\"string\"))\n",
    "    .withColumn(\"소재지_정제\", F.lit(None).cast(\"string\"))\n",
    ")\n",
    "\n",
    "final_toji_df = (\n",
    "    toji_building_restaurant_gg_df\n",
    "    .unionByName(toji_with_0_building_gg_df)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c708edd6-e739-4a92-9643-8438a9e6dd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final: 92487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "건물1+음식점: 2223\n",
      "건물0: 90264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|건물개수|count|\n",
      "+--------+-----+\n",
      "|       0|90264|\n",
      "|       1| 2223|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/16 10:14:54 ERROR TaskSchedulerImpl: Lost executor 0 on 172.22.0.4: Command exited with code 137\n",
      "26/02/16 10:14:54 WARN TaskSetManager: Lost task 0.0 in stage 239.0 (TID 349) (172.22.0.4 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "26/02/16 10:14:54 WARN TaskSetManager: Lost task 2.0 in stage 239.0 (TID 351) (172.22.0.4 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "26/02/16 10:14:56 WARN TaskSetManager: Lost task 0.0 in stage 246.0 (TID 366) (172.22.0.3 executor 1): FetchFailed(null, shuffleId=75, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 75 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1781)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1726)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1725)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1313)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1725)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1359)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1321)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:135)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:230)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      ")\n",
      "26/02/16 10:14:56 WARN TaskSetManager: Lost task 0.0 in stage 244.0 (TID 365) (172.22.0.3 executor 1): FetchFailed(null, shuffleId=74, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 74 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1781)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1726)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1725)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1313)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1725)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1359)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1321)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:135)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:230)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      ")\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------------+-----+--------------+--------+----+----+----+------------------+--------------+--------------+-------------------------------+--------------------+---------------------------------+\n",
      "|           고유번호|                     법정동명| 지번|소유권변동일자|토지면적|지목|본번|부번|관리_건축물대장_PK|옥외자주식면적|대장_구분_코드|                         업체명|              대표자|                      소재지_정제|\n",
      "+-------------------+-----------------------------+-----+--------------+--------+----+----+----+------------------+--------------+--------------+-------------------------------+--------------------+---------------------------------+\n",
      "|4146110100100310011|경기도 용인시 처인구 김량장동|31-11|    2015-06-16|   139.0|  대|  31|  11|        1116119256|          11.5|             1|                 GS25용인태성점|               이*용|경기도 용인시 처인구 백옥대로 ...|\n",
      "|4146110100100410005|경기도 용인시 처인구 김량장동| 41-5|    1996-05-15|   340.0|  대|  41|   5|        1116121579|             0|             1|                       차곡차곡|               이*옥|경기도 용인시 처인구 중부대로 ...|\n",
      "|4146110100100420006|경기도 용인시 처인구 김량장동| 42-6|    2024-06-21|   120.0|  대|  42|   6|        1116124213|             0|             1|           지에스25용인터미널점|               박*목|경기도 용인시 처인구 백옥대로 ...|\n",
      "|4146110100100630009|경기도 용인시 처인구 김량장동| 63-9|    2025-10-17|    3.39|  대|  63|   9|    11161100314937|             0|             2|(주)에스엠에스트레이딩 용인지점|K*OLNAZAROV BAKHT...|경기도 용인시 처인구 백옥대로 ...|\n",
      "|4146110100100630009|경기도 용인시 처인구 김량장동| 63-9|    2025-10-17|    3.39|  대|  63|   9|    11161100314937|             0|             2|       굿킨 야시장 용인터미널점|               이*호|경기도 용인시 처인구 백옥대로 ...|\n",
      "+-------------------+-----------------------------+-----+--------------+--------+----+----+----+------------------+--------------+--------------+-------------------------------+--------------------+---------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 전체 row 수 = (건물1+음식점) + (건물0)\n",
    "print(\"final:\", final_toji_df.count())\n",
    "print(\"건물1+음식점:\", toji_building_restaurant_gg_df.count())\n",
    "print(\"건물0:\", toji_with_0_building_gg_df.count())\n",
    "\n",
    "# 건물 개수 분포\n",
    "final_toji_df.withColumn(\n",
    "    \"건물개수\",\n",
    "    F.when(F.col(\"관리_건축물대장_PK\").isNull(), 0).otherwise(1)\n",
    ").groupBy(\"건물개수\").count().orderBy(\"건물개수\").show()\n",
    "\n",
    "final_toji_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c400e516-959b-4006-9f4f-119fc7404cb0",
   "metadata": {},
   "source": [
    "# 같은 본번을 가진 토지 그룹핑 -> 음식점이 있는 그룹만 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65c9b815-5cfd-4c4d-af24-440002021707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필터 후 row 수: 5740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 624:>                                                        (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+----+-------------------+-----+--------------+--------+----+----+------------------+--------------+--------------+--------------------------+------+--------------------------------------+\n",
      "|법정동명                     |본번|고유번호           |지번 |소유권변동일자|토지면적|지목|부번|관리_건축물대장_PK|옥외자주식면적|대장_구분_코드|업체명                    |대표자|소재지_정제                           |\n",
      "+-----------------------------+----+-------------------+-----+--------------+--------+----+----+------------------+--------------+--------------+--------------------------+------+--------------------------------------+\n",
      "|경기도 용인시 처인구 김량장동|333 |4146110100103330001|333-1|1987-06-24    |226.1   |대  |1   |1116126076        |0             |1             |보석포차                  |유*숙 |경기도 용인시 처인구 금령로 24        |\n",
      "|경기도 용인시 처인구 해곡동  |175 |4146111000101750003|175-3|2015-09-09    |600.0   |대  |3   |1116111330        |0             |1             |엉끌드파리(uncle de paris)|김*희 |경기도 용인시 처인구 해곡로 30        |\n",
      "|경기도 용인시 처인구 역북동  |754 |4146110200107540001|754-1|2025-07-22    |4.51    |대  |1   |11161100451106    |0             |2             |금별맥주 용인역북점       |장*준 |경기도 용인시 처인구 명지로40번길 15-9|\n",
      "|경기도 용인시 처인구 역북동  |754 |4146110200107540001|754-1|2025-07-22    |4.51    |대  |1   |11161100451106    |0             |2             |단성무이 용인역북점       |노*범 |경기도 용인시 처인구 명지로40번길 15-9|\n",
      "|경기도 용인시 처인구 역북동  |754 |4146110200107540001|754-1|2025-07-22    |4.51    |대  |1   |11161100451106    |0             |2             |바리스닭                  |박*수 |경기도 용인시 처인구 명지로40번길 15-9|\n",
      "+-----------------------------+----+-------------------+-----+--------------+--------+----+----+------------------+--------------+--------------+--------------------------+------+--------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "group_has_restaurant_df = (\n",
    "    final_toji_df\n",
    "    .groupBy(\"법정동명\", \"본번\")\n",
    "    .agg(\n",
    "        F.max(\n",
    "            F.when(F.col(\"업체명\").isNotNull(), 1).otherwise(0)\n",
    "        ).alias(\"has_restaurant\")\n",
    "    )\n",
    "    .filter(F.col(\"has_restaurant\") == 1)\n",
    "    .select(\"법정동명\", \"본번\")\n",
    ")\n",
    "\n",
    "filtered_final_toji_df = (\n",
    "    final_toji_df\n",
    "    .join(group_has_restaurant_df, on=[\"법정동명\", \"본번\"], how=\"inner\")\n",
    ")\n",
    "\n",
    "print(\"필터 후 row 수:\", filtered_final_toji_df.count())\n",
    "filtered_final_toji_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b956a7ba-6b1c-435c-9999-1ed5959665ef",
   "metadata": {},
   "source": [
    "# 같은 지주 소유로 추정되는 토지 그룹핑 -> 토지대장 발급 리스트\n",
    "* 같은 본번으로 묶이는 토지 내에서 소유권변동일자가 같으면 같은 사람이 매입한 것으로 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b1673e9-1bb1-40c6-8c57-ac2f6e2b0619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/16 10:44:16 ERROR TaskSchedulerImpl: Lost executor 6 on 172.22.0.4: Command exited with code 137\n",
      "26/02/16 10:44:16 WARN TaskSetManager: Lost task 0.0 in stage 902.0 (TID 1306) (172.22.0.3 executor 7): FetchFailed(null, shuffleId=283, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 283 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1781)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1726)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1725)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1313)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1725)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1359)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1321)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:135)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:67)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:61)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      ")\n",
      "[Stage 900:>                                                        (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+----+--------------+----------+\n",
      "|법정동명                   |본번|소유권변동일자|부번리스트|\n",
      "+---------------------------+----+--------------+----------+\n",
      "|경기도 용인시 처인구 고림동|193 |2023-12-26    |4         |\n",
      "|경기도 용인시 처인구 고림동|208 |2006-11-17    |14        |\n",
      "|경기도 용인시 처인구 고림동|208 |2025-02-28    |11        |\n",
      "|경기도 용인시 처인구 고림동|264 |2025-05-08    |18        |\n",
      "|경기도 용인시 처인구 고림동|264 |2025-07-30    |3         |\n",
      "|경기도 용인시 처인구 고림동|264 |2025-10-16    |1         |\n",
      "|경기도 용인시 처인구 고림동|266 |2012-12-27    |1         |\n",
      "|경기도 용인시 처인구 고림동|341 |2019-10-31    |6         |\n",
      "|경기도 용인시 처인구 고림동|341 |2022-05-04    |2         |\n",
      "|경기도 용인시 처인구 고림동|342 |1998-04-01    |1         |\n",
      "+---------------------------+----+--------------+----------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "toji_group_df = (\n",
    "    filtered_final_toji_df\n",
    "    .groupBy(\"법정동명\", \"본번\", \"소유권변동일자\")\n",
    "    .agg(\n",
    "        F.min(\"부번\").alias(\"부번리스트\")\n",
    "    )\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "print(toji_group_df.count())\n",
    "toji_group_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b1f1dfc-5d6a-4f1a-b569-fa970b232dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered 저장 경로: /opt/spark/data/output/silver_stage_1/경기도_용인시_처인구_filtered_final_toji\n",
      "group 저장 경로: /opt/spark/data/output/silver_stage_1/경기도_용인시_처인구_toji_group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/16 10:58:04 ERROR TaskSchedulerImpl: Lost executor 8 on 172.22.0.4: Command exited with code 137\n",
      "26/02/16 10:58:04 WARN TaskSetManager: Lost task 0.0 in stage 1076.0 (TID 1605) (172.22.0.4 executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "26/02/16 10:58:04 WARN TaskSetManager: Lost task 1.0 in stage 1076.0 (TID 1606) (172.22.0.4 executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "26/02/16 10:58:05 WARN TaskSetManager: Lost task 3.0 in stage 1072.0 (TID 1608) (172.22.0.3 executor 9): FetchFailed(BlockManagerId(8, 172.22.0.4, 44389, None), shuffleId=332, mapIndex=2, mapId=1560, reduceId=200, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:426)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1261)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:988)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:604)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:618)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage30.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage30.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.fetchNextRow(WindowGroupLimitExec.scala:213)\n",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.<init>(WindowGroupLimitExec.scala:219)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory.$anonfun$createEvaluator$4(WindowGroupLimitEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory$WindowGroupLimitPartitionEvaluator.eval(WindowGroupLimitEvaluatorFactory.scala:60)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1(WindowGroupLimitExec.scala:89)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1$adapted(WindowGroupLimitExec.scala:87)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:108)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: [INTERNAL_ERROR_NETWORK] The relative remote executor(Id: 8), which maintains the block data to fetch is dead. SQLSTATE: XX000\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:146)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:180)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:159)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:157)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:383)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1228)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1220)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.$anonfun$initialize$2(ShuffleBlockFetcherIterator.scala:729)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.withFetchWaitTimeTracked(ShuffleBlockFetcherIterator.scala:198)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:729)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:194)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:232)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\t... 36 more\n",
      "\n",
      ")\n",
      "26/02/16 10:58:05 WARN TaskSetManager: Lost task 2.0 in stage 1072.0 (TID 1607) (172.22.0.3 executor 9): FetchFailed(BlockManagerId(8, 172.22.0.4, 44389, None), shuffleId=332, mapIndex=2, mapId=1560, reduceId=0, message=\n",
      "org.apache.spark.shuffle.FetchFailedException\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.fetchFailedError(SparkCoreErrors.scala:426)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1261)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:988)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:86)\n",
      "\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n",
      "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:604)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:618)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage30.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage30.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.fetchNextRow(WindowGroupLimitExec.scala:213)\n",
      "\tat org.apache.spark.sql.execution.window.GroupedLimitIterator.<init>(WindowGroupLimitExec.scala:219)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory.$anonfun$createEvaluator$4(WindowGroupLimitEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitEvaluatorFactory$WindowGroupLimitPartitionEvaluator.eval(WindowGroupLimitEvaluatorFactory.scala:60)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1(WindowGroupLimitExec.scala:89)\n",
      "\tat org.apache.spark.sql.execution.window.WindowGroupLimitExec.$anonfun$doExecute$1$adapted(WindowGroupLimitExec.scala:87)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:108)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.ExecutorDeadException: [INTERNAL_ERROR_NETWORK] The relative remote executor(Id: 8), which maintains the block data to fetch is dead. SQLSTATE: XX000\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:146)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:180)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:159)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:157)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:383)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.send$1(ShuffleBlockFetcherIterator.scala:1228)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:1220)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.$anonfun$initialize$2(ShuffleBlockFetcherIterator.scala:729)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.withFetchWaitTimeTracked(ShuffleBlockFetcherIterator.scala:198)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:729)\n",
      "\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:194)\n",
      "\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:232)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\t... 36 more\n",
      "\n",
      ")\n",
      "26/02/16 10:58:05 WARN TaskSetManager: Lost task 1.1 in stage 1076.0 (TID 1609) (172.22.0.3 executor 9): FetchFailed(null, shuffleId=333, mapIndex=-1, mapId=-1, reduceId=200, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 333 partition 200\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1781)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1726)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1725)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1313)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1725)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1359)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1321)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:135)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:67)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:61)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:108)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      ")\n",
      "26/02/16 10:58:05 WARN TaskSetManager: Lost task 0.1 in stage 1076.0 (TID 1610) (172.22.0.3 executor 9): FetchFailed(null, shuffleId=333, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 333 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1781)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1726)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1725)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1313)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1725)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1359)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1321)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:135)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:67)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:61)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:108)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      ")\n",
      "26/02/16 10:58:05 WARN TaskSetManager: Lost task 2.0 in stage 1076.0 (TID 1611) (172.22.0.3 executor 9): FetchFailed(null, shuffleId=333, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 333 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1781)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1726)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1725)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:630)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:628)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1313)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1725)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1359)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1321)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:135)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:67)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:61)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:108)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      ")\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장 완료\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 저장 경로 생성\n",
    "# =========================\n",
    "base_output_path = os.path.join(\n",
    "    OUTPUT_BASE,\n",
    "    f\"{REGION}_{SIGUNGU.replace(' ', '_')}\"\n",
    ")\n",
    "\n",
    "filtered_path = base_output_path + \"_filtered_final_toji\"\n",
    "group_path = base_output_path + \"_toji_group\"\n",
    "\n",
    "print(\"filtered 저장 경로:\", filtered_path)\n",
    "print(\"group 저장 경로:\", group_path)\n",
    "\n",
    "# =========================\n",
    "# parquet 저장\n",
    "# =========================\n",
    "filtered_final_toji_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(filtered_path)\n",
    "\n",
    "toji_group_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(group_path)\n",
    "\n",
    "print(\"✅ 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d4ad1e3-8cef-40e8-96e0-00ddf0ceebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8728721-e983-4242-823c-36c1807991f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
