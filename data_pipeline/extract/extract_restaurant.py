import os
import sys
import re
import time
import logging
import requests
from zipfile import ZipFile
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Tuple, List
from urllib.parse import urlencode

from dotenv import load_dotenv

# slack_utils.pyë¥¼ ì°¾ê¸° ìœ„í•´ ìƒìœ„ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).resolve().parent.parent))
from data_pipeline.utils.slack_utils import SlackNotifier

# =========================
# ENV
# =========================
load_dotenv()

# =========================
# Config
# =========================
@dataclass
class Config:
    detail_url: str = "https://www.data.go.kr/data/15083033/fileData.do"
    download_base_url: str = "https://www.data.go.kr/cmm/cmm/fileDownload.do"
    project_root: str = "data/restaurant"
    retries: int = 3
    retry_sleep_sec: int = 5
    timeout_sec: int = 60
    parquet_compression: str = "snappy"
    parquet_overwrite: bool = False
    skip_if_latest_quarter_exists: bool = True
    force_run: bool = False
    slack_webhook_url: Optional[str] = os.getenv("SLACK_WEBHOOK_URL")

# =========================
# Logger & Helpers
# =========================
def build_logger(log_file: Path) -> logging.Logger:
    logger = logging.getLogger("restaurant_extract")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    fmt = logging.Formatter("%(asctime)s | %(levelname)s | %(message)s", "%Y-%m-%d %H:%M:%S")
    
    sh = logging.StreamHandler(sys.stdout)
    sh.setFormatter(fmt)
    fh = logging.FileHandler(log_file, encoding="utf-8")
    fh.setFormatter(fmt)
    
    logger.addHandler(sh)
    logger.addHandler(fh)
    return logger

def init_run_dirs(cfg: Config) -> dict:
    base = Path(cfg.project_root) / "_work"
    paths = {
        "base": base,
        "zips": base / "zips",
        "unzipped": base / "unzipped",
        "log_file": base / "run.log",
        "parquet_root": Path(cfg.project_root) / "parquet",
    }
    paths["zips"].mkdir(parents=True, exist_ok=True)
    paths["unzipped"].mkdir(parents=True, exist_ok=True)
    return paths

# =========================
# Quarter helpers
# =========================
def yyyymmdd_to_quarter(tag: str) -> str:
    if not re.fullmatch(r"\d{8}", tag):
        raise ValueError(f"Invalid YYYYMMDD: {tag}")

    year = tag[:4]
    mmdd = tag[4:]

    if mmdd == "0331": q = "Q1"
    elif mmdd == "0630": q = "Q2"
    elif mmdd == "0930": q = "Q3"
    elif mmdd == "1231": q = "Q4"
    else:
        raise ValueError(f"Invalid quarter date (0331/0630/0930/1231): {tag}")

    return f"{year}{q}"


def quarter_success_marker(parquet_root: Path, quarter_tag: str) -> Path:
    return parquet_root / f"dataset={quarter_tag}" / "_SUCCESS"


def latest_quarter_already_processed(cfg: Config, paths: dict, quarter_tag: str) -> bool:
    return quarter_success_marker(paths["parquet_root"], quarter_tag).exists()


# =========================
# Download helpers
# =========================
def fetch_detail_html(cfg: Config) -> str:
    headers = {"User-Agent": "Mozilla/5.0", "Referer": "https://www.data.go.kr/", "Accept": "text/html,*/*"}
    r = requests.get(cfg.detail_url, headers=headers, timeout=cfg.timeout_sec)
    r.raise_for_status()
    return r.text


def resolve_latest_dataset_yyyymmdd(cfg: Config, logger: logging.Logger) -> str:
    logger.info(f"Fetching detail page: {cfg.detail_url}")
    html = fetch_detail_html(cfg)
    m = re.search(r"ì†Œìƒê³µì¸ì‹œì¥ì§„í¥ê³µë‹¨_ìƒê°€\(ìƒê¶Œ\)ì •ë³´_(\d{8})", html)
    if not m:
        m = re.search(r"_(\d{8})", html)
    if not m:
        raise RuntimeError("Failed to parse latest dataset YYYYMMDD")

    yyyymmdd = m.group(1)
    logger.info(f"Latest dataset YYYYMMDD on portal: {yyyymmdd}")
    return yyyymmdd


def resolve_download_params(cfg: Config, logger: logging.Logger) -> Tuple[str, str]:
    html = fetch_detail_html(cfg)
    m1 = re.search(r"atchFileId=?(FILE_[0-9]+)", html)
    m2 = re.search(r"fileDetailSn=([0-9]+)", html)
    if not (m1 and m2):
        raise RuntimeError("Failed to parse atchFileId / fileDetailSn")
    return m1.group(1), m2.group(1)


def is_zip(path: Path) -> bool:
    if not path.exists() or path.stat().st_size < 4:
        return False
    with open(path, "rb") as f:
        return f.read(4) == b"PK\x03\x04"


def download_zip(cfg: Config, paths: dict, logger: logging.Logger) -> Path:
    atch, sn = resolve_download_params(cfg, logger)
    url = cfg.download_base_url + "?" + urlencode({"atchFileId": atch, "fileDetailSn": sn})
    zip_path = paths["zips"] / "sme_store.zip"
    headers = {"User-Agent": "Mozilla/5.0", "Referer": cfg.detail_url, "Accept": "*/*"}

    for attempt in range(1, cfg.retries + 1):
        try:
            logger.info(f"Downloading ({attempt}/{cfg.retries})")
            with requests.get(url, headers=headers, stream=True, timeout=cfg.timeout_sec) as r:
                r.raise_for_status()
                with open(zip_path, "wb") as f:
                    for chunk in r.iter_content(8192):
                        if chunk: f.write(chunk)

            if not is_zip(zip_path):
                raise ValueError("Downloaded file is not a ZIP")
            return zip_path
        except Exception as e:
            logger.error(f"Download failed: {e}")
            if attempt < cfg.retries: time.sleep(cfg.retry_sleep_sec)
            else: raise


def extract_zip(zip_path: Path, paths: dict, logger: logging.Logger):
    logger.info(f"Extracting: {zip_path}")
    with ZipFile(zip_path, "r") as z:
        z.extractall(paths["unzipped"])


# =========================
# Parquet
# =========================
def find_dataset_dir(unzipped_dir: Path) -> Path:
    children = list(unzipped_dir.iterdir())
    if len(children) == 1 and children[0].is_dir():
        return children[0]
    return unzipped_dir


def list_csv_files(dataset_dir: Path) -> List[Path]:
    return sorted([p for p in dataset_dir.rglob("*.csv") if p.is_file()])


def parse_region_from_filename(name: str) -> str:
    m = re.search(r"_([ê°€-í£]+)_\d{6}\.csv$", name)
    return m.group(1) if m else "unknown"


def write_success_marker(parquet_root: Path, quarter_tag: str):
    d = parquet_root / f"dataset={quarter_tag}"
    d.mkdir(parents=True, exist_ok=True)
    (d / "_SUCCESS").write_text(f"ok {time.strftime('%Y-%m-%d %H:%M:%S')}\n", encoding="utf-8")


def convert_csvs_to_parquet(cfg: Config, paths: dict, logger: logging.Logger, quarter_tag: str) -> int:
    dataset_dir = find_dataset_dir(paths["unzipped"])
    csv_files = list_csv_files(dataset_dir)

    if not csv_files:
        logger.warning("No CSV files found to convert.")
        return 0

    parquet_base = paths["parquet_root"] / f"dataset={quarter_tag}"
    parquet_base.mkdir(parents=True, exist_ok=True)

    import pandas as pd
    import pyarrow as pa
    import pyarrow.parquet as pq

    written = 0
    total = len(csv_files)

    for idx, csv_path in enumerate(csv_files, 1):
        region = parse_region_from_filename(csv_path.name)
        out_dir = parquet_base / f"region={region}"
        out_dir.mkdir(parents=True, exist_ok=True)
        out_path = out_dir / "part.parquet"

        # íŒŒì¼ ì¡´ì¬ ì‹œ ìŠ¤í‚µ ë¡œê·¸
        if out_path.exists() and not cfg.parquet_overwrite:
            logger.info(f"[{idx}/{total}] Skipping (already exists): region={region}")
            written += 1
            continue

        logger.info(f"[{idx}/{total}] Converting: {csv_path.name} -> region={region}")
        
        writer = None
        try:
            # chunk ë‹¨ìœ„ë¡œ ì½ì–´ì„œ ë©”ëª¨ë¦¬ íš¨ìœ¨í™”
            reader = pd.read_csv(
                csv_path, 
                encoding="utf-8-sig", 
                chunksize=300_000, 
                dtype="string", 
                low_memory=False
            )
            
            for chunk in reader:
                table = pa.Table.from_pandas(chunk.astype("string"), preserve_index=False)
                if writer is None:
                    writer = pq.ParquetWriter(out_path, table.schema, compression=cfg.parquet_compression)
                writer.write_table(table)
            
            written += 1
            logger.info(f"Successfully saved: {out_path}")

        except Exception as e:
            logger.error(f"Failed to convert {csv_path.name}: {e}")
            raise # í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•˜ë©´ ì¤‘ë‹¨ (í•„ìš”ì— ë”°ë¼ continueë¡œ ë³€ê²½ ê°€ëŠ¥)
        finally:
            if writer:
                writer.close()

    # ì™„ë£Œ í›„ _SUCCESS íŒŒì¼ ìƒì„±
    write_success_marker(paths["parquet_root"], quarter_tag)
    logger.info(f"Parquet conversion complete. Total {written} regions processed.")
    
    return written


# =========================
# Main Logic
# =========================
def main():
    cfg = Config()
    paths = init_run_dirs(cfg)
    logger = build_logger(paths["log_file"])
    
    # 1. ì•Œë¦¬ë¯¸ ì´ˆê¸°í™”
    notifier = SlackNotifier(cfg.slack_webhook_url, "EXTRACT-ì‹ë‹¹ì •ë³´", logger)

    logger.info("===== EXTRACT START =====")
    
    try:
        # 0. í¬í„¸ ë°ì´í„° ë‚ ì§œ í™•ì¸
        latest_yyyymmdd = resolve_latest_dataset_yyyymmdd(cfg, logger)
        latest_quarter = yyyymmdd_to_quarter(latest_yyyymmdd)
        
        notifier.info("ì‘ì—… ì‹œì‘", f"ëŒ€ìƒ ë¶„ê¸°: {latest_quarter} (Portal Date: {latest_yyyymmdd})")

        # [STEP 1] ZIP í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
        zip_path = paths["zips"] / "sme_store.zip"
        if zip_path.exists() and is_zip(zip_path) and not cfg.force_run:
            logger.warning("â­ ZIP íŒŒì¼ì´ ì´ë¯¸ ë¡œì»¬ì— ì¡´ì¬í•˜ì—¬ ë‹¤ìš´ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        else:
            logger.info("ğŸŒ ê³µê³µë°ì´í„°í¬í„¸ì—ì„œ ZIP íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            zip_path = download_zip(cfg, paths, logger)
            logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {zip_path.name}")

        # [STEP 2] CSV í™•ì¸ ë° ì••ì¶• í•´ì œ
        existing_csvs = list_csv_files(paths["unzipped"])
        if existing_csvs and not cfg.force_run:
            logger.warning("â­ ì••ì¶• í•´ì œëœ CSVê°€ ì´ë¯¸ ì¡´ì¬í•˜ì—¬ Skipí•©ë‹ˆë‹¤.")
        else:
            logger.info("ğŸ”“ ì••ì¶• í•´ì œ(Extract ZIP) ì‹œì‘...")
            extract_zip(zip_path, paths, logger)
            logger.info("âœ… ì••ì¶• í•´ì œ ì™„ë£Œ")

        # [STEP 3] Parquet í™•ì¸ ë° ë³€í™˜
        if (cfg.skip_if_latest_quarter_exists and not cfg.force_run 
            and latest_quarter_already_processed(cfg, paths, latest_quarter)):
            logger.warning(f"â­ {latest_quarter} Parquet ê²°ê³¼ë¬¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ë³€í™˜ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            n_parquet = "Skipped (Existing)"
        else:
            logger.info(f"ğŸ“¦ CSV -> Parquet ë³€í™˜ ë° ì§€ì—­ë³„ íŒŒí‹°ì…”ë‹ ì‹œì‘...")
            written_count = convert_csvs_to_parquet(cfg, paths, logger, latest_quarter)
            n_parquet = f"Success ({written_count} regions)"
            logger.info(f"âœ… ë³€í™˜ ì™„ë£Œ: {n_parquet}")

        # [SUCCESS]
        notifier.success("ì‘ì—… ì™„ë£Œ", f"ë¶„ê¸° ë°ì´í„° ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n*ê²°ê³¼*: {n_parquet}")
        logger.info(f"===== SUCCESS (status={n_parquet}) =====")

    except Exception as e:
        # [CRITICAL ERROR]
        logger.error(f"ğŸš¨ íŒŒì´í”„ë¼ì¸ ì—ëŸ¬ ë°œìƒ: {str(e)}")
        notifier.error("ì†Œìƒê³µì¸ ìƒê°€ì •ë³´ ìˆ˜ì§‘ ì¤‘ë‹¨", e)
        sys.exit(1)

if __name__ == "__main__":
    main()