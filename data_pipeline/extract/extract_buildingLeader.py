import os
import sys
import time
import re
import zipfile
import logging
import traceback
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Optional

import requests
from dotenv import load_dotenv
from playwright.sync_api import sync_playwright

import pandas as pd

# slack_utils.pyë¥¼ ì°¾ê¸° ìœ„í•´ ìƒìœ„ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).resolve().parent.parent))
from data_pipeline.utils.slack_utils import SlackNotifier

# =========================
# ENV
# =========================
load_dotenv()

# =========================
# Config
# =========================
@dataclass
class HubConfig:
    url: str = "https://www.hub.go.kr/portal/opn/lps/idx-lgcpt-pvsn-srvc-list.do"
    category_label: str = "ê±´ì¶•ë¬¼ëŒ€ì¥"
    service_keyword: str = "í‘œì œë¶€"
    usage_reason_label: str = "ì°¸ê³ ìë£Œ"
    headless: bool = True
    retries: int = 3
    retry_sleep_sec: int = 5
    timeout_ms: int = 30_000
    work_dir: str = "data/buildingLeader/_work"
    slack_webhook_url: Optional[str] = os.getenv("SLACK_WEBHOOK_URL")
    unzip_txt_only: bool = True

# =========================
# Logging Helpers
# =========================
def build_logger(log_file: Path) -> logging.Logger:
    logger = logging.getLogger("hub_extract")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    fmt = logging.Formatter("%(asctime)s | %(levelname)s | %(message)s", "%Y-%m-%d %H:%M:%S")
    
    sh = logging.StreamHandler(sys.stdout)
    sh.setFormatter(fmt)
    fh = logging.FileHandler(log_file, encoding="utf-8")
    fh.setFormatter(fmt)
    
    logger.addHandler(sh)
    logger.addHandler(fh)
    return logger

# =========================
# FS Helpers
# =========================
def create_run_dirs(base: str):
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    root = Path(base) / f"run_{ts}"
    dirs = {
        "root": root,
        "zips": root / "zips",
        "unzipped": root / "unzipped",
        "artifacts": root / "artifacts",
        "logs": root / "logs",
    }
    for d in dirs.values():
        d.mkdir(parents=True, exist_ok=True)
    return dirs


def unzip_zip(zip_path: Path, out_dir: Path, txt_only: bool, logger):
    out_dir.mkdir(parents=True, exist_ok=True)
    extracted = []

    with zipfile.ZipFile(zip_path, "r") as zf:
        zf.extractall(out_dir)
        for name in zf.namelist():
            p = out_dir / name
            if p.is_file():
                extracted.append(p)

    if txt_only:
        txts = [p for p in extracted if p.suffix.lower() == ".txt"]
        logger.info(f"Unzipped TXT files: {len(txts)}")
        return txts

    logger.info(f"Unzipped files: {len(extracted)}")
    return extracted


# =========================
# Core Helpers
# =========================
def parse_actual_data_year_month(container) -> tuple[int, int]:
    """
    'í‘œì œë¶€ (2025ë…„ 12ì›”)' â†’ (2025, 12)
    """
    text = container.inner_text()
    m = re.search(r"\((\d{4})ë…„\s*(\d{1,2})ì›”\)", text)
    if not m:
        raise RuntimeError("ì‹¤ì œ ë°ì´í„° ê¸°ì¤€ì›” íŒŒì‹± ì‹¤íŒ¨")
    return int(m.group(1)), int(m.group(2))


# ---------------------------------------------------------------------------
# 1. í—ˆìš©ëœ ì§€ì—­ ëª…ì¹­ ë¦¬ìŠ¤íŠ¸ (ì´ ëª…ì¹­ìœ¼ë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ ì €ì¥ ì•ˆ í•¨)
# ---------------------------------------------------------------------------
SIDO_MAP = {
    "ì„œìš¸": "ì„œìš¸", "ë¶€ì‚°": "ë¶€ì‚°", "ëŒ€êµ¬": "ëŒ€êµ¬", "ì¸ì²œ": "ì¸ì²œ",
    "ê´‘ì£¼": "ê´‘ì£¼", "ëŒ€ì „": "ëŒ€ì „", "ìš¸ì‚°": "ìš¸ì‚°", "ì„¸ì¢…": "ì„¸ì¢…",
    "ê²½ê¸°": "ê²½ê¸°", "ê°•ì›": "ê°•ì›", "ì¶©ë¶": "ì¶©ë¶", "ì¶©ë‚¨": "ì¶©ë‚¨",
    "ì „ë¶": "ì „ë¶", "ì „ë‚¨": "ì „ë‚¨", "ê²½ë¶": "ê²½ë¶", "ê²½ë‚¨": "ê²½ë‚¨", "ì œì£¼": "ì œì£¼",
    # ê¸´ ëª…ì¹­ ëŒ€ì‘
    "ì„œìš¸íŠ¹ë³„ì‹œ": "ì„œìš¸", "ë¶€ì‚°ê´‘ì—­ì‹œ": "ë¶€ì‚°", "ëŒ€êµ¬ê´‘ì—­ì‹œ": "ëŒ€êµ¬", "ì¸ì²œê´‘ì—­ì‹œ": "ì¸ì²œ",
    "ê´‘ì£¼ê´‘ì—­ì‹œ": "ê´‘ì£¼", "ëŒ€ì „ê´‘ì—­ì‹œ": "ëŒ€ì „", "ìš¸ì‚°ê´‘ì—­ì‹œ": "ìš¸ì‚°", "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ": "ì„¸ì¢…",
    "ê²½ê¸°ë„": "ê²½ê¸°", "ê°•ì›ë„": "ê°•ì›", "ê°•ì›íŠ¹ë³„ìì¹˜ë„": "ê°•ì›",
    "ì¶©ì²­ë¶ë„": "ì¶©ë¶", "ì¶©ì²­ë‚¨ë„": "ì¶©ë‚¨", "ì „ë¼ë¶ë„": "ì „ë¶", "ì „ë¶íŠ¹ë³„ìì¹˜ë„": "ì „ë¶",
    "ì „ë¼ë‚¨ë„": "ì „ë‚¨", "ê²½ìƒë¶ë„": "ê²½ë¶", "ê²½ìƒë‚¨ë„": "ê²½ë‚¨", "ì œì£¼íŠ¹ë³„ìì¹˜ë„": "ì œì£¼"
}

def parquet_already_exists(parquet_root: Path, year: int, month: int) -> bool:
    target_dir = parquet_root / f"year={year}" / f"month={month:02d}"
    return target_dir.exists() and any(target_dir.iterdir())

# ---------------------------------------------------------------------------
# 2. í•µì‹¬ í•¨ìˆ˜: TXT -> Parquet (ì™¸êµ­ ì œì™¸ + ë‘ ê¸€ì ë³€í™˜)
# ---------------------------------------------------------------------------
def txt_to_parquet_by_region(
    txt_path: Path,
    parquet_root: Path,
    year: int,
    month: int,
    chunksize: int = 500_000,
):
    columns = [
        "ê´€ë¦¬_ê±´ì¶•ë¬¼ëŒ€ì¥_PK", "ëŒ€ì¥_êµ¬ë¶„_ì½”ë“œ", "ëŒ€ì¥_êµ¬ë¶„_ì½”ë“œ_ëª…", "ëŒ€ì¥_ì¢…ë¥˜_ì½”ë“œ", "ëŒ€ì¥_ì¢…ë¥˜_ì½”ë“œ_ëª…",
        "ëŒ€ì§€_ìœ„ì¹˜", "ë„ë¡œëª…_ëŒ€ì§€_ìœ„ì¹˜", "ê±´ë¬¼_ëª…", "ì‹œêµ°êµ¬_ì½”ë“œ", "ë²•ì •ë™_ì½”ë“œ", 
        "ëŒ€ì§€_êµ¬ë¶„_ì½”ë“œ", "ë²ˆ", "ì§€", "íŠ¹ìˆ˜ì§€_ëª…", "ë¸”ë¡", "ë¡œíŠ¸", "ì™¸í•„ì§€_ìˆ˜",
        "ìƒˆì£¼ì†Œ_ë„ë¡œ_ì½”ë“œ", "ìƒˆì£¼ì†Œ_ë²•ì •ë™_ì½”ë“œ", "ìƒˆì£¼ì†Œ_ì§€ìƒì§€í•˜_ì½”ë“œ", "ìƒˆì£¼ì†Œ_ë³¸_ë²ˆ", "ìƒˆì£¼ì†Œ_ë¶€_ë²ˆ",
        "ë™_ëª…", "ì£¼_ë¶€ì†_êµ¬ë¶„_ì½”ë“œ", "ì£¼_ë¶€ì†_êµ¬ë¶„_ì½”ë“œ_ëª…", "ëŒ€ì§€_ë©´ì (ã¡)", "ê±´ì¶•_ë©´ì (ã¡)",
        "ê±´í_ìœ¨(%)", "ì—°ë©´ì (ã¡)", "ìš©ì _ë¥ _ì‚°ì •_ì—°ë©´ì (ã¡)", "ìš©ì _ë¥ (%)", "êµ¬ì¡°_ì½”ë“œ",
        "êµ¬ì¡°_ì½”ë“œ_ëª…", "ê¸°íƒ€_êµ¬ì¡°", "ì£¼_ìš©ë„_ì½”ë“œ", "ì£¼_ìš©ë„_ì½”ë“œ_ëª…", "ê¸°íƒ€_ìš©ë„",
        "ì§€ë¶•_ì½”ë“œ", "ì§€ë¶•_ì½”ë“œ_ëª…", "ê¸°íƒ€_ì§€ë¶•", "ì„¸ëŒ€_ìˆ˜(ì„¸ëŒ€)", "ê°€êµ¬_ìˆ˜(ê°€êµ¬)",
        "ë†’ì´(m)", "ì§€ìƒ_ì¸µ_ìˆ˜", "ì§€í•˜_ì¸µ_ìˆ˜", "ìŠ¹ìš©_ìŠ¹ê°•ê¸°_ìˆ˜", "ë¹„ìƒìš©_ìŠ¹ê°•ê¸°_ìˆ˜",
        "ë¶€ì†_ê±´ì¶•ë¬¼_ìˆ˜", "ë¶€ì†_ê±´ì¶•ë¬¼_ë©´ì (ã¡)", "ì´_ë™_ì—°ë©´ì (ã¡)", "ì˜¥ë‚´_ê¸°ê³„ì‹_ëŒ€ìˆ˜(ëŒ€)",
        "ì˜¥ë‚´_ê¸°ê³„ì‹_ë©´ì (ã¡)", "ì˜¥ì™¸_ê¸°ê³„ì‹_ëŒ€ìˆ˜(ëŒ€)", "ì˜¥ì™¸_ê¸°ê³„ì‹_ë©´ì (ã¡)", "ì˜¥ë‚´_ìì£¼ì‹_ëŒ€ìˆ˜(ëŒ€)",
        "ì˜¥ë‚´_ìì£¼ì‹_ë©´ì (ã¡)", "ì˜¥ì™¸_ìì£¼ì‹_ëŒ€ìˆ˜(ëŒ€)", "ì˜¥ì™¸_ìì£¼ì‹_ë©´ì (ã¡)", "í—ˆê°€_ì¼",
        "ì°©ê³µ_ì¼", "ì‚¬ìš©ìŠ¹ì¸_ì¼", "í—ˆê°€ë²ˆí˜¸_ë…„", "í—ˆê°€ë²ˆí˜¸_ê¸°ê´€_ì½”ë“œ", "í—ˆê°€ë²ˆí˜¸_ê¸°ê´€_ì½”ë“œ_ëª…",
        "í—ˆê°€ë²ˆí˜¸_êµ¬ë¶„_ì½”ë“œ", "í—ˆê°€ë²ˆí˜¸_êµ¬ë¶„_ì½”ë“œ_ëª…", "í˜¸_ìˆ˜(í˜¸)", "ì—ë„ˆì§€íš¨ìœ¨_ë“±ê¸‰",
        "ì—ë„ˆì§€ì ˆê°_ìœ¨", "ì—ë„ˆì§€_EPIì ìˆ˜", "ì¹œí™˜ê²½_ê±´ì¶•ë¬¼_ë“±ê¸‰", "ì¹œí™˜ê²½_ê±´ì¶•ë¬¼_ì¸ì¦ì ìˆ˜",
        "ì§€ëŠ¥í˜•_ê±´ì¶•ë¬¼_ë“±ê¸‰", "ì§€ëŠ¥í˜•_ê±´ì¶•ë¬¼_ì¸ì¦ì ìˆ˜", "ìƒì„±_ì¼ì", "ë‚´ì§„_ì„¤ê³„_ì ìš©_ì—¬ë¶€", "ë‚´ì§„_ëŠ¥ë ¥"
    ]

    encodings = ["cp949", "euc-kr", "utf-8"]
    reader = None

    for enc in encodings:
        try:
            reader = pd.read_csv(
                txt_path, sep="|", encoding=enc, chunksize=chunksize, 
                names=columns, dtype=str, low_memory=False, index_col=False
            )
            next(reader) 
            reader = pd.read_csv(txt_path, sep="|", encoding=enc, chunksize=chunksize, names=columns, dtype=str, index_col=False)
            break
        except Exception:
            continue

    if reader is None: raise RuntimeError("txt read failed")

    for i, df in enumerate(reader):
        df['ëŒ€ì§€_ìœ„ì¹˜'] = df['ëŒ€ì§€_ìœ„ì¹˜'].fillna('')

        # 1. ì²« ë‹¨ì–´ ì¶”ì¶œ (ex: 'ì„œìš¸íŠ¹ë³„ì‹œ', 'ì „ë¶íŠ¹ë³„ìì¹˜ë„')
        df['raw_sido'] = df['ëŒ€ì§€_ìœ„ì¹˜'].str.split().str[0].str.strip()

        # 2. ì§€ëª… ë§¤í•‘ (ex: 'ì„œìš¸íŠ¹ë³„ì‹œ' -> 'ì„œìš¸')
        # SIDO_MAPì— ì—†ëŠ” ê²ƒ(ì™¸êµ­, ë²ˆì§€ìˆ˜ ë“±)ì€ NaNì´ ë¨
        df['sido'] = df['raw_sido'].map(SIDO_MAP)

        # 3. ì™¸êµ­ ë° ë²ˆì§€ìˆ˜ ë°ì´í„° í•„í„°ë§ (NaN ì œê±°)
        valid_df = df.dropna(subset=['sido']).copy()

        if valid_df.empty: continue

        for sido, group in valid_df.groupby('sido'):
            target_dir = parquet_root / f"year={year}" / f"month={month:02d}" / f"region={sido}"
            target_dir.mkdir(parents=True, exist_ok=True)

            out_path = target_dir / f"part-{i:05d}.parquet"
            # ë³´ì¡° ì»¬ëŸ¼ ì œê±° í›„ ì €ì¥
            save_df = group.drop(columns=['raw_sido'])
            save_df.to_parquet(out_path, index=False, engine='pyarrow')

# =========================
# Main Logic
# =========================
def run(cfg: HubConfig):
    # ê²½ë¡œ ì„¤ì •
    base_path = Path(cfg.work_dir)
    zip_dir = base_path / "zips"
    unzip_dir = base_path / "unzipped"
    parquet_root = Path("data/buildingLeader/parquet")

    for d in [zip_dir, unzip_dir, parquet_root]:
        d.mkdir(parents=True, exist_ok=True)

    # ë¡œê±° ë° ì•Œë¦¬ë¯¸ ì´ˆê¸°í™”
    logger = build_logger(base_path / "run.log")
    notifier = SlackNotifier(cfg.slack_webhook_url, "EXTRACT-ê±´ì¶•ë¬¼ëŒ€ì¥", logger)

    logger.info("===== EXTRACT START =====")

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=cfg.headless)
        context = browser.new_context(accept_downloads=True)
        page = context.new_page()
        page.set_default_timeout(cfg.timeout_ms)

        try:
            # [START] í˜ì´ì§€ ì ‘ì† ë° ê²€ìƒ‰
            logger.info(f"ğŸŒ í˜ì´ì§€ ì ‘ì† ì¤‘: {cfg.url}")
            page.goto(cfg.url, wait_until="domcontentloaded")

            page.locator("select").filter(has=page.locator(f"option:has-text('{cfg.category_label}')")).select_option(label=cfg.category_label)
            page.locator("input[type='text']").first.fill(cfg.service_keyword)
            page.get_by_role("button", name="ê²€ìƒ‰").click()

            page.wait_for_selector(f"text={cfg.service_keyword}")
            row = page.locator(f"text={cfg.service_keyword}").first
            container = row
            for _ in range(8):
                if "ë°ì´í„°ì œê³µë…„ì›”" in container.inner_text(): break
                container = container.locator("xpath=..")

            data_year, data_month = parse_actual_data_year_month(container)
            notifier.info("ì‘ì—… ì‹œì‘", f"ëŒ€ìƒ ê¸°ê°„: {data_year}ë…„ {data_month:02d}ì›”")

            # [STEP 1] ZIP í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
            zip_path = None
            existing_zips = list(zip_dir.glob("*.zip"))
            existing_txts = list(unzip_dir.glob("*.txt"))

            if existing_txts:
                logger.warning(f"â­ TXT íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ë‹¤ìš´ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            elif existing_zips:
                zip_path = existing_zips[0]
                logger.warning(f"â­ ZIP íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ë‹¤ìš´ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            else:
                logger.info("ğŸ’¾ ë¸Œë¼ìš°ì €ë¥¼ í†µí•´ ì „ì²´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
                download_btn = None
                tmp = container
                for _ in range(6):
                    btn = tmp.locator("button", has_text="ì „ì²´")
                    if btn.count() > 0:
                        download_btn = btn.first
                        break
                    tmp = tmp.locator("xpath=..")
                
                if not download_btn: raise RuntimeError("ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                with page.expect_download() as dl_info:
                    download_btn.click()
                    page.get_by_text(cfg.usage_reason_label, exact=True).click()
                    page.get_by_role("button", name="í™•ì¸").click()

                download = dl_info.value
                zip_path = zip_dir / download.suggested_filename
                download.save_as(zip_path)
                logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {zip_path.name}")

            # [STEP 2] ì••ì¶• í•´ì œ
            if not existing_txts:
                logger.info("ğŸ”“ ì••ì¶• í•´ì œ(Unzip) ì‹œì‘...")
                unzip_zip(zip_path, unzip_dir, cfg.unzip_txt_only, logger)
                existing_txts = list(unzip_dir.glob("*.txt"))
                logger.info(f"âœ… ì••ì¶• í•´ì œ ì™„ë£Œ ({len(existing_txts)}ê°œ TXT)")
            else:
                logger.warning("â­ TXTê°€ ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ì••ì¶• í•´ì œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

            # [STEP 3] Parquet ë³€í™˜ ë° ìµœì¢… í™•ì¸
            if parquet_already_exists(parquet_root, data_year, data_month):
                logger.warning(f"â­ {data_year}-{data_month:02d} Parquet ê²°ê³¼ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
                notifier.info("ì‘ì—… ê±´ë„ˆëœœ", f"{data_year}-{data_month:02d} ë°ì´í„°ê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            else:
                logger.info(f"ğŸ“¦ CSV(TXT) -> Parquet ë³€í™˜ ì‹œì‘...")
                for txt_file in existing_txts:
                    if txt_file.stat().st_size == 0: continue
                    txt_to_parquet_by_region(txt_file, parquet_root, data_year, data_month)
                
                # [SUCCESS]
                notifier.success("ì‘ì—… ì™„ë£Œ", f"{data_year}ë…„ {data_month:02d}ì›” ë°ì´í„° ì§€ì—­ë³„ íŒŒí‹°ì…”ë‹ ì™„ë£Œ")
                logger.info("âœ… ëª¨ë“  ë³€í™˜ ê³µì • ì™„ë£Œ")

            browser.close()

        except Exception as e:
            # [CRITICAL ERROR]
            logger.error(f"ğŸš¨ íŒŒì´í”„ë¼ì¸ ì—ëŸ¬ ë°œìƒ: {str(e)}")
            notifier.error("ê±´ì¶•ë¬¼ëŒ€ì¥ ìˆ˜ì§‘ ì¤‘ë‹¨", e)
            if 'browser' in locals(): browser.close()
            raise

if __name__ == "__main__":
    cfg = HubConfig()
    run(cfg)