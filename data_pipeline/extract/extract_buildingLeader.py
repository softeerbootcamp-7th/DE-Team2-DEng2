import os
import sys
import re
import zipfile
import logging
import traceback
import shutil
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Optional

from dotenv import load_dotenv
load_dotenv()
from playwright.sync_api import sync_playwright
import pandas as pd

# slack_utils.pyë¥¼ ì°¾ê¸° ìœ„í•´ ìƒìœ„ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).resolve().parent.parent.parent))

try:
    from data_pipeline.utils.slack_utils import SlackNotifier
except ImportError:
    class SlackNotifier:
        def __init__(self, url, title, logger): self.logger = logger
        def info(self, m, d): self.logger.info(f"[SLACK INFO] {m}: {d}")
        def success(self, m, d): self.logger.info(f"[SLACK SUCCESS] {m}: {d}")
        def error(self, m, e): self.logger.error(f"[SLACK ERROR] {m}: {e}")

# =========================
# Logging
# =========================
def build_logger(log_file: Path) -> logging.Logger:
    logger = logging.getLogger("hub_extract")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    fmt = logging.Formatter("%(asctime)s | %(levelname)s | %(message)s", "%Y-%m-%d %H:%M:%S")
    sh = logging.StreamHandler(sys.stdout)
    sh.setFormatter(fmt)
    fh = logging.FileHandler(log_file, encoding="utf-8")
    fh.setFormatter(fmt)
    logger.addHandler(sh)
    logger.addHandler(fh)
    return logger

@dataclass
class HubConfig:
    url: str = "https://www.hub.go.kr/portal/opn/lps/idx-lgcpt-pvsn-srvc-list.do"
    category_label: str = "ê±´ì¶•ë¬¼ëŒ€ì¥"
    service_keyword: str = "í‘œì œë¶€"
    usage_reason_label: str = "ì°¸ê³ ìë£Œ"
    headless: bool = True
    timeout_ms: int = 40_000
    # ê²½ë¡œ ì„¤ì • ë³€ê²½
    base_dir: str = "data/bronze/buildingLeader"
    work_dir: str = "data/bronze/buildingLeader/_work"
    parquet_dir: str = "data/bronze/buildingLeader/parquet"
    slack_webhook_url: Optional[str] = os.getenv("SLACK_WEBHOOK_URL")

# =========================
# FS Helpers (year/month êµ¬ì¡°)
# =========================
def get_target_paths(cfg: HubConfig, data_year: int, data_month: int):
    """year=YYYY/month=MM í˜•ì‹ìœ¼ë¡œ ê²½ë¡œ êµ¬ì„±"""
    # ê³µí†µ ê²½ë¡œ ë¶€ë¶„
    partition_path = f"year={data_year}/month={data_month:02d}"

    work_root = Path(cfg.work_dir) / partition_path
    parquet_root = Path(cfg.parquet_dir) / partition_path

    paths = {
        "work_root": work_root,
        "zips": work_root / "zips",
        "unzipped": work_root / "unzipped",
        "logs": work_root / "logs",
        "parquet_root": parquet_root 
    }

    for p in paths.values():
        p.mkdir(parents=True, exist_ok=True)
    return paths

def remove_old_data_runs(cfg: HubConfig, data_year: int, data_month: int, logger: logging.Logger):
    """
    M, M-1 ì›”ì„ ì œì™¸í•œ ëª¨ë“  year=*/month=* í´ë”ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
    """
    target_month_str = f"month={data_month:02d}"
    target_year_str = f"year={data_year}"
    
    if data_month == 1:
        prev_month_str = "month=12"
        prev_year_str = f"year={data_year - 1}"
    else:
        prev_month_str = f"month={data_month - 1:02d}"
        prev_year_str = f"year={data_year}"

    # ìœ ì§€í•´ì•¼ í•  ìƒëŒ€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    allowed_paths = [
        f"{target_year_str}/{target_month_str}",
        f"{prev_year_str}/{prev_month_str}"
    ]
    
    logger.info(f"ğŸ§¹ ë°ì´í„° ì •ë¦¬ ì‹œì‘ (ìœ ì§€ ëŒ€ìƒ: {allowed_paths})")

    for base_path in [Path(cfg.work_dir), Path(cfg.parquet_dir)]:
        if not base_path.exists(): continue
        
        # ëª¨ë“  month=XX í´ë” íƒìƒ‰
        for month_dir in base_path.glob("year=*/month=*"):
            # ê¸°ì¤€ í´ë”ë¡œë¶€í„°ì˜ ìƒëŒ€ ê²½ë¡œ ì¶”ì¶œ (ì˜ˆ: year=2025/month=11)
            rel_path = f"{month_dir.parent.name}/{month_dir.name}"
            
            if rel_path not in allowed_paths:
                logger.info(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ: {base_path.name}/{rel_path}")
                shutil.rmtree(month_dir)
                
                # ë§Œì•½ í•´ë‹¹ ì—°ë„(year=) í´ë”ê°€ ë¹„ì–´ìˆë‹¤ë©´ ì—°ë„ í´ë”ë„ ì‚­ì œ
                year_dir = month_dir.parent
                if not any(year_dir.iterdir()):
                    year_dir.rmdir()

# =========================
# Core ETL Helpers
# =========================
def parse_actual_data_year_month(container) -> tuple[int, int]:
    text = container.inner_text()
    m = re.search(r"\((\d{4})ë…„\s*(\d{1,2})ì›”\)", text)
    if not m: raise RuntimeError("ì‹¤ì œ ë°ì´í„° ê¸°ì¤€ì›” íŒŒì‹± ì‹¤íŒ¨")
    return int(m.group(1)), int(m.group(2))

def parquet_already_exists(parquet_root: Path, year: int, month: int) -> bool:
    target_dir = parquet_root / f"year={year}" / f"month={month:02d}"
    # í´ë”ê°€ ì¡´ì¬í•˜ê³  ë‚´ë¶€ íŒŒì¼ì´ í•˜ë‚˜ë¼ë„ ìˆëŠ”ì§€ í™•ì¸
    return target_dir.exists() and any(target_dir.iterdir())

SIDO_MAP = {
    "ì„œìš¸": "ì„œìš¸", "ë¶€ì‚°": "ë¶€ì‚°", "ëŒ€êµ¬": "ëŒ€êµ¬", "ì¸ì²œ": "ì¸ì²œ", "ê´‘ì£¼": "ê´‘ì£¼", "ëŒ€ì „": "ëŒ€ì „", 
    "ìš¸ì‚°": "ìš¸ì‚°", "ì„¸ì¢…": "ì„¸ì¢…", "ê²½ê¸°": "ê²½ê¸°", "ê°•ì›": "ê°•ì›", "ì¶©ë¶": "ì¶©ë¶", "ì¶©ë‚¨": "ì¶©ë‚¨",
    "ì „ë¶": "ì „ë¶", "ì „ë‚¨": "ì „ë‚¨", "ê²½ë¶": "ê²½ë¶", "ê²½ë‚¨": "ê²½ë‚¨", "ì œì£¼": "ì œì£¼",
    "ì„œìš¸íŠ¹ë³„ì‹œ": "ì„œìš¸", "ë¶€ì‚°ê´‘ì—­ì‹œ": "ë¶€ì‚°", "ëŒ€êµ¬ê´‘ì—­ì‹œ": "ëŒ€êµ¬", "ì¸ì²œê´‘ì—­ì‹œ": "ì¸ì²œ",
    "ê´‘ì£¼ê´‘ì—­ì‹œ": "ê´‘ì£¼", "ëŒ€ì „ê´‘ì—­ì‹œ": "ëŒ€ì „", "ìš¸ì‚°ê´‘ì—­ì‹œ": "ìš¸ì‚°", "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ": "ì„¸ì¢…",
    "ê²½ê¸°ë„": "ê²½ê¸°", "ê°•ì›ë„": "ê°•ì›", "ê°•ì›íŠ¹ë³„ìì¹˜ë„": "ê°•ì›", "ì¶©ì²­ë¶ë„": "ì¶©ë¶", "ì¶©ì²­ë‚¨ë„": "ì¶©ë‚¨", 
    "ì „ë¼ë¶ë„": "ì „ë¶", "ì „ë¶íŠ¹ë³„ìì¹˜ë„": "ì „ë¶", "ì „ë¼ë‚¨ë„": "ì „ë‚¨", "ê²½ìƒë¶ë„": "ê²½ë¶", "ê²½ìƒë‚¨ë„": "ê²½ë‚¨", "ì œì£¼íŠ¹ë³„ìì¹˜ë„": "ì œì£¼"
}

def txt_to_parquet_by_region(txt_path: Path, partition_root: Path):
    columns = [
        "ê´€ë¦¬_ê±´ì¶•ë¬¼ëŒ€ì¥_PK", "ëŒ€ì¥_êµ¬ë¶„_ì½”ë“œ", "ëŒ€ì¥_êµ¬ë¶„_ì½”ë“œ_ëª…", "ëŒ€ì¥_ì¢…ë¥˜_ì½”ë“œ", "ëŒ€ì¥_ì¢…ë¥˜_ì½”ë“œ_ëª…",
        "ëŒ€ì§€_ìœ„ì¹˜", "ë„ë¡œëª…_ëŒ€ì§€_ìœ„ì¹˜", "ê±´ë¬¼_ëª…", "ì‹œêµ°êµ¬_ì½”ë“œ", "ë²•ì •ë™_ì½”ë“œ", 
        "ëŒ€ì§€_êµ¬ë¶„_ì½”ë“œ", "ë²ˆ", "ì§€", "íŠ¹ìˆ˜ì§€_ëª…", "ë¸”ë¡", "ë¡œíŠ¸", "ì™¸í•„ì§€_ìˆ˜",
        "ìƒˆì£¼ì†Œ_ë„ë¡œ_ì½”ë“œ", "ìƒˆì£¼ì†Œ_ë²•ì •ë™_ì½”ë“œ", "ìƒˆì£¼ì†Œ_ì§€ìƒì§€í•˜_ì½”ë“œ", "ìƒˆì£¼ì†Œ_ë³¸_ë²ˆ", "ìƒˆì£¼ì†Œ_ë¶€_ë²ˆ",
        "ë™_ëª…", "ì£¼_ë¶€ì†_êµ¬ë¶„_ì½”ë“œ", "ì£¼_ë¶€ì†_êµ¬ë¶„_ì½”ë“œ_ëª…", "ëŒ€ì§€_ë©´ì (ã¡)", "ê±´ì¶•_ë©´ì (ã¡)",
        "ê±´í_ìœ¨(%)", "ì—°ë©´ì (ã¡)", "ìš©ì _ë¥ _ì‚°ì •_ì—°ë©´ì (ã¡)", "ìš©ì _ë¥ (%)", "êµ¬ì¡°_ì½”ë“œ",
        "êµ¬ì¡°_ì½”ë“œ_ëª…", "ê¸°íƒ€_êµ¬ì¡°", "ì£¼_ìš©ë„_ì½”ë“œ", "ì£¼_ìš©ë„_ì½”ë“œ_ëª…", "ê¸°íƒ€_ìš©ë„",
        "ì§€ë¶•_ì½”ë“œ", "ì§€ë¶•_ì½”ë“œ_ëª…", "ê¸°íƒ€_ì§€ë¶•", "ì„¸ëŒ€_ìˆ˜(ì„¸ëŒ€)", "ê°€êµ¬_ìˆ˜(ê°€êµ¬)",
        "ë†’ì´(m)", "ì§€ìƒ_ì¸µ_ìˆ˜", "ì§€í•˜_ì¸µ_ìˆ˜", "ìŠ¹ìš©_ìŠ¹ê°•ê¸°_ìˆ˜", "ë¹„ìƒìš©_ìŠ¹ê°•ê¸°_ìˆ˜",
        "ë¶€ì†_ê±´ì¶•ë¬¼_ìˆ˜", "ë¶€ì†_ê±´ì¶•ë¬¼_ë©´ì (ã¡)", "ì´_ë™_ì—°ë©´ì (ã¡)", "ì˜¥ë‚´_ê¸°ê³„ì‹_ëŒ€ìˆ˜(ëŒ€)",
        "ì˜¥ë‚´_ê¸°ê³„ì‹_ë©´ì (ã¡)", "ì˜¥ì™¸_ê¸°ê³„ì‹_ëŒ€ìˆ˜(ëŒ€)", "ì˜¥ì™¸_ê¸°ê³„ì‹_ë©´ì (ã¡)", "ì˜¥ë‚´_ìì£¼ì‹_ëŒ€ìˆ˜(ëŒ€)",
        "ì˜¥ë‚´_ìì£¼ì‹_ë©´ì (ã¡)", "ì˜¥ì™¸_ìì£¼ì‹_ëŒ€ìˆ˜(ëŒ€)", "ì˜¥ì™¸_ìì£¼ì‹_ë©´ì (ã¡)", "í—ˆê°€_ì¼",
        "ì°©ê³µ_ì¼", "ì‚¬ìš©ìŠ¹ì¸_ì¼", "í—ˆê°€ë²ˆí˜¸_ë…„", "í—ˆê°€ë²ˆí˜¸_ê¸°ê´€_ì½”ë“œ", "í—ˆê°€ë²ˆí˜¸_ê¸°ê´€_ì½”ë“œ_ëª…",
        "í—ˆê°€ë²ˆí˜¸_êµ¬ë¶„_ì½”ë“œ", "í—ˆê°€ë²ˆí˜¸_êµ¬ë¶„_ì½”ë“œ_ëª…", "í˜¸_ìˆ˜(í˜¸)", "ì—ë„ˆì§€íš¨ìœ¨_ë“±ê¸‰",
        "ì—ë„ˆì§€ì ˆê°_ìœ¨", "ì—ë„ˆì§€_EPIì ìˆ˜", "ì¹œí™˜ê²½_ê±´ì¶•ë¬¼_ë“±ê¸‰", "ì¹œí™˜ê²½_ê±´ì¶•ë¬¼_ì¸ì¦ì ìˆ˜",
        "ì§€ëŠ¥í˜•_ê±´ì¶•ë¬¼_ë“±ê¸‰", "ì§€ëŠ¥í˜•_ê±´ì¶•ë¬¼_ì¸ì¦ì ìˆ˜", "ìƒì„±_ì¼ì", "ë‚´ì§„_ì„¤ê³„_ì ìš©_ì—¬ë¶€", "ë‚´ì§„_ëŠ¥ë ¥"
    ]

    for enc in ["cp949", "euc-kr", "utf-8"]:
        try:
            reader = pd.read_csv(txt_path, sep="|", encoding=enc, chunksize=500_000, names=columns, dtype=str, index_col=False)
            for i, df in enumerate(reader):
                df['sido'] = df['ëŒ€ì§€_ìœ„ì¹˜'].fillna('').str.split().str[0].str.strip().map(SIDO_MAP)
                valid_df = df.dropna(subset=['sido']).copy()
                for sido, group in valid_df.groupby('sido'):
                    out_dir = partition_root / f"region={sido}"
                    out_dir.mkdir(parents=True, exist_ok=True)
                    group.to_parquet(out_dir / f"part-{i:05d}.parquet", index=False)
            return
        except Exception: continue

# =========================
# Main Logic
# =========================
def run(cfg: HubConfig):
    base_path = Path(cfg.work_dir)
    base_path.mkdir(parents=True, exist_ok=True)

    # ì´ˆê¸° ë¶€íŠ¸ìŠ¤íŠ¸ë© ë¡œê±° (ì„ì‹œ)
    temp_log_dir = base_path / "bootstrap_logs"
    temp_log_dir.mkdir(parents=True, exist_ok=True)
    logger = build_logger(temp_log_dir / "bootstrap.log")

    notifier = SlackNotifier(cfg.slack_webhook_url, "EXTRACT-ê±´ì¶•ë¬¼ëŒ€ì¥", logger)

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=cfg.headless)
        page = browser.new_page(accept_downloads=True)

        try:
            logger.info("ğŸŒ í˜ì´ì§€ ì ‘ì† ì¤‘...")
            page.goto(cfg.url, wait_until="domcontentloaded", timeout=cfg.timeout_ms)

            # --- ê²€ìƒ‰ ë¡œì§ ---
            page.locator("select").filter(has=page.locator(f"option:has-text('{cfg.category_label}')")).select_option(label=cfg.category_label)
            page.locator("input[type='text']").first.fill(cfg.service_keyword)
            page.get_by_role("button", name="ê²€ìƒ‰").click()
            page.wait_for_selector(f"text={cfg.service_keyword}")

            # ë°ì´í„° ì‹œì  íŒŒì‹±ì„ ìœ„í•œ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            container = page.locator(f"text={cfg.service_keyword}").first
            for _ in range(8):
                if "ë°ì´í„°ì œê³µë…„ì›”" in container.inner_text(): break
                container = container.locator("xpath=..")

            data_year, data_month = parse_actual_data_year_month(container)
            datestr = f"{data_year}{data_month:02d}"

            # 1. ê²½ë¡œ ìƒì„± ë° ì¤‘ë³µ ì²´í¬
            paths = get_target_paths(cfg, data_year, data_month)

            # ì´ë¯¸ parquet ê²°ê³¼ë¬¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if any(paths["parquet_root"].glob("region=*")):
                logger.info(f"â­  {datestr} ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                notifier.success("ì‘ì—… ì™„ë£Œ", f"{data_year}ë…„ {data_month:02d}ì›” ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤")
            else:
                # 2. ë¡œê±° êµì²´ ë° ìˆ˜ì§‘ ì‹œì‘
                logger = build_logger(paths["logs"] / "run.log")
                logger.info(f"ğŸš€ ì‹ ê·œ ë°ì´í„° ë°œê²¬! ìˆ˜ì§‘ ì‹œì‘: {data_year}ë…„ {data_month}ì›”")
                notifier.info("ì‘ì—… ì‹œì‘", f"{data_year}ë…„ {data_month:02d}ì›” ë°ì´í„° ì¶”ì¶œ ì‹œì‘")

                # 3. ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì°¾ê¸° ë° í´ë¦­
                download_btn = None
                tmp = container
                for _ in range(6):
                    btn = tmp.get_by_role("button", name="ì „ì²´")
                    if btn.count() > 0:
                        download_btn = btn.first
                        break
                    tmp = tmp.locator("xpath=..")

                if not download_btn: raise RuntimeError("ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                with page.expect_download() as dl_info:
                    download_btn.click()
                    page.get_by_text(cfg.usage_reason_label, exact=True).click()
                    page.get_by_role("button", name="í™•ì¸").click()

                zip_path = paths["zips"] / dl_info.value.suggested_filename
                dl_info.value.save_as(zip_path)
                logger.info(f"âœ… ZIP ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {zip_path.name}")

                # 4. ETL (Unzip & Parquet)
                with zipfile.ZipFile(zip_path, "r") as zf:
                    zf.extractall(paths["unzipped"])

                for txt_file in paths["unzipped"].glob("*.txt"):
                    if txt_file.stat().st_size > 0:
                        logger.info(f"ğŸ“¦ Parquet ë³€í™˜ ì¤‘: {txt_file.name}")
                        # Hive íŒŒí‹°ì…˜ ê²½ë¡œ(paths["parquet_root"])ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬
                        txt_to_parquet_by_region(txt_file, paths["parquet_root"])


                logger.info(f"{data_year}ë…„ {data_month:02d}ì›” ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
                notifier.success("ì‘ì—… ì™„ë£Œ", f"{data_year}ë…„ {data_month:02d}ì›” ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")

            # 5. ì •ë¦¬ (ë°ì´í„° ì‹œì  ê¸°ì¤€ìœ¼ë¡œ M, M-1 ì™¸ ì •ë¦¬)
            # remove_old_data_runs(cfg, data_year, data_month, logger)

        except Exception as e:
            logger.error(f"ğŸš¨ ì—ëŸ¬ ë°œìƒ: {str(e)}\n{traceback.format_exc()}")
            if 'notifier' in locals(): notifier.error("ìˆ˜ì§‘ ì¤‘ë‹¨", str(e))
            raise
        finally:
            browser.close()
            # ì„ì‹œ ë¶€íŠ¸ìŠ¤íŠ¸ë© ë¡œê·¸ ì‚­ì œ
            if (temp_log_dir / "bootstrap.log").exists():
                try: shutil.rmtree(temp_log_dir)
                except: pass

if __name__ == "__main__":
    run(HubConfig())
    
    