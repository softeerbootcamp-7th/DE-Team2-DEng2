import os
import sys
import time
import logging
import zipfile
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, List, Iterable

import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from dotenv import load_dotenv

# slack_utils.py ê²½ë¡œ ì¶”ê°€ (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)
sys.path.append(str(Path(__file__).resolve().parent.parent))
from slack_utils import SlackNotifier

# =========================
# ENV & Config
# =========================
load_dotenv()

@dataclass
class Config:
    # ìˆ˜ì§‘ ëŒ€ìƒ URL
    url: str = "https://stat.molit.go.kr/portal/cate/statView.do?hRsId=58&hFormId=5498&hSelectId=5498&sStyleNum=2&sStart=202601&sEnd=202601&hPoint=00&hAppr=1&oFileName=&rFileName=&midpath="
    project_root: str = "data/chajoo_dist"
    retries: int = 3
    timeout_sec: int = 120
    headless: bool = True  # íŒŒì¼ ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•´ ê°€ê¸‰ì  False ê¶Œì¥
    parquet_compression: str = "snappy"
    slack_webhook_url: Optional[str] = os.getenv("SLACK_WEBHOOK_URL")
    parquet_overwrite: bool = False
    force_run: bool = False
    sigungu_mapping_csv: str = "data/chajoo_dist/_work/csv/êµ­í† êµí†µë¶€_ë²•ì •ë™ì½”ë“œ_20250805.csv"

# =========================
# Logger & Helpers
# =========================
def build_logger(log_file: Path) -> logging.Logger:
    logger = logging.getLogger("chajoo_extract")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    fmt = logging.Formatter("%(asctime)s | %(levelname)s | %(message)s", "%Y-%m-%d %H:%M:%S")
    
    sh = logging.StreamHandler(sys.stdout)
    sh.setFormatter(fmt)
    fh = logging.FileHandler(log_file, encoding="utf-8")
    fh.setFormatter(fmt)
    
    logger.addHandler(sh)
    logger.addHandler(fh)
    return logger

def init_run_dirs(cfg: Config) -> dict:
    base = Path(cfg.project_root)
    paths = {
        "base": base,
        "work": base / "_work",
        "xlsx": base / "_work" / "xlsx",
        "parquet": base / "parquet",
        "log_file": base / "_work" / "run.log",
    }
    paths["xlsx"].mkdir(parents=True, exist_ok=True)
    paths["parquet"].mkdir(parents=True, exist_ok=True)
    paths["work"].mkdir(parents=True, exist_ok=True)
    return paths

# =========================
# Data Processing Helpers (New)
# =========================
def flatten_col(col: Iterable) -> str:
    """MultiIndex ì»¬ëŸ¼ì„ ë‹¨ì¼ ë¬¸ìì—´ë¡œ í‰íƒ„í™” ë° ì—°ì† ì¤‘ë³µ ì œê±°"""
    parts: List[str] = []
    for x in col:
        s = str(x).strip()
        if s.lower() in ("nan", "none", ""):
            continue
        if not parts or parts[-1] != s:
            parts.append(s)
    return "_".join(parts)

def pick_col(cands: List[str], columns: List[str]) -> str:
    """ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ì—ì„œ í›„ë³´ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ì²« ë²ˆì§¸ ì»¬ëŸ¼ ë°˜í™˜"""
    for key in cands:
        hit = [c for c in columns if key in c]
        if hit:
            return hit[0]
    raise KeyError(f"í•„ìˆ˜ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í›„ë³´={cands}")

# =========================
# Selenium & Download Logic
# =========================
def build_driver(download_dir: Path, cfg: Config) -> webdriver.Chrome:
    opts = Options()
    if cfg.headless:
        opts.add_argument("--headless=new")

    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--window-size=1400,1000")
    opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    opts.add_experimental_option("useAutomationExtension", False)

    prefs = {
        "download.default_directory": str(download_dir.resolve()),
        "download.prompt_for_download": False,
        "download.directory_upgrade": True,
        "safebrowsing.enabled": True,
    }
    opts.add_experimental_option("prefs", prefs)

    driver = webdriver.Chrome(options=opts)
    driver.set_page_load_timeout(60)
    return driver

def is_xlsx_zip(path: Path) -> bool:
    try:
        with path.open("rb") as f:
            sig = f.read(2)
        return sig == b"PK" and zipfile.is_zipfile(path)
    except Exception:
        return False

def wait_for_download(download_dir: Path, timeout: int) -> Path:
    end = time.time() + timeout
    while time.time() < end:
        # .xlsx íŒŒì¼ë§Œ í•„í„°ë§ (ì„ì‹œ íŒŒì¼ .crdownload ì œì™¸)
        files = list(download_dir.glob("*.xlsx"))
        files = [f for f in files if not f.name.endswith(".crdownload")]
        
        if files:
            latest = max(files, key=lambda p: p.stat().st_mtime)
            # [ì¤‘ìš”] íŒŒì¼ ì“°ê¸°ê°€ ì™„ë£Œë˜ì—ˆëŠ”ì§€ ì²´í¬ (í¬ê¸° ë³€í™” ê´€ì°°)
            prev_size = -1
            for _ in range(5): 
                curr_size = latest.stat().st_size
                if curr_size > 0 and curr_size == prev_size:
                    return latest
                prev_size = curr_size
                time.sleep(0.5)
        time.sleep(1.0)

def perform_download(driver: webdriver.Chrome, logger: logging.Logger, cfg: Config, download_dir: Path) -> Path:
    wait = WebDriverWait(driver, 30)
    
    for attempt in range(1, cfg.retries + 1):
        try:
            logger.info(f"ë‹¤ìš´ë¡œë“œ ì‹œë„ ({attempt}/{cfg.retries})")
            start_ts = time.time()
            main_handle = driver.current_window_handle

            # 1. ë©”ì¸ ë²„íŠ¼ í´ë¦­
            btn = wait.until(EC.element_to_be_clickable((By.XPATH, "//button[@title='íŒŒì¼ ë‹¤ìš´ë¡œë“œ' or normalize-space()='íŒŒì¼ ë‹¤ìš´ë¡œë“œ']")))
            driver.execute_script("arguments[0].scrollIntoView({block:'center'});", btn)
            time.sleep(0.5)
            btn.click()

            # 2. ìƒˆ ì°½(íŒì—…) í™•ì¸
            time.sleep(2)
            handles = driver.window_handles
            if len(handles) > 1:
                popup_handle = [h for h in handles if h != main_handle][0]
                driver.switch_to.window(popup_handle)
                
                # íŒì—… ë‚´ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í´ë¦­
                dl_btn = wait.until(EC.element_to_be_clickable((By.XPATH, "//button[contains(.,'ë‹¤ìš´ë¡œë“œ')] | //a[contains(.,'ë‹¤ìš´ë¡œë“œ')]")))
                dl_btn.click()
                time.sleep(1)
                driver.close()
                driver.switch_to.window(main_handle)
            else:
                # ëª¨ë‹¬ í˜•íƒœì¼ ê²½ìš°
                modal_btn = wait.until(EC.element_to_be_clickable((By.XPATH, "//div[contains(@class,'modal')]//button[contains(.,'ë‹¤ìš´ë¡œë“œ')]")))
                modal_btn.click()

            # 3. íŒŒì¼ ëŒ€ê¸°
            path = wait_for_download(download_dir, cfg.timeout_sec)
            if path.stat().st_mtime >= start_ts:
                logger.info(f"ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {path.name}")
                return path
            
        except Exception as e:
            logger.warning(f"ì‹œë„ {attempt} ì‹¤íŒ¨: {e}")
            if attempt < cfg.retries:
                driver.refresh()
                time.sleep(3)
            else:
                raise

# =========================
# Parquet Conversion
# =========================
SIDO_CODE_MAP = {
    "11": "ì„œìš¸",
    "26": "ë¶€ì‚°",
    "27": "ëŒ€êµ¬",
    "28": "ì¸ì²œ",
    "29": "ê´‘ì£¼",
    "30": "ëŒ€ì „",
    "31": "ìš¸ì‚°",
    "36": "ì„¸ì¢…",
    "41": "ê²½ê¸°",
    "42": "ê°•ì›",
    "43": "ì¶©ë¶",
    "44": "ì¶©ë‚¨",
    "45": "ì „ë¶",
    "46": "ì „ë‚¨",
    "47": "ê²½ë¶",
    "48": "ê²½ë‚¨",
    "50": "ì œì£¼",
}

def convert_xlsx_to_parquet(
    xlsx_path: Path,
    out_dir_root: Path,
    cfg: Config,
    logger: logging.Logger
) -> str:

    logger.info(f"ğŸ“¦ ì „ì²˜ë¦¬ ì‹œì‘: {xlsx_path.name}")

    # --------------------------------------------------
    # 1. ì—‘ì…€ ì½ê¸°
    # --------------------------------------------------
    df = pd.read_excel(xlsx_path, header=[4, 5], engine="openpyxl")

    if isinstance(df.columns, pd.MultiIndex):
        df.columns = [flatten_col(col) for col in df.columns]
    else:
        df.columns = [str(c).strip() for c in df.columns]

    col_month = pick_col(["ì›”(Monthly)", "ì›”"], list(df.columns))
    col_sido = pick_col(["ì‹œë„ëª…"], list(df.columns))
    col_sigungu = pick_col(["ì‹œêµ°êµ¬"], list(df.columns))

    cargo_sales_cols = [c for c in df.columns if ("í™”ë¬¼" in c and "ì˜ì—…ìš©" in c)]
    if not cargo_sales_cols:
        raise KeyError("'í™”ë¬¼ ì˜ì—…ìš©' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    col_cargo_sales = sorted(cargo_sales_cols, key=len)[0]

    # --------------------------------------------------
    # 2. ë°ì´í„° ì •ì œ
    # --------------------------------------------------
    out = df[[col_month, col_sido, col_sigungu, col_cargo_sales]].copy()

    out[col_cargo_sales] = pd.to_numeric(
        out[col_cargo_sales].astype(str).str.replace(",", "").str.strip(),
        errors="coerce"
    )

    out = out[
        out[col_sigungu].notna() &
        (out[col_sigungu].astype(str).str.strip() != "ê³„")
    ]

    result = (
        out.groupby([col_month, col_sido, col_sigungu], dropna=False, as_index=False)[col_cargo_sales]
        .sum()
        .rename(columns={
            col_month: "year_month",
            col_sido: "sido",
            col_sigungu: "sigungu",
            col_cargo_sales: "cargo_sales_count",
        })
    )

    # --------------------------------------------------
    # ğŸ”¥ 3. ì‹œêµ°êµ¬ ì½”ë“œ ë§¤í•‘ (êµ¬ í¬í•¨ ì•ˆì • ë²„ì „)
    # --------------------------------------------------
    logger.info("ğŸ”— ì‹œêµ°êµ¬ ì½”ë“œ ë§¤í•‘ ì‹œì‘")

    mapping_df = pd.read_csv(
        cfg.sigungu_mapping_csv,
        encoding="euc-kr"
    )

    mapping_df["ë²•ì •ë™ì½”ë“œ"] = mapping_df["ë²•ì •ë™ì½”ë“œ"].astype(str)

    # 5ìë¦¬ ì‹œêµ°êµ¬ ì½”ë“œ
    mapping_df["sigungu_code"] = mapping_df["ë²•ì •ë™ì½”ë“œ"].str[:5]

    # ì‹œë„ ì½”ë“œ ì¶”ì¶œ
    mapping_df["sido_code"] = mapping_df["ë²•ì •ë™ì½”ë“œ"].str[:2]
    mapping_df["sido"] = mapping_df["sido_code"].map(SIDO_CODE_MAP)

    # ğŸ”¥ ë²•ì •ë™ëª… ë¶„ë¦¬
    name_split = mapping_df["ë²•ì •ë™ëª…"].str.split()

    # ì‹œë„ëª…
    mapping_df["sido"] = mapping_df["sido"]

    # ğŸ”¥ ì‹œêµ°êµ¬ëª… ìƒì„± (êµ¬ í¬í•¨)
    def build_sigungu(parts):
        if len(parts) >= 3:
            return parts[1] + parts[2]  # ê³ ì–‘ì‹œ + ë•ì–‘êµ¬
        elif len(parts) >= 2:
            return parts[1]
        return None

    mapping_df["sigungu"] = name_split.apply(build_sigungu)

    # ğŸ”¥ ê³µë°± ì œê±° (ì •ê·œí™”)
    def normalize(x):
        if pd.isna(x):
            return x
        return str(x).replace(" ", "").strip()

    mapping_df["sigungu"] = mapping_df["sigungu"].apply(normalize)
    mapping_df["sido"] = mapping_df["sido"].apply(normalize)

    mapping_df = (
        mapping_df[["sigungu_code", "sido", "sigungu"]]
        .drop_duplicates()
        .dropna()
    )

    mapping_df = pd.concat([
        mapping_df,
        pd.DataFrame({
            "sigungu_code": ["36110"],
            "sido": ["ì„¸ì¢…"],
            "sigungu": ["ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ"]
        })
    ])


    # --------------------------------------------------
    # ğŸ”¥ resultë„ ë™ì¼ ì •ê·œí™”
    # --------------------------------------------------
    result["sido"] = result["sido"].apply(normalize)
    result["sigungu"] = result["sigungu"].apply(normalize)

    # merge
    result = result.merge(
        mapping_df,
        on=["sido", "sigungu"],
        how="left"
    )

    if result["sigungu_code"].isna().any():
        logger.warning("âš  ì¼ë¶€ ì‹œêµ°êµ¬ ì½”ë“œ ë§¤í•‘ ì‹¤íŒ¨ ì¡´ì¬")

    # --------------------------------------------------
    # 4. ì›”ë³„ ê³„ì¸µ ì €ì¥
    # --------------------------------------------------
    result["year_month_dt"] = pd.to_datetime(
        result["year_month"].astype(str).str.replace("/", "-")
    )

    unique_months = result["year_month_dt"].unique()
    saved_count = 0

    for target_dt in unique_months:

        ts = pd.Timestamp(target_dt)
        year_str = ts.strftime("%Y")
        month_str = ts.strftime("%m")

        partition_dir = out_dir_root / f"year={year_str}" / f"month={month_str}"
        partition_dir.mkdir(parents=True, exist_ok=True)

        target_path = partition_dir / "part.parquet"

        if target_path.exists() and not cfg.parquet_overwrite and not cfg.force_run:
            continue

        monthly_df = result[result["year_month_dt"] == target_dt].copy()
        monthly_df = monthly_df.drop(columns=["year_month", "year_month_dt"])

        monthly_df.to_parquet(
            target_path,
            index=False,
            compression=cfg.parquet_compression
        )

        saved_count += 1

    logger.info(f"âœ… ê³„ì¸µí˜• ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ ì›” ì €ì¥")

    return f"Processed {len(unique_months)} months, Updated {saved_count} files"

def convert_xlsx_to_parquet(
    xlsx_path: Path,
    out_dir_root: Path,
    cfg: Config,
    logger: logging.Logger
) -> str:

    logger.info(f"ğŸ“¦ ì „ì²˜ë¦¬ ì‹œì‘: {xlsx_path.name}")

    # --------------------------------------------------
    # 1. ì—‘ì…€ ì½ê¸°
    # --------------------------------------------------
    df = pd.read_excel(xlsx_path, header=[4, 5], engine="openpyxl")

    if isinstance(df.columns, pd.MultiIndex):
        df.columns = [flatten_col(col) for col in df.columns]
    else:
        df.columns = [str(c).strip() for c in df.columns]

    col_month = pick_col(["ì›”(Monthly)", "ì›”"], list(df.columns))
    col_sido = pick_col(["ì‹œë„ëª…"], list(df.columns))
    col_sigungu = pick_col(["ì‹œêµ°êµ¬"], list(df.columns))

    cargo_sales_cols = [c for c in df.columns if ("í™”ë¬¼" in c and "ì˜ì—…ìš©" in c)]
    if not cargo_sales_cols:
        raise KeyError("'í™”ë¬¼ ì˜ì—…ìš©' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    col_cargo_sales = sorted(cargo_sales_cols, key=len)[0]

    # --------------------------------------------------
    # 2. ë°ì´í„° ì •ì œ
    # --------------------------------------------------
    out = df[[col_month, col_sido, col_sigungu, col_cargo_sales]].copy()

    out[col_cargo_sales] = pd.to_numeric(
        out[col_cargo_sales].astype(str).str.replace(",", "").str.strip(),
        errors="coerce"
    )

    out = out[
        out[col_sigungu].notna() &
        (out[col_sigungu].astype(str).str.strip() != "ê³„")
    ]

    result = (
        out.groupby([col_month, col_sido, col_sigungu], as_index=False)[col_cargo_sales]
        .sum()
        .rename(columns={
            col_month: "year_month",
            col_sido: "sido",
            col_sigungu: "sigungu",
            col_cargo_sales: "cargo_sales_count",
        })
    )

    # ë¬¸ìì—´ ì •ë¦¬
    result["sido"] = result["sido"].astype(str).str.strip()
    result["sigungu"] = result["sigungu"].astype(str).str.strip()

    # --------------------------------------------------
    # ğŸ”¥ 3. SHP_CD ë§¤í•‘
    # --------------------------------------------------
    logger.info("ğŸ”— SHP_CD ë§¤í•‘ ì‹œì‘")

    shp_mapping_path = Path("data/chajoo_dist/_work/csv/SHP_CD_mapping.csv")

    shp_df = pd.read_csv(
        shp_mapping_path,
        dtype={"SHP_CD": str}
    )

    shp_df["sido"] = shp_df["sido"].astype(str).str.strip()
    shp_df["sigungu"] = shp_df["sigungu"].astype(str).str.strip()
    shp_df["SHP_CD"] = shp_df["SHP_CD"].astype(str)

    result = result.merge(
        shp_df[["sido", "sigungu", "SHP_CD"]],
        on=["sido", "sigungu"],
        how="left"
    )

    if result["SHP_CD"].isna().any():
        logger.warning("âš  SHP_CD ë§¤í•‘ ì‹¤íŒ¨ ì¡´ì¬")

    # --------------------------------------------------
    # 4. year / month ë¶„ë¦¬
    # --------------------------------------------------
    result["year_month_dt"] = pd.to_datetime(
        result["year_month"].astype(str).str.replace("/", "-")
    )

    unique_months = result["year_month_dt"].unique()
    saved_count = 0

    for target_dt in unique_months:

        ts = pd.Timestamp(target_dt)
        year_str = ts.strftime("%Y")
        month_str = ts.strftime("%m")

        partition_dir = out_dir_root / f"year={year_str}" / f"month={month_str}"
        partition_dir.mkdir(parents=True, exist_ok=True)

        parquet_path = partition_dir / "part.parquet"
        csv_path = partition_dir / "part.csv"

        monthly_df = result[result["year_month_dt"] == target_dt].copy()
        monthly_df = monthly_df.drop(columns=["year_month_dt"])

        # Parquet
        if not parquet_path.exists() or cfg.force_run:
            monthly_df.to_parquet(
                parquet_path,
                index=False,
                compression=cfg.parquet_compression
            )
            logger.info(f"ğŸ’¾ Parquet ì €ì¥ ì™„ë£Œ: {parquet_path}")

        saved_count += 1

    logger.info(f"âœ… ê³„ì¸µí˜• ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ ì›” ì²˜ë¦¬")

    return f"Processed {len(unique_months)} months"


# =========================
# Main Logic
# =========================
def main():
    cfg = Config()
    paths = init_run_dirs(cfg)
    logger = build_logger(paths["log_file"])
    notifier = SlackNotifier(cfg.slack_webhook_url, "EXTRACT-ì°¨ì£¼ë¶„í¬", logger)

    logger.info("===== EXTRACT CHAJOO START =====")
    driver = None

    try:
        notifier.info("ì‘ì—… ì‹œì‘", "êµ­í† ë¶€ ì°¨ì£¼ë¶„í¬ ë°ì´í„° ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")

        # [STEP 1] XLSX í™•ë³´
        existing_xlsx = list(paths["xlsx"].glob("*.xlsx"))
        if existing_xlsx and not cfg.force_run:
            logger.warning("â­ ë¡œì»¬ ì—‘ì…€ íŒŒì¼ ì‚¬ìš© (Skip Download)")
            xlsx_path = max(existing_xlsx, key=lambda p: p.stat().st_mtime)
        else:
            driver = build_driver(paths["xlsx"], cfg)
            driver.get(cfg.url)
            xlsx_path = perform_download(driver, logger, cfg, paths["xlsx"])

        # [STEP 2] Parquet ë³€í™˜ (ë³€ê²½ëœ í•¨ìˆ˜ í˜¸ì¶œ)
        status_msg = convert_xlsx_to_parquet(xlsx_path, paths["parquet"], cfg, logger)

        # ì™„ë£Œ ì•Œë¦¼
        notifier.success("ì‘ì—… ì™„ë£Œ", f"ê²°ê³¼: {status_msg}")
        logger.info(f"===== SUCCESS ({status_msg}) =====")

    except Exception as e:
        logger.error(f"ğŸš¨ ì—ëŸ¬: {e}", exc_info=True)
        notifier.error("ì°¨ì£¼ë¶„í¬ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨", e)
        sys.exit(1)
    finally:
        if driver: driver.quit()

if __name__ == "__main__":
    main()