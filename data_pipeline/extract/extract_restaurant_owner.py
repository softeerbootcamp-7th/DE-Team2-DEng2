import os
import sys
import csv
import logging
import random
import re
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional

import requests
from playwright.sync_api import sync_playwright
from dotenv import load_dotenv  # 1. import ì¶”ê°€

import argparse
from datetime import datetime
import pandas as pd

# 2. .env ë¡œë“œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ëª…ì‹œ)
# í˜„ì¬ íŒŒì¼: data_pipeline/extract/extract_restaurant_owner.py
# .env ìœ„ì¹˜: / (ë£¨íŠ¸)
env_path = Path(__file__).resolve().parent.parent.parent / '.env'
load_dotenv(dotenv_path=env_path)

# slack_utils.pyë¥¼ ì°¾ê¸° ìœ„í•´ ìƒìœ„ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).resolve().parent.parent))
from data_pipeline.utils.slack_utils import SlackNotifier

# =========================
# Config (ì„¤ì • í†µí•©)
# =========================
@dataclass
class Config:
    url: str = "https://www.foodsafetykorea.go.kr/portal/specialinfo/searchInfoCompany.do"
    headless: bool = True
    retries: int = 1
    retry_sleep_sec: int = 5
    timeout_ms: int = 30_000
    project_root: str = "data/restaurant_owner"
    slack_webhook_url: Optional[str] = os.getenv("SLACK_WEBHOOK_URL")

    sido_mapping: dict = field(default_factory=lambda: {
        "ì„œìš¸íŠ¹ë³„ì‹œ": "ì„œìš¸íŠ¹ë³„ì‹œ", "ë¶€ì‚°ê´‘ì—­ì‹œ": "ë¶€ì‚°ê´‘ì—­ì‹œ", "ëŒ€êµ¬ê´‘ì—­ì‹œ": "ëŒ€êµ¬ê´‘ì—­ì‹œ",
        "ì¸ì²œê´‘ì—­ì‹œ": "ì¸ì²œê´‘ì—­ì‹œ", "ê´‘ì£¼ê´‘ì—­ì‹œ": "ê´‘ì£¼ê´‘ì—­ì‹œ", "ëŒ€ì „ê´‘ì—­ì‹œ": "ëŒ€ì „ê´‘ì—­ì‹œ",
        "ìš¸ì‚°ê´‘ì—­ì‹œ": "ìš¸ì‚°ê´‘ì—­ì‹œ", "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ": "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ", "ê²½ê¸°ë„": "ê²½ê¸°ë„",
        "ê°•ì›ë„": "ê°•ì›íŠ¹ë³„ìì¹˜ë„", "ì¶©ì²­ë¶ë„": "ì¶©ì²­ë¶ë„", "ì¶©ì²­ë‚¨ë„": "ì¶©ì²­ë‚¨ë„",
        "ì „ë¼ë¶ë„": "ì „ë¶íŠ¹ë³„ìì¹˜ë„", "ì „ë¼ë‚¨ë„": "ì „ë¼ë‚¨ë„", "ê²½ìƒë¶ë„": "ê²½ìƒë¶ë„",
        "ê²½ìƒë‚¨ë„": "ê²½ìƒë‚¨ë„", "ì œì£¼íŠ¹ë³„ìì¹˜ë„": "ì œì£¼íŠ¹ë³„ìì¹˜ë„",
    })

# =========================
# Logger
# =========================
def build_logger(log_file: Path) -> logging.Logger:
    logger = logging.getLogger("food_safety_search")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    fmt = logging.Formatter("%(asctime)s | %(levelname)s | %(message)s", "%Y-%m-%d %H:%M:%S")

    sh = logging.StreamHandler(sys.stdout)
    sh.setFormatter(fmt)
    fh = logging.FileHandler(log_file, encoding="utf-8")
    fh.setFormatter(fmt)

    logger.addHandler(sh)
    logger.addHandler(fh)
    return logger

# =========================
# Anti-Ban Helpers
# =========================
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_4_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3.1 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:125.0) Gecko/20100101 Firefox/125.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0",
]

VIEWPORTS = [
    {"width": 1920, "height": 1080},
    {"width": 1366, "height": 768},
    {"width": 1536, "height": 864},
    {"width": 1440, "height": 900},
    {"width": 1280, "height": 720},
]


def _random_delay(min_sec: float = 1.0, max_sec: float = 3.0) -> None:
    """ëœë¤ ë”œë ˆì´"""
    time.sleep(random.uniform(min_sec, max_sec))


def _human_like_delay() -> None:
    """ì‚¬ëŒì²˜ëŸ¼ ë³´ì´ëŠ” ì§§ì€ ë”œë ˆì´ (í´ë¦­/ì…ë ¥ ì‚¬ì´)"""
    time.sleep(random.uniform(0.3, 1.2))


def _simulate_mouse_movement(page) -> None:
    """ëœë¤ ë§ˆìš°ìŠ¤ ì´ë™ìœ¼ë¡œ ë´‡ íƒì§€ ìš°íšŒ"""
    for _ in range(random.randint(2, 5)):
        x = random.randint(100, 800)
        y = random.randint(100, 600)
        page.mouse.move(x, y)
        time.sleep(random.uniform(0.05, 0.15))


def _create_context(browser, logger: logging.Logger):
    ua = random.choice(USER_AGENTS)
    vp = random.choice(VIEWPORTS)

    context = browser.new_context(
        user_agent=ua,
        viewport=vp,
        locale=random.choice(["ko-KR", "ko"]),
        timezone_id="Asia/Seoul",
        extra_http_headers={
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        },
    )
    page = context.new_page()

    logger.debug(f"í¬ë¡¤ë§ ì»¨í…ìŠ¤íŠ¸ ìƒì„± | UA: {ua[:50]}... | VP: {vp}")
    return page

# =========================
# Main Crawler Logic
# =========================
def search_and_save_all_pages(
    sido_name: str,
    address: str,
    cfg: Config,
    logger: logging.Logger,
    output_file: Path,
) -> None:
    notifier = SlackNotifier(cfg.slack_webhook_url, "EXTRACT-ì‹ë‹¹ëŒ€í‘œì", logger)

    if sido_name not in cfg.sido_mapping:
        logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‹œë„ëª…: {sido_name}")
        return

    checkbox_label = cfg.sido_mapping[sido_name]
    HEADERS = ["ë²ˆí˜¸", "ì¸í—ˆê°€ë²ˆí˜¸", "ì—…ì²´ëª…", "ì—…ì¢…", "ëŒ€í‘œì", "ì†Œì¬ì§€", "ì¸í—ˆê°€ê¸°ê´€", "ì˜ì—…ìƒíƒœ", "ë¹„ê³ "]

    try:
        notifier.info("ì‘ì—… ì‹œì‘", f"ëŒ€ìƒ: {sido_name} {address}")

        with sync_playwright() as p:
            browser = p.chromium.launch(headless=cfg.headless)

            # ì»¨í…ìŠ¤íŠ¸ ë° í˜ì´ì§€ ìƒì„±
            page = _create_context(browser, logger)
            logger.info(f"ğŸŒ í˜ì´ì§€ ì ‘ì† ì¤‘: {cfg.url}")
            page.goto(cfg.url, timeout=cfg.timeout_ms)
            page.wait_for_load_state("networkidle")
            _random_delay(3.0, 5.0)

            # ì¹´í…Œê³ ë¦¬ ë° ì¡°ê±´ ì„ íƒ
            page.locator('.dsL li[val="rstrt"] a').click()
            page.locator(f'input[type="checkbox"][title="{checkbox_label}"]').first.check()

            # ì£¼ì†Œ ì…ë ¥ (Human-like íƒ€ì´í•‘)
            addr_input = page.locator('input[name="site_addr"]').first
            addr_input.click()
            for char in address:
                addr_input.type(char, delay=random.randint(50, 150))

            # ê²€ìƒ‰ ë° ê²°ê³¼ ì„¤ì • (50ê°œì”© ë³´ê¸°)
            page.locator("#srchBtn").click()
            page.wait_for_load_state("networkidle")
            _random_delay(4.0, 6.0)

            # ê²°ê³¼ ìœ ë¬´ ì²´í¬
            first_row = page.locator("#tbl_bsn_list tbody tr").first
            if first_row.count() == 0 or "ì¡°íšŒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤" in first_row.text_content():
                logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {address}")
                notifier.info("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ", f"{sido_name} {address} ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            # ë¦¬ìŠ¤íŠ¸ ìˆ˜ ë³€ê²½
            page.locator("#a_list_cnt").click()
            page.locator('a[val="50"]').click()
            page.wait_for_load_state("networkidle")
            _random_delay(4.0, 6.0)

            # í˜ì´ì§€ ì •ë³´ íŒŒì‹±
            total_pages = page.evaluate("$('.pagination').pagination('getPagesCount')")
            logger.info(f"ì´ {total_pages} í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘")

            # ë°ì´í„° ì¶”ì¶œ ë° íŒŒì¼ ì €ì¥
            with open(output_file, "w", encoding="utf-8-sig", newline="") as f:
                writer = csv.writer(f)
                writer.writerow(HEADERS)

                for page_num in range(1, total_pages + 1):
                    if page_num > 1:
                        page.locator('.pagination li:last-child a').click()
                        page.wait_for_load_state("networkidle")
                        time.sleep(random.uniform(4.0, 6.0))

                    rows = page.locator("#tbl_bsn_list tbody tr")
                    row_count = rows.count()

                    for i in range(row_count):
                        cells = rows.nth(i).locator("td")
                        row_data = []

                        for c in range(min(cells.count(), len(HEADERS))):
                            # 1. ì¼ë‹¨ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ (ê²°ê³¼: "ë²ˆí˜¸5527")
                            raw_text = cells.nth(c).text_content().strip()

                            # 2. ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì‹œì‘ ë¶€ë¶„ì— ìˆëŠ” Header ì´ë¦„ì„ ì§€ì›ë‹ˆë‹¤.
                            # ì˜ˆ: "ë²ˆí˜¸5527" -> "5527"
                            # ^ ê¸°í˜¸ëŠ” ë¬¸ìì—´ì˜ ì‹œì‘ë¶€ë¶„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
                            header_name = HEADERS[c]
                            clean_val = re.sub(f"^{header_name}", "", raw_text).strip()

                            row_data.append(clean_val)

                        writer.writerow(row_data)

                    logger.info(f"ì§„í–‰ ì¤‘: {page_num}/{total_pages} í˜ì´ì§€ ì™„ë£Œ")

                    if page_num % 10 == 0:
                        _random_delay(5.0, 10.0)

            # [SUCCESS]
            notifier.success("ì‘ì—… ì™„ë£Œ", f"{sido_name} {address} ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ (ì´ {total_pages}p)")
            browser.close()

    except Exception as e:
        # [CRITICAL ERROR]
        logger.error(f"ğŸš¨ í¬ë¡¤ëŸ¬ ì¤‘ë‹¨ë¨: {str(e)}")
        notifier.error("ì‹í’ˆì•ˆì „ë‚˜ë¼ í¬ë¡¤ëŸ¬ ì¤‘ë‹¨", e)
        if 'browser' in locals(): browser.close()
        raise e

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--sido", type=str, default="ê²½ê¸°ë„")
    parser.add_argument("--addr", type=str, default="ìš©ì¸ì‹œ ì²˜ì¸êµ¬")
    args = parser.parse_args()

    # 1. ê²½ë¡œ ë° íŒŒí‹°ì…˜ ì„¤ì •
    now = datetime.now()
    year = now.strftime("%Y")
    month = now.strftime("%m")
    region = args.sido[:2] 

    base_path = Path(Config.project_root)
    work_dir = base_path / "_work"
    parquet_dir = base_path / f"parquet/year={year}/month={month}/region={region}"

    work_dir.mkdir(parents=True, exist_ok=True)
    parquet_dir.mkdir(parents=True, exist_ok=True)

    safe_addr = args.addr.replace(" ", "_")
    log_file = work_dir / f"run_{region}_{safe_addr}.log"
    csv_file = work_dir / f"{region}_{safe_addr}.csv"
    parquet_file = parquet_dir / f"{region}_{safe_addr}.parquet"

    logger = build_logger(log_file)

    # 2. í¬ë¡¤ë§ ë‹¨ê³„ (CSV ì¡´ì¬ ì—¬ë¶€ ì²´í¬)
    if csv_file.exists():
        logger.info(f"â­ï¸  ì´ë¯¸ CSV íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤. í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤: {csv_file.name}")
    else:
        logger.info(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: {args.sido} {args.addr}")
        search_and_save_all_pages(
            sido_name=args.sido,
            address=args.addr,
            cfg=Config(),
            logger=logger,
            output_file=csv_file
        )

    # 3. ë³€í™˜ ë‹¨ê³„ (Parquet ì¡´ì¬ ì—¬ë¶€ ì²´í¬)
    if parquet_file.exists():
        logger.info(f"â­ï¸  ì´ë¯¸ Parquet íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ë³€í™˜ì„ ê±´ë„ˆëœë‹ˆë‹¤: {parquet_file.name}")
    elif csv_file.exists():
        try:
            logger.info("ğŸ“„ CSVë¥¼ Parquetë¡œ ë³€í™˜ ì¤‘...")
            df = pd.read_csv(csv_file)
            # ë°ì´í„° ì¶”ì¶œ ì‹œ ë°œìƒí–ˆë˜ í—¤ë” ì¤‘ë³µ ì´ìŠˆ ë“±ì„ í•œ ë²ˆ ë” ë°©ì–´ì ìœ¼ë¡œ ì²˜ë¦¬
            df.to_parquet(parquet_file, engine='pyarrow', index=False, compression='snappy')
            logger.info(f"âœ… ë³€í™˜ ì™„ë£Œ: {parquet_file}")
        except Exception as e:
            logger.error(f"âŒ Parquet ë³€í™˜ ì‹¤íŒ¨: {e}")
    else:
        logger.error("âš ï¸ ë³€í™˜í•  CSV íŒŒì¼ì´ ì—†ì–´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()