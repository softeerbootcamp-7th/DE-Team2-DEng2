#!/bin/bash
# ==========================================================
# ì£¼ë°°ì¹˜: ì‹ë‹¹ Clean â†’ S1 (í¬ë¡¤ë§ ë¦¬ìŠ¤íŠ¸ ìƒì„±)
# ì‹ë‹¹ ëŒ€í‘œì ë°ì´í„° (ì£¼ 1íšŒ ê°±ì‹ )
# ì›”ë°°ì¹˜ ìŠ¤í¬ë¦½íŠ¸ (run_transform_monthly.sh) ì‹¤í–‰í•˜ì—¬
# s0/address, s0/toji_building ì—…ë°ì´íŠ¸ í›„ ì‹¤í–‰
#
# USE CASE:
#   cd <í”„ë¡œì íŠ¸ ë£¨íŠ¸>
#   bash scripts/run_transform_weekly.sh
#   bash scripts/run_transform_weekly.sh --with-s3
# ==========================================================
set -euo pipefail

# ê²½ë¡œ ì„¤ì •
SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"
DATA_DIR="$PROJECT_ROOT/data"
UTILS_DIR="$PROJECT_ROOT/data_pipeline/utils"
JOBS_DIR="/opt/spark/jobs"  # ì»¨í…Œì´ë„ˆ: data_pipeline/transform â†’ /opt/spark/jobs

# .env ë¡œë“œ
ENV_FILE="$PROJECT_ROOT/.env"
if [ -f "$ENV_FILE" ]; then
    set -a
    source "$ENV_FILE"
    set +a
fi

WITH_S3=false
for arg in "$@"; do
    case "$arg" in
        --with-s3) WITH_S3=true ;;
    esac
done


# spark-submit
run_spark() {
    local script="$1"
    echo "========================================"
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] SPARK: $script"
    echo "========================================"
    docker exec spark-master spark-submit "$JOBS_DIR/$script"
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] DONE:  $script"
    echo ""
}

# S3 ë‹¤ìš´ë¡œë“œ
s3_download() {
    local prefix="$1"
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] S3 DOWNLOAD: $prefix"
    python "$UTILS_DIR/download_data_from_s3.py" \
        --prefix "$prefix" --local-dir "$DATA_DIR"
}

# S3 ì—…ë¡œë“œ
s3_upload() {
    local local_dir="$1"
    local prefix="$2"
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] S3 UPLOAD: $prefix"
    python "$UTILS_DIR/upload_data_to_s3.py" \
        --local-dir "$local_dir" --prefix "$prefix" \
        --exclude _work --exclude .DS_Store --exclude _SUCCESS
}

notify_slack() {
    python "$SCRIPT_DIR/notify_slack.py" "$@" || true
}

# â”€â”€â”€ main â”€â”€â”€
TOTAL_START=$(date +%s)
BATCH_NAME="ì£¼ë°°ì¹˜ Transform (Clean â†’ S1)"

echo "========== $BATCH_NAME ì‹œì‘ =========="
if $WITH_S3; then
    echo "[MODE] S3 ì—°ë™ ëª¨ë“œ (--with-s3)"
else
    echo "[MODE] ë¡œì»¬ ëª¨ë“œ"
fi
echo ""

notify_slack info "$BATCH_NAME ì‹œì‘" "ëª¨ë“œ: $(if $WITH_S3; then echo 'S3 ì—°ë™'; else echo 'ë¡œì»¬'; fi)"

if $WITH_S3; then
    echo "--- MFA ì¸ì¦ ---"
    python "$UTILS_DIR/init_mfa_session.py" --prompt-mfa
    echo ""
fi

# â”€â”€â”€ Phase 1: Bronze â†’ Silver Clean (ì‹ë‹¹) â”€â”€â”€
echo "--- Phase 3: Clean Restaurant ---"

if $WITH_S3; then s3_download "data/bronze/restaurant_owner"; fi
run_spark clean_restaurant.py
if $WITH_S3; then s3_upload "$DATA_DIR/silver/clean/restaurant" "data/silver/clean/restaurant"; fi

# â”€â”€â”€ Phase 2: Silver Clean + S0 â†’ Silver S1 â”€â”€â”€
echo "--- Phase 4: S1 ---"

# chk_s0_to_s1
if $WITH_S3; then s3_download "data/silver/s0/address"; fi
run_spark transform_chk_s0_to_s1.py
if $WITH_S3; then s3_upload "$DATA_DIR/silver/s1/restaurant_coord" "data/silver/s1/restaurant_coord"; fi

# s0_to_s1
if $WITH_S3; then s3_download "data/silver/s0/toji_building"; fi
run_spark transform_s0_to_s1.py
if $WITH_S3; then
    s3_upload "$DATA_DIR/silver/s1/toji_list" "data/silver/s1/toji_list"
    s3_upload "$DATA_DIR/silver/s1/crawling_list" "data/silver/s1/crawling_list"
fi

TOTAL_END=$(date +%s)
ELAPSED=$(( TOTAL_END - TOTAL_START ))

echo "=========================================="
echo "[$(date '+%Y-%m-%d %H:%M:%S')] $BATCH_NAME ì™„ë£Œ"
echo "ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„: crawling_list ê¸°ë°˜ Gov24 í¬ë¡¤ë§ ì‹¤í–‰ í›„ run_transform_to_gold.sh ì‹¤í–‰"
echo "ì´ ì†Œìš”ì‹œê°„: ${ELAPSED}ì´ˆ"
echo "=========================================="

notify_slack success "$BATCH_NAME ì™„ë£Œ" "ì´ ì†Œìš”ì‹œê°„: ${ELAPSED}ì´ˆ\në‹¤ìŒ ë‹¨ê³„: crawling_list ê¸°ë°˜ í¬ë¡¤ë§ ì‹¤í–‰ í›„ run_transform_to_gold.sh"
