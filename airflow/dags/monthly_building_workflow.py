from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta
import sys
import os
import glob

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append('/opt/airflow/project')

try:
    from data_pipeline.extract.extract_buildingLeader import run_workflow
except ImportError as e:
    print(f"Import Error: {e}")

# [ì¶”ê°€] íŒŒì¼ ê²€ì¦ í•¨ìˆ˜
def validate_parquet_files(**kwargs):
    # ì „ì›” ê¸°ì¤€ ê²½ë¡œ ê³„ì‚° (ìˆ˜ì§‘ ë¡œì§ê³¼ ë™ì¼í•˜ê²Œ ë§ì¶¤)
    now = datetime.now()
    # ì‹¤í–‰ ì‹œì ì˜ ì „ì›” (ì˜ˆ: 2ì›” ì‹¤í–‰ ì‹œ 1ì›” í´ë” í™•ì¸)
    first_day_of_current_month = now.replace(day=1)
    last_day_of_prev_month = first_day_of_current_month - timedelta(days=1)

    y = last_day_of_prev_month.strftime('%Y')
    m = last_day_of_prev_month.strftime('%m')

    target_path = f"/opt/airflow/project/data/bronze/buildingLeader/parquet/year={y}/month={m}"


    print(f"ğŸ” ê²€ì¦ ê²½ë¡œ: {target_path}")

    # 1. ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
    if not os.path.exists(target_path):
        raise FileNotFoundError(f"âŒ ê²€ì¦ ì‹¤íŒ¨: ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ -> {target_path}")

    # 2. .parquet íŒŒì¼ì´ í•˜ë‚˜ë¼ë„ ìˆëŠ”ì§€ í™•ì¸
    search_pattern = os.path.join(target_path, "**", "*.parquet")
    parquet_files = glob.glob(search_pattern, recursive=True)
    if not parquet_files:
        raise FileNotFoundError(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {target_path} ì— Parquet íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    print(f"âœ… ê²€ì¦ ì™„ë£Œ: {len(parquet_files)}ê°œì˜ íŒŒì¼ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")

default_args = {
    'owner': 'PHW',
    'retries': 2,
    'retry_delay': timedelta(minutes=10),
}

with DAG(
    dag_id='building_leader_monthly_extract',
    default_args=default_args,
    description='ë§¤ë‹¬ ê±´ì¶•ë¬¼ëŒ€ì¥ í‘œì œë¶€ ë°ì´í„° ìˆ˜ì§‘',
    schedule='0 10 1 * *',
    start_date=datetime(2026, 2, 1),
    catchup=False,
    tags=['monthly', 'bronze'],
) as dag:

    # 1. ë°ì´í„° ìˆ˜ì§‘
    extract_task = PythonOperator(
        task_id='extract_building_leader',
        python_callable=run_workflow,
        provide_context=True
    )

    # 2. [ì‹ ê·œ] ìˆ˜ì§‘ ê²°ê³¼ ë°ì´í„° ê²€ì¦
    validate_task = PythonOperator(
        task_id='validate_collected_data',
        python_callable=validate_parquet_files,
        provide_context=True
    )

    # 3. S3 ì—…ë¡œë“œ
    upload_s3 = BashOperator(
        task_id='upload_to_s3',
        bash_command=(
            f"python /opt/airflow/project/data_pipeline/utils/upload_data_to_s3.py "
            f"--local-dir /opt/airflow/project/data/bronze/buildingLeader "
            f"--prefix data/bronze/buildingLeader "
            f"--exclude _work --exclude .DS_Store"
        ),
        trigger_rule='all_success' 
    )

    # íë¦„: ìˆ˜ì§‘ -> ê²€ì¦ -> S3 ì—…ë¡œë“œ
    extract_task >> validate_task >> upload_s3